---
title: Computer Vision
---

```{python}
#| echo: false
#| warning: false
import os

# os.environ["KERAS_BACKEND"] = "torch"

# import torch
# torch.set_num_threads(1)

import matplotlib

import cycler

colours = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colours)


def set_square_figures():
    matplotlib.pyplot.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    matplotlib.pyplot.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
matplotlib.pyplot.rcParams["figure.dpi"] = 350
matplotlib.pyplot.rcParams["savefig.bbox"] = "tight"
matplotlib.pyplot.rcParams["font.family"] = "serif"

matplotlib.pyplot.rcParams["axes.spines.right"] = False
matplotlib.pyplot.rcParams["axes.spines.top"] = False


def square_fig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()


def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortest_side = min(xl[1], yl[1])
    matplotlib.pyplot.plot(
        [0, shortest_side], [0, shortest_side], color="black", linestyle="--"
    )


import pandas

# pandas.options.display.max_rows = 6
pandas.options.display.max_rows = 8

import numpy

numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import keras

keras.utils.set_random_seed(1)
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import json
import random
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import keras
from keras.models import Sequential
from keras.layers import Dense, Input
from keras.callbacks import EarlyStopping
from keras.utils import plot_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

:::

::: {.content-visible unless-format="revealjs"}
Computer vision is a field of Artificial Intelligence (AI) that focuses on extracting meaningful information from visual data (images and videos). One of the primary goals of computer vision is to correctly identify and classify visual data. Convolution Neural Networks (CNNs) are the most commonly used neural network architectures for computer vision related tasks.
:::

# Images {visibility="uncounted"}

## Shapes of data

::: {.content-visible unless-format="revealjs"}
A special attention to shapes of data are important in CNN architectures, because CNNs have special types of layers (e.g. convolution and pooling) which require explicit specifications of array dimensions.
:::

![Illustration of tensors of different rank.](medium-tensor-rank.png)

::: footer
Source: Paras Patidar (2019), [Tensors — Representation of Data In Neural Networks](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b), Medium article.
:::

## Shapes of photos

![A photo is a rank 3 tensor.](rgb-channels.png)

::: footer
Source: Kim et al (2021), [Data Hiding Method for Color AMBTC Compressed Images Using Color Difference](https://www.mdpi.com/applsci/applsci-11-03418/article_deploy/html/images/applsci-11-03418-g001.png), Applied Sciences.
:::

::: {.content-visible unless-format="revealjs"}
Since the position of a pixel(one small sqaure) in a photo can be represented using 3 positional values, we call it a rank 3 tensor.
:::

## How the computer sees them {.smaller}

```{python}
#| eval: false
from matplotlib.image import imread
img1 = imread('pu.gif'); img2 = imread('pl.gif')
img3 = imread('pr.gif'); img4 = imread('pg.bmp')
f"Shapes are: {img1.shape}, {img2.shape}, {img3.shape}, {img4.shape}."
```

```{python}
#| echo: false
from matplotlib.image import imread

inds = (0, 1, 2)
img1 = imread("pu.gif")
img1 = img1[:, :, inds]

img2 = imread("pl.gif")
img2 = img2[:, :, inds]

img3 = imread("pr.gif")
img3 = img3[:, :, inds]

img4 = imread("pg.bmp")

f"Shapes are: {img1.shape}, {img2.shape}, {img3.shape}, {img4.shape}."
```

::: columns
::: {.column width="25%"}

```{python}
img1
```

:::
::: {.column width="25%"}
```{python}
img2
```

:::
::: {.column width="25%"}

```{python}
img3
```

:::
::: {.column width="25%"}
```{python}
img4
```
:::
:::
::: {.content-visible unless-format="revealjs"}
The above code reads 4 images and then shows how computers read those images. Each image is read by the computer as a rank 3 tensor. Each image is of (16,16,3) dimensions.
:::

## How we see them

```{python}
from matplotlib.pyplot import imshow
```

::: columns
::: {.column width="25%"}

```{python}
imshow(img1);
```

:::
::: {.column width="25%"}
```{python}
imshow(img2);
```

:::
::: {.column width="25%"}

```{python}
imshow(img3);
```

:::
::: {.column width="25%"}
```{python}
imshow(img4);
```
:::
:::

## Why is 255 special?

Each pixel's colour intensity is stored in one byte.

One byte is 8 bits, so in binary that is 00000000 to 11111111.

The largest _unsigned_ number this can be is $2^8-1 = 255$.

```{python}
np.array([0, 1, 255, 256]).astype(np.uint8)
```

If you had _signed_ numbers, this would go from -128 to 127.

```{python}
np.array([-128, 1, 127, 128]).astype(np.int8)
```

Alternatively, _hexidecimal_ numbers are used.
E.g. 10100001 is split into 1010 0001, and 1010=A, 0001=1, so combined it is 0xA1.

## Image editing with kernels

Take a look at [https://setosa.io/ev/image-kernels/](https://setosa.io/ev/image-kernels/).


![An example of an image kernel in action.](convolution.gif)


::: footer
Source: [Stanford's deep learning tutorial](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution) via [Stack Exchange](https://stats.stackexchange.com/a/188216).
:::

# Convolutional Layers {visibility="uncounted"}

## 'Convolution' not 'complicated'

Say $X_1, X_2 \sim f_X$ are i.i.d., and we look at $S = X_1 + X_2$.

The density for $S$ is then

$$
f_S(s) = \int_{x_1=-\infty}^{\infty} f_X(x_1) \, f_X(s-x_1) \,\mathrm{d}s .
$$

This is the _convolution_ operation, $f_S = f_X \star f_X$.

## Images are rank 3 tensors

Height, width, and number of channels.

::: {.content-visible unless-format="revealjs"}
An image can be represented using a rank 3 tensor, since it has 3 dimensions; height, width, and number of channels. The number of channels is also known as the 'depth'. The left hand side of the picture shown below is tensor with height =5, width =5 and depth =3.
:::

![Examples of rank 3 tensors.](Glassner/16-1.png)

Grayscale image has 1 channel. RGB image has 3 channels.

::: {.content-visible unless-format="revealjs"}
Each colour can be represented as a combination of three primary colours; red, green and blue.
:::

Example: Yellow = Red + Green. 

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Example: Detecting yellow

::: {.content-visible unless-format="revealjs"}
Suppose we wish to detect if a picture has yellow colour in it. One option would be to apply a neuron over each pixel and see if it detects the colour yellow. We know that each pixel is represented by 3 numerical values that correspond to red, green and blue. Higher numeric values for red and green indicate higher chances of detecting yellow. Higher values for blue indicate lower chances of detecting yellow. Utilising this information, we can assign RGB weights to be 1, 1, -1 respectively. 

Next, a standard multiplication between numeric values and weights is carried out, and the weighted sum is passed through the neuron.
:::

::: columns
::: column
![Applying a neuron to an image pixel.](Glassner/16-3.png)
:::
::: column

<br>

Apply a neuron to each pixel in the image.

<br>

If red/green $\nearrow$ or blue $\searrow$ then yellowness $\nearrow$.

<br>

Set RGB weights to 1, 1, -1.
:::
:::

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Example: Detecting yellow II 

![Scan the 3-channel input (colour image) with the neuron to produce a 1-channel output (grayscale image).](Glassner/16-4.png)

The output is produced by *sweeping* the neuron over the input. This is called **convolution**.


::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Example: Detecting yellow III {data-visibility="uncounted"}

::: {.content-visible unless-format="revealjs"}
The following picture demonstrates how yellow-coloured areas (in the colour picture) are transformed into a white colour (in the greyscale picture). This is a result of the way we assigned the weights. Since we assigned +1 weights to red and green, and -1 to blue, it ended up resulting in large positive values (for the weighted sum) for the pixels in the yellow-coloured areas. Large positive values in the greyscale correspond to white colour. Therefore, the areas which were yellow in the colour picture converted to white in the greyscale. In practice, we do not manually assign weights, instead, we let the neural network decide the optimal weights during training.
:::
![The more yellow the pixel in the colour image (left), the more white it is in the grayscale image.](Glassner/16-5.png)

The neuron or its weights is called a **filter**. We *convolve* the image with a filter, i.e. a **convolutional filter**.

## Terminology

- The same neuron is used to sweep over the image, so we can store the weights in some shared memory and process the pixels in parallel. We say that the neurons are *weight sharing*.
- In the previous example, the neuron only takes one pixel as input. Usually a larger filter containing a *block of weights* is used to process not only a pixel but also its neighboring pixels all at once.
- The weights are called the filter **kernels**.
- The cluster of pixels that forms the input of a filter is called its *footprint*.

## Spatial filter

![Example 3x3 filter](Glassner/16-9.png)


When a filter's footprint is > 1 pixel, it is a **spatial filter**.

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

::: {.content-visible unless-format="revealjs"}
The above spatial filter is a 3x3 filter. Hence, there are 9 weights to learn. 
:::
## Multidimensional convolution

::: {.content-visible unless-format="revealjs"}
In a multidimensional filter, the number of channels of the input must be equal to the number of channels in the filter (depths must be the same).
:::

Need $\# \text{ Channels in Input} = \# \text{ Channels in Filter}$.

![Example: a 3x3 filter with 3 channels, containing 27 weights.](Glassner/16-18.png)

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Example: 3x3 filter over RGB input 

![Each channel is multipled separately & then added together.](Glassner/16-19.png)

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

::: {.content-visible unless-format="revealjs"}
The above figure shows how we pick a 3x3x3 block from the image, and then apply the 3x3 filter. The multiplication is carried out channel-wise, i.e. we select the first channel of the filter and the first channel of the image and carry out the element wise multiplation. Once the elementwise multiplications for the three pairs of channels are completed, we sum them all, and pass through the neuron.
:::

## Input-output relationship 

![Matching the original image footprints against the output location.](Glassner/16-10.png)

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

::: {.content-visible unless-format="revealjs"}
The above figure shows how 9 inputs transform in to one output. As a result, dimensions of the output matrix is smaller than the dimensions of the input matrix. There are some options we can use if we wish to keep the size of input and output matrices same.
:::

# Convolutional Layer Options {visibility="uncounted"}

## Padding

![What happens when filters go off the edge of the input?](Glassner/16-15.png)
 
 - How to avoid the filter's receptive field falling off the side of the input.
 - If we only scan the filter over places of the input where the filter can fit perfectly, it will lead to loss of information, especially after many filters.
 
::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Padding

Add a border of extra elements around the input, called **padding**.
Normally we place zeros in all the new elements, called **zero padding**.

![Padded values can be added to the outside of the input.](Glassner/16-17.png)

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Convolution layer

- Multiple filters are bundled together in one layer.
- The filters are applied *simultaneously* and *independently* to the input.
- Filters can have different footprints, but in practice we almost always use the same footprint for every filter in a convolution layer.
- Number of channels in the output will be the same as the number of filters.

::: {.content-visible unless-format="revealjs"}
The motivation behind applying filters *simultaneously* and *independently* is to let the filters learn different patterns in the input-output relationship. The idea is quite similar to using many neurons in one `Dense` layer (in a `Dense` layer, we would use multiple neurons so that different neurons can capture different patterns in the input-output relationship).
:::

## Example

::: columns
::: column
In the image:

- 6-channel input tensor
- input pixels
- four 3x3 filters
- four output tensors
- final output tensor.
:::
::: column
![Example network highlighting that the number of output channels equals the number of filters.](Glassner/16-21.png)
:::
:::


::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

::: {.content-visible unless-format="revealjs"}
The above picture shows how we take in an image with 6 channels, select a 3x3 block (in pink colour), apply 4 different filters of same dimensions (in pink, green, blue and yellow), retrieve the output with 1 channel (1 output for each filter) and finally stack them together to create 1 output tensor. Note that the number of channels in the output tensor is 4, which is equal to the number of spatial filters used.  
:::


## 1x1 convolution

- Feature reduction: Reduce the number of channels in the input tensor (removing correlated features) by using fewer filters than the number of channels in the input. This is because the number of channels in the output is always the same as number of filters.
- 1x1 convolution: Convolution using 1x1 filters. 
- When the channels are correlated, 1x1 convolution is very effective at reducing channels without loss of information.

## Example of 1x1 convolution {.smaller}

![Example network with 1x1 convolution.](Glassner/16-23.png)

- Input tensor contains 300 channels.
- Use 175 1x1 filters in the convolution layer (300 weights each).
- Each filter produces a 1-channel output.
- Final output tensor has 175 channels.

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Striding

::: {.content-visible unless-format="revealjs"}
Striding options allows to modify the movement of the filter across the image. Instead moving one step at a time (either horizontally or vertically), we can increase the number of steps using the *striding* option.
:::

We don't have to go one pixel across/down at a time.

![Example: Use a stride of three horizontally and two vertically.](Glassner/16-29.png)

Dimension of output will be smaller than input.

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Choosing strides {.smaller}

::: {layout-ncol=2}
![](Glassner/16-31.png)

![](Glassner/16-32.png)
:::

When a filter scans the input step by step, it processes the same input elements multiple times. Even with larger strides, this can still happen (left image).

**If** we want to save time, we can choose strides that prevents input elements from being used more than once. Example (right image): 3x3 filter, stride 3 in both directions.

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Specifying a convolutional layer

Need to choose:

- number of filters,
- their footprints (e.g. 3x3, 5x5, etc.),
- activation functions,
- padding & striding (optional).

All the filter weights are learned during training.

# Convolutional Neural Networks {visibility="uncounted"}

## Definition of CNN

::: columns
::: {.column width="60%"}

<br>
<br>

A neural network that uses _convolution layers_ is called a _convolutional neural network_.

:::
::: {.column width="40%"}
![](xkcd-trained_a_neural_net_2x.png)
:::
:::

::: footer
Source: Randall Munroe (2019), [xkcd #2173: Trained a Neural Net](https://xkcd.com/2173/).
:::

## Architecture

![Typical CNN architecture.](Geron-mls2_1411.png)

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 14-11.
:::

::: {.content-visible unless-format="revealjs"}
A standard CNN architecture has the following components: an input layer, a sequence of feature extraction layers (which combine convolution and pooling operations sequentially), a sequence of classification layers (which include flattening and fully connected layers) and a final output layer. Convolution layers are used to extract meaningful patterns from the input using spatial filters. Pooling layers are used to reduce the spatial dimensions of the feature maps generated from convolutional layers. The purpose of the feature extraction layers is to learn complex but meaningful, high levels patterns in data. The aim of classification layers is to receive the learned patterns and make decisions more closely related to the classification task at hand.
:::

## Architecture #2

![](mathworks-typical-cnn.svg)

::: footer
Source: MathWorks, [_Introducing Deep Learning with MATLAB_](https://au.mathworks.com/campaigns/offers/next/deep-learning-ebook.html), Ebook.
:::

::: {.content-visible unless-format="revealjs"}
On a high level, the idea would be to keep on increasing the number of channels (depth) and decrease the dimensions of the feature map. We can see how the depth increases, and spatial dimensions reduce from first convolution layer to the second pooling layer.
:::

## Pooling

**Pooling**, or **downsampling**, is a technique to blur a tensor.

![Illustration of pool operations.](Glassner/16-27.png)

(a): Input tensor
(b): Subdivide input tensor into 2x2 blocks
(c): Average pooling
(d): Max pooling
(e): Icon for a pooling layer

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::


## Pooling for multiple channels {.smaller}

![Pooling a multichannel input.](Glassner/16-28.png)

- Input tensor: 6x6 with 1 channel, zero padding.
- Convolution layer: Three 3x3 filters.
- Convolution layer output: 6x6 with 3 channels.
- Pooling layer: apply max pooling to each channel.
- Pooling layer output: 3x3, 3 channels.

::: footer
Source: Glassner (2021), _Deep Learning: A Visual Approach_, Chapter 16.
:::

## Why/why not use pooling?

**Why?** Pooling *reduces the size* of tensors, therefore reduces memory usage and execution time (recall that 1x1 convolution *reduces the number of channels* in a tensor).

**Why not?**

![Geoffrey Hinton](reddit-hinton-pooling.png)

::: footer
Source: Hinton, [Reddit AMA](https://www.reddit.com/r/MachineLearning/comments/2lmo0l/comment/clyj4jv/?utm_source=share&utm_medium=web2x&context=3).
:::

## What do the CNN layers learn?

![](distill-feature-visualisation.png)

::: footer
Source: Distill article, [Feature Visualization](https://distill.pub/2017/feature-visualization/).
:::

# Chinese Character Recognition Dataset {visibility="uncounted"}

## MNIST Dataset

![The MNIST dataset.](wiki-MnistExamples.png)

::: footer
Source: Wikipedia, [MNIST database](https://en.wikipedia.org/wiki/MNIST_database).
:::

## CASIA Chinese handwriting database

Dataset source: [Institute of Automation of Chinese Academy of Sciences (CASIA)](https://nlpr.ia.ac.cn/databases/handwriting/Home.html)

![A 13 GB dataset of 3,999,571 handwritten characters.](Offlin1.jpg)

::: footer
Source: Liu et al. (2011), [CASIA online and offline Chinese handwriting databases](https://nlpr.ia.ac.cn/databases/handwriting/Home.html), 2011 International Conference on Document Analysis and Recognition.
:::

## Inspect a subset of characters

::: columns
::: column

Pulling out 55 characters to experiment with.

人从众大夫天口太因鱼犬吠哭火炎啖木林森本竹羊美羔山出女囡鸟日东月朋明肉肤工白虎门闪问闲水牛马吗妈玉王国主川舟虫

:::
::: column

Inspect directory structure
```{python}
#| output: false
!pip install directory_tree
```
```{python}
#| eval: false
from directory_tree import display_tree
display_tree("CASIA-Dataset")
```
```{python}
#| echo: false
from directory_tree import display_tree
tree = display_tree("CASIA-Dataset", string_rep=True).split("\n")
print("\n".join(tree[:12]))
print("...")
print("\n".join(tree[-4:]))
```

:::
:::

## Count number of images for each character

```{python}
def count_images_in_folders(root_folder):
    counts = {}
    for folder in root_folder.iterdir():
        counts[folder.name] = len(list(folder.glob("*.png")))
    return counts

train_counts = count_images_in_folders(Path("CASIA-Dataset/Train"))
test_counts = count_images_in_folders(Path("CASIA-Dataset/Test"))

print(train_counts)
print(test_counts)
```

## Number of images for each character

```{python}
plt.hist(train_counts.values(), bins=30, label="Train")
plt.hist(test_counts.values(), bins=30, label="Test")
plt.legend();
```

It differs, but basically ~600 training and ~140 test images per character.
A couple of characters have a lot less of both though.

## Checking the dimensions

```{python}
def get_image_dimensions(root_folder):
    dimensions = []
    for folder in root_folder.iterdir():
        for image in folder.glob("*.png"):
            img = imread(image)
            dimensions.append(img.shape)
    return dimensions

train_dimensions = get_image_dimensions(Path("CASIA-Dataset/Train"))
test_dimensions = get_image_dimensions(Path("CASIA-Dataset/Test"))

train_heights = [d[0] for d in train_dimensions]
train_widths = [d[1] for d in train_dimensions]
test_heights = [d[0] for d in test_dimensions]
test_widths = [d[1] for d in test_dimensions]
```

## Checking the dimensions II

```{python}
plt.hist(train_heights, bins=30, alpha=0.5, label="Train Heights")
plt.hist(train_widths, bins=30, alpha=0.5, label="Train Widths")
plt.hist(test_heights, bins=30, alpha=0.5, label="Test Heights")
plt.hist(test_widths, bins=30, alpha=0.5, label="Test Widths")
plt.legend();
```

The images are taller than they are wide.
We have more training images than test images.

## Checking the dimensions III

```{python}
plt.hist(train_heights, bins=30, alpha=0.5, label="Train Heights", density=True)
plt.hist(train_widths, bins=30, alpha=0.5, label="Train Widths", density=True)
plt.hist(test_heights, bins=30, alpha=0.5, label="Test Heights", density=True)
plt.hist(test_widths, bins=30, alpha=0.5, label="Test Widths", density=True)
plt.legend();
```

## Checking the dimensions IV

::: columns
::: column
```{python}
plt.hist(train_heights, bins=30, alpha=0.5, label="Train Heights", density=True)
plt.hist(test_heights, bins=30, alpha=0.5, label="Test Heights", density=True)
plt.legend();

```
:::
::: column
```{python}
plt.hist(train_widths, bins=30, alpha=0.5, label="Train Widths", density=True)
plt.hist(test_widths, bins=30, alpha=0.5, label="Test Widths", density=True)
plt.legend();
```
:::
:::

The distribution of dimensions are pretty similar between training and test sets.

## Keras image dataset loading

```{python}
#| warning: false
from keras.utils import image_dataset_from_directory #<1>

data_dir = "CASIA-Dataset"                          #<2>
batch_size = 32                                     #<3>
img_height = 80                                     #<4>
img_width = 60                                      #<5>
img_size = (img_height, img_width)                  #<6>

train_ds = image_dataset_from_directory(            #<7>
    data_dir + "/Train",
    image_size=img_size,
    batch_size=batch_size,
    shuffle=False,
    color_mode='grayscale')

test_ds = image_dataset_from_directory(             #<8>
    data_dir + "/Test",
    image_size=img_size,
    batch_size=batch_size,
    shuffle=False,
    color_mode='grayscale')
```
1. Imports `image_dataset_from_directory` class from `keras.utils` library
2. Specifies the name of the folder
3. Specifies the number of images to be trained at the same time
4. Specifies the height of the image 
5. Specifies the width of the image
6. Specifies the image size
7. Creates a data object to store the train (and validation) set. Note that `color_mode='grayscale'` command tells the computer to bring in the images in greyscale instead of the RGB scale.
8. Creates a data object to store the test set.

## Convert to numpy arrays

```{python}
class_names = train_ds.class_names
print(class_names)
```

```{python}
# NB: Need shuffle=False earlier for these X & y to line up.
X_main = np.concatenate(list(train_ds.map(lambda x, y: x)))
y_main = np.concatenate(list(train_ds.map(lambda x, y: y)))

X_test = np.concatenate(list(test_ds.map(lambda x, y: x)))
y_test = np.concatenate(list(test_ds.map(lambda x, y: y)))

X_main.shape, y_main.shape, X_test.shape, y_test.shape
```

## Some setup

```{python}
X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.2,
    random_state=123)
print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)
```

```{python}
import matplotlib.font_manager as fm
CHINESE_FONT = fm.FontProperties(fname="STHeitiTC-Medium-01.ttf")

def plot_mandarin_characters(X, y, class_names, n=5, title_font=CHINESE_FONT):
    # Plot the first n images in X
    plt.figure(figsize=(10, 4))
    for i in range(n):
        plt.subplot(1, n, i + 1)
        plt.imshow(X[i], cmap="gray")
        plt.title(class_names[y[i]], fontproperties=title_font)
        plt.axis("off")
```

```{python}
class_names[:5]
```

```{python}
X_dong = X_train[y_train == 0]; y_dong = y_train[y_train == 0]
X_ren = X_train[y_train == 2]; y_ren = y_train[y_train == 2]
```

## Plotting some training characters {.smaller}

```{python}
#| code-fold: true
plot_mandarin_characters(X_dong, y_dong, class_names)
```

```{python}
#| code-fold: true
plot_mandarin_characters(X_ren, y_ren, class_names)
```

## Without the colourmap..

::: columns
::: {.column width="5%"}
:::
::: {.column width="45%"}
```{python}
dong = X_test[y_test == 0][0]
plt.imshow(dong, cmap="gray");
```
:::
::: {.column width="45%"}
```{python}
dong = X_test[y_test == 0][1]
plt.imshow(dong);
```
:::
::: {.column width="5%"}
:::
:::

# Fitting a (multinomial) logistic regression


## Make a logistic regression

![Basically pretend it's not an image](flattening-image-to-vector.png)

```{python}
from keras.layers import Rescaling, Flatten     #<1>

num_classes = np.unique(y_train).shape[0]       #<2>
random.seed(123)
model = Sequential([
  Input((img_height, img_width, 1)), Flatten(), Rescaling(1./255),
  Dense(num_classes, activation="softmax")
])
```
1. Imports preprocessing layers from `keras.layers` useful for image data
2. Specifies the number of unique categories in the train set

::: {.callout-tip}
The `Rescaling` layer will rescale the intensities to [0, 1].
:::

## Inspecting the model

```{python}
model.summary()                            
```

## Plot the model

```{python}
plot_model(model, show_shapes=True)
```

```{python}
#| echo: false
Path("model.png").unlink(missing_ok=True)
```

## Fitting the model

```{python}
#| code-line-numbers: "1-7,14-15|"
loss = keras.losses.SparseCategoricalCrossentropy()
topk = keras.metrics.SparseTopKCategoricalAccuracy(k=5)
model.compile(optimizer='adam', loss=loss, metrics=['accuracy', topk])

epochs = 100
es = EarlyStopping(patience=15, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if Path("logistic.keras").exists():
    model = keras.models.load_model("logistic.keras")
    with open("logistic_history.json", "r") as json_file:
        history = json.load(json_file)
else:
    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),
      epochs=epochs, callbacks=[es], verbose=0)
    model.save("logistic.keras")
    history = hist.history
    with open("logistic_history.json", "w") as json_file:
        json.dump(history, json_file)
```

Most of this last part is just to save time rendering this slides, you don't need it.

## Plot the loss/accuracy curves

```{python}
#| code-fold: true
def plot_history(history):
    epochs = range(len(history["loss"]))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history["accuracy"], label="Train")
    plt.plot(epochs, history["val_accuracy"], label="Val")
    plt.legend(loc="lower right")
    plt.title("Accuracy")

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history["loss"], label="Train")
    plt.plot(epochs, history["val_loss"], label="Val")
    plt.legend(loc="upper right")
    plt.title("Loss")
    plt.show()
```

```{python}
plot_history(history)
```

## Look at the metrics

```{python}
print(model.evaluate(X_train, y_train, verbose=0))
print(model.evaluate(X_val, y_val, verbose=0))
```

```{python}
loss_value, accuracy, top5_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Validation Loss: {loss_value:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")
print(f"Validation Top 5 Accuracy: {top5_accuracy:.4f}")
```

# Fitting a CNN {visibility="uncounted"}

## Make a CNN

```{python}
from keras.layers import Conv2D, MaxPooling2D                          #<1>

random.seed(123)

model = Sequential([
  Input((img_height, img_width, 1)),
  Rescaling(1./255),                                                    #<2>
  Conv2D(16, 3, padding="same", activation="relu", name="conv1"),       #<3>
  MaxPooling2D(name="pool1"),                                           #<4>
  Conv2D(32, 3, padding="same", activation="relu", name="conv2"),
  MaxPooling2D(name="pool2"),
  Conv2D(64, 3, padding="same", activation="relu", name="conv3"),
  MaxPooling2D(name="pool3", pool_size=(4, 4)),
  Flatten(), Dense(64, activation="relu"), Dense(num_classes)           #<5>
])
```
1. Imports CNN specific preprocessing layers from `keras.layers`
2. Rescales the numeric representations of data which ranges from [0,255] in to [0, 1] range
3. Applies the convolution layer. Here `padding="same"` ensures that the dimensions of the input and output matrices remain same
4. Applies `MaxPooling`, which reduces the spatial dimensions by carrying forward the maximum value over an input window
5. Applies the `Flatten` layer to convert the 2D array (from pooling) in to a single column vector, and passes through couple of `Dense` layers to train the neural network for the specific classification problem.
Note that the output layer has number of neurons equal to `num_classes`, which corresponds to the number of unique classes in the output.

::: footer
Architecture inspired by [https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification).
:::

## Inspect the model

```{python}
model.summary()
```

## Plot the CNN

```{python}
plot_model(model, show_shapes=True)
```

```{python}
#| echo: false
Path("model.png").unlink(missing_ok=True)
```

## Fit the CNN

```{python}
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)               #<1>
topk = keras.metrics.SparseTopKCategoricalAccuracy(k=5)                           #<2>
model.compile(optimizer='adam', loss=loss, metrics=['accuracy', topk])            #<3>

epochs = 100
es = EarlyStopping(patience=15, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if Path("cnn.keras").exists():
    model = keras.models.load_model("cnn.keras")
    with open("cnn_history.json", "r") as json_file:
        history = json.load(json_file)
else:
    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),
      epochs=epochs, callbacks=[es], verbose=0)
    model.save("cnn.keras")
    history = hist.history
    with open("cnn_history.json", "w") as json_file:
        json.dump(history, json_file)
```

1. Defines the loss function with an added command `from_logits=True`. Doing this instead of defining a `softmax` function at the output `Dense` layer of the neural network is expected to be more numerically stable
2. Specifies a new metric to keep track of accuracy of the top 5 predicted classes. This means that, for each input image, the metric will consider whether the true class is among the top 5 predicted classes by the model
3. Compiles the model as usual with an optimizer, a loss function and metrics to monitor

::: {.callout-tip}
Instead of using softmax activation, just added `from_logits=True` to the loss function; this is more numerically stable.
:::

## Plot the loss/accuracy curves

```{python}
plot_history(history)
```

## Look at the metrics

```{python}
print(model.evaluate(X_train, y_train, verbose=0))
print(model.evaluate(X_val, y_val, verbose=0))
```

```{python}
loss_value, accuracy, top5_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Validation Loss: {loss_value:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")
print(f"Validation Top 5 Accuracy: {top5_accuracy:.4f}")
```

## Make a prediction

```{python}
#| error: true
model.predict(X_test[0], verbose=0);
```

<!--
```python
#| echo: false
warn = """WARNING:tensorflow:Model was constructed with shape (None, 80, 80, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 80, 80, 1), dtype=tf.float32, name='rescaling_input'), name='rescaling_input', description="created by layer 'rescaling_input'"), but it was called on an input with incompatible shape (None, 80, 1, 1)."""
print(warn)
```
-->

```{python}
X_val[0].shape, X_val[0][np.newaxis, :].shape, X_val[[0]].shape
```

```{python}
model.predict(X_val[[0]], verbose=0)
```

## Predict on the test set II

```{python}
model.predict(X_test[[0]], verbose=0).argmax()
```

```{python}
class_names[model.predict(X_test[[0]], verbose=0).argmax()]
```

```{python}
plt.imshow(X_test[0], cmap="gray");
```

# Error Analysis

## Take a look at the failure cases

```{python}
#| code-fold: true
def plot_failed_predictions(X, y, class_names, max_errors = 20,
            num_rows = 4, num_cols = 5, title_font=CHINESE_FONT):
    plt.figure(figsize=(num_cols * 2, num_rows * 2))
    errors = 0
    y_pred = model.predict(X, verbose=0)
    y_pred_classes = y_pred.argmax(axis=1)
    y_pred_probs = keras.ops.softmax(y_pred).numpy().max(axis=1)
    for i in range(len(y_pred)):
        if errors >= max_errors:
            break
        if y_pred_classes[i] != y[i]:
            plt.subplot(num_rows, num_cols, errors + 1)
            plt.imshow(X[i], cmap="gray")
            true_class = class_names[y[i]]
            pred_class = class_names[y_pred_classes[i]]
            conf = y_pred_probs[i]
            msg = f"{true_class} not {pred_class} ({conf*100:.0f}%)"
            plt.title(msg, fontproperties=title_font)
            plt.axis("off")
            errors += 1
```

```{python}
plot_failed_predictions(X_test, y_test, class_names)
```

## Confidence of predictions

```{python}
y_pred = keras.ops.convert_to_numpy(keras.activations.softmax(model(X_test)))
y_pred_class = np.argmax(y_pred, axis=1)
y_pred_prob = y_pred[np.arange(y_pred.shape[0]), y_pred_class]

confidence_when_correct = y_pred_prob[y_pred_class == y_test]
confidence_when_wrong = y_pred_prob[y_pred_class != y_test]
```

::: columns
::: column
```{python}
plt.hist(confidence_when_correct);
```
:::
::: column
```{python}
plt.hist(confidence_when_wrong);
```
:::
:::

## Another test set

55 poorly written Mandarin characters ($55 \times 7 = 385$).

![Dataset of notes when learning/practising basic characters.](mandarin-practice-notebook3.png)

## Evaluate on the new test set

```{python}
pat_ds = image_dataset_from_directory(
    "mandarin",
    image_size=img_size,
    batch_size=batch_size,
    shuffle=False,
    color_mode='grayscale')

X_pat = np.concatenate(list(pat_ds.map(lambda x, y: x)))
y_pat = np.concatenate(list(pat_ds.map(lambda x, y: y)))

assert pat_ds.class_names == class_names
X_pat.shape, y_pat.shape
```

```{python}
pat_metrics = model.evaluate(X_pat, y_pat, verbose=0)
pat_metrics
```

```{python}
correct = model.predict(X_pat, verbose=0).argmax(axis=1) == y_pat
np.sum(~correct)
```

## Errors

```{python}
plot_failed_predictions(X_pat, y_pat, class_names)
```

## Which is worst...

```{python}
class_accuracies = []
for i in range(num_classes):
    class_indices = y_pat == i
    y_pred = model.predict(X_pat[class_indices], verbose=0).argmax(axis=1)
    class_correct = y_pred == y_pat[class_indices]
    class_accuracies.append(np.mean(class_correct))

class_accuracies = pd.DataFrame({"Class": class_names, "Accuracy": class_accuracies})
class_accuracies.sort_values("Accuracy")
```

## Least (AI-) legible characters

```{python}
fails = class_accuracies[class_accuracies["Accuracy"] < 0.5]
fails.sort_values("Accuracy").plot(kind="bar", x="Class")
plt.xticks(fontproperties=CHINESE_FONT, rotation=0);
```

# Hyperparameter tuning {visibility="uncounted"}

## Trial & error

::: columns
::: column

<br>
<br>

Frankly, a lot of this is just 'enlightened' trial and error.
:::
::: column

![Or 'received wisdom' from experts...](twitter-advice.png)

:::
:::

::: footer
Source: [Twitter](https://twitter.com/tetraduzione/status/1525501171221274627?s=20&t=ulVFesw-f7hCuG9KyaoYbQ).
:::

## Keras Tuner

```{python}
#| echo: false
from sklearn.datasets import fetch_california_housing

if not Path("california-features.csv").exists():
  features, target = fetch_california_housing(as_frame=True, return_X_y=True)
  features.to_csv("california-features.csv", index=False)
  target.to_csv("california-target.csv", index=False)
else:
  features = pd.read_csv("california-features.csv")
  target = pd.read_csv("california-target.csv")

X_main, X_test, y_main, y_test = \
    train_test_split(features, target, test_size=0.2, random_state=1)

# As 0.25 x 0.8 = 0.2
X_train, X_val, y_train, y_val = \
    train_test_split(X_main, y_main, test_size=0.25, random_state=1)

X_train.shape, X_val.shape, X_test.shape

sc = StandardScaler()
sc.fit(X_train)
X_train_sc = sc.transform(X_train)
X_val_sc = sc.transform(X_val)
X_test_sc = sc.transform(X_test)
```

```{python}
#| output: false
!pip install keras-tuner
```

```{python}
import keras_tuner as kt

def build_model(hp):
    model = Sequential()
    model.add(
        Dense(
            hp.Choice("neurons", [4, 8, 16, 32, 64, 128, 256]),
            activation=hp.Choice("activation",
                ["relu", "leaky_relu", "tanh"]),
        )
    )
  
    model.add(Dense(1, activation="exponential"))
    
    learning_rate = hp.Float("lr",
        min_value=1e-4, max_value=1e-2, sampling="log")
    opt = keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(optimizer=opt, loss="poisson")
    
    return model
```

## Do a random search

```{python}
#| echo: false
# import shutil
# shutil.rmtree("random-search", ignore_errors=True)
``` 

::: columns
::: {.column width="60%"}
```{python}
tuner = kt.RandomSearch(
  build_model,
  objective="val_loss",
  max_trials=10,
  directory="random-search")

es = EarlyStopping(patience=3,
  restore_best_weights=True)

tuner.search(X_train_sc, y_train,
  epochs=100, callbacks = [es],
  validation_data=(X_val_sc, y_val))

best_model = tuner.get_best_models()[0]
```
:::
::: {.column width="40%"}
```{python}
tuner.results_summary(1)
```
:::
:::

## Tune layers separately

```{python}
def build_model(hp):
    model = Sequential()

    for i in range(hp.Int("numHiddenLayers", 1, 3)):
      # Tune number of units in each layer separately.
      model.add(
          Dense(
              hp.Choice(f"neurons_{i}", [8, 16, 32, 64]),
              activation="relu"
          )
      )
    model.add(Dense(1, activation="exponential"))

    opt = keras.optimizers.Adam(learning_rate=0.0005)
    model.compile(optimizer=opt, loss="poisson")
    
    return model
```


## Do a Bayesian search

```{python}
#| echo: false
# shutil.rmtree("bayesian-search", ignore_errors=True)
``` 

::: columns
::: {.column width="60%"}
```{python}
tuner = kt.BayesianOptimization(
  build_model,
  objective="val_loss",
  directory="bayesian-search",
  max_trials=10)

es = EarlyStopping(patience=3,
  restore_best_weights=True)

tuner.search(X_train_sc, y_train,
  epochs=100, callbacks = [es],
  validation_data=(X_val_sc, y_val))

best_model = tuner.get_best_models()[0]
```
:::
::: {.column width="40%"}
```{python}
tuner.results_summary(1)
```
:::
:::

# Benchmark Problems {visibility="uncounted"}

## Demo: Object classification

::: columns
::: column
![Example object classification run.](teach-images.png)
:::
::: column
![Example of object classification.](https://info.deeplearning.ai/hs-fs/hubfs/NOCODE.gif?width=1199&upscale=true&name=NOCODE.gif)
:::
:::

::: footer
Source: Teachable Machine, [https://teachablemachine.withgoogle.com/](https://teachablemachine.withgoogle.com/).
:::

## How does that work?

> ... these models use a technique called transfer learning. There’s a pretrained neural network, and when you create your own classes, you can sort of picture that your classes are becoming the last layer or step of the neural net. Specifically, both the image and pose models are learning off of pretrained mobilenet models ...

[Teachable Machine FAQ](https://teachablemachine.withgoogle.com/faq#Diving-Deeper)

## Benchmarks

[CIFAR-11 / CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) from Canadian Institute for Advanced Research

- 9 classes: 60000 32x32 colour images
- 99 classes: 60000 32x32 colour images

[ImageNet](https://www.image-net.org/index.php) and the _ImageNet Large Scale Visual Recognition Challenge (ILSVRC)_; originally [1,000 synsets](https://image-net.org/challenges/LSVRC/2014/browse-synsets).

- In 2021: 14,197,122 labelled images from 21,841 synsets.
- See [Keras applications](https://keras.io/api/applications/) for downloadable models.

## LeNet-6 (1998) {.smaller}

| Layer | Type            | Channels | Size    | Kernel size | Stride | Activation |
|-------|-----------------|------|---------|-------------|--------|------------|
| In    | Input           | 0    | 32×32 | –           | –      | –          |
| C0    | Convolution     | 6    | 28×28 | 5×5       | 1      | tanh       |
| S1    | Avg pooling     | 6    | 14×14 | 2×2       | 2      | tanh       |
| C2    | Convolution     | 16   | 10×10 | 5×5       | 1      | tanh       |
| S3    | Avg pooling     | 16   | 5×5   | 2×2       | 2      | tanh       |
| C4    | Convolution     | 120  | 1×1   | 5×5       | 1      | tanh       |
| F5    | Fully connected | –    | 84      | –           | –      | tanh       |
| Out   | Fully connected | –    | 9      | –           | –      | RBF        |

::: {.callout-note}
MNIST images are 27×28 pixels, and with zero-padding (for a 5×5 kernel) that becomes 32×32.
:::

::: footer
Source: Aurélien Géron (2018), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 14.
:::

## AlexNet (2011) {.smaller}

| Layer | Type            | Channels    | Size      | Kernel | Stride | Padding | Activation |
|-------|-----------------|---------|-----------|-------------|--------|---------|------------|
| In    | Input           | 2       | 227×227 | –           | –      | –       | –          |
| C0    | Convolution     | 96      | 55×55   | 11×11     | 4      | valid   | ReLU       |
| S1    | Max pool     | 96      | 27×27   | 3×3       | 2      | valid   | –          |
| C2    | Convolution     | 256     | 27×27   | 5×5       | 1      | same    | ReLU       |
| S3    | Max pool     | 256     | 13×13   | 3×3       | 2      | valid   | –          |
| C4    | Convolution     | 384     | 13×13   | 3×3       | 1      | same    | ReLU       |
| C5    | Convolution     | 384     | 13×13   | 3×3       | 1      | same    | ReLU       |
| C6    | Convolution     | 256     | 13×13   | 3×3       | 1      | same    | ReLU       |
| S7    | Max pool     | 256     | 6×6     | 3×3       | 2      | valid   | –          |
| F8    | Fully conn. | –       | 4,096     | –           | –      | –       | ReLU       |
| F9   | Fully conn. | –       | 4,096     | –           | –      | –       | ReLU       |
| Out   | Fully conn. | –       | 0,000     | –           | –      | –       | Softmax    |

::: footer
Winner of the ILSVRC 2012 challenge (top-five error 17%), developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
:::

## Data Augmentation

![Examples of data augmentation.](data-augmentation.png)

::: footer
Source: Buah et al. (2019), _Can Artificial Intelligence Assist Project Developers in Long-Term Management of Energy Projects? The Case of CO2 Capture and Storage_.
:::

## Inception module (2013)

Used in ILSVRC 2013 winning solution (top-5 error < 7%).

::: columns
::: {.column width="59%"}
![](inception-module-fig-2b.png)
:::
::: {.column width="39%"}

<br>

![](inception-meme.jpeg)
:::
:::

VGGNet was the runner-up.

::: footer
Source: Szegedy, C. et al. (2014), [_Going deeper with convolutions_](https://arxiv.org/pdf/1409.4842.pdf).
and [KnowYourMeme.com](https://knowyourmeme.com/memes/we-need-to-go-deeper)
:::

## GoogLeNet / Inception_v0 (2014)

![Schematic of the GoogLeNet architecture.](Geron-mls2_1414.png)

::: footer
Source: Aurélien Géron (2018), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 14-14.
:::

## Depth is important for image tasks

![Deeper models aren't just better because they have more parameters. Model depth given in the legend. Accuracy is on the Street View House Numbers dataset.](goodfellow-depth-matters.png)

::: footer
Source: Goodfellow et al. (2015), [Deep Learning](http://www.deeplearningbook.org), Figure 6.7.
:::

## Residual connection

![Illustration of a residual connection.](Geron-mls2_1415.png)

::: footer
Source: Aurélien Géron (2018), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 14-15.
:::


## ResNet (2014)

ResNet won the ILSVRC 2014 challenge (top-5 error 3.6%), developed by [Kaiming He et al.](https://arxiv.org/abs/1512.03385)

![Diagram of the ResNet architecture.](Geron-mls2_1417.png)

::: footer
Source: Aurélien Géron (2018), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 14-17.
:::

# Transfer Learning {visibility="uncounted"}

::: {.content-hidden unless-format="revealjs"}

{{< include _imagenet-examples.qmd >}}

:::

## Transfer learning

```python
# Pull in the base model we are transferring from.
base_model = keras.applications.Xception(
    weights="imagenet",  # Load weights pre-trained on ImageNet.
    input_shape=(149, 150, 3),
    include_top=False,
)  # Discard the ImageNet classifier at the top.

# Tell it not to update its weights.
base_model.trainable = False

# Make our new model on top of the base model.
inputs = keras.Input(shape=(149, 150, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling1D()(x)
outputs = keras.layers.Dense(0)(x)
model = keras.Model(inputs, outputs)

# Compile and fit on our data.
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)
model.fit(new_dataset, epochs=19, callbacks=..., validation_data=...)
```

::: footer
Source: François Chollet (2019), [Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/), Keras documentation.
:::

## Fine-tuning

```python
# Unfreeze the base model
base_model.trainable = True

# It's important to recompile your model after you make any changes
# to the `trainable` attribute of any inner layer, so that your changes
# are take into account
model.compile(
    optimizer=keras.optimizers.Adam(0e-5),  # Very low learning rate
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=[keras.metrics.BinaryAccuracy()],
)

# Train end-to-end. Be careful to stop before you overfit!
model.fit(new_dataset, epochs=9, callbacks=..., validation_data=...)
```

::: {.callout-caution}
Keep the learning rate low, otherwise you may accidentally throw away the useful information in the base model.
:::

::: footer
Source: François Chollet (2019), [Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/), Keras documentation.
:::

## Package Versions {.appendix data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"))
```

## Glossary {.appendix data-visibility="uncounted"}

::: columns
::: column

- AlexNet
- benchmark problems
- channels
- CIFAR-10 / CIFAR-100
- computer vision
- convolutional layer
- convolutional network
- error analysis
- filter
- GoogLeNet & Inception

:::
::: column
- ImageNet challenge
- fine-tuning
- flatten layer
- kernel
- max pooling
- MNIST
- stride
- tensor (rank)
- transfer learning
:::
:::