---
title: Recurrent Neural Networks
subtitle: ACTL3143 & ACTL5111 Deep Learning for Actuaries
author: Dr Patrick Laub
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.2
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)
  # matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 3.0)
  # matplotlib.pyplot.rcParams['figure.dpi'] = 350

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def squareFig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()

    minLeft = min(xl[0], yl[0])
    maxRight = max(xl[1], yl[1])

    matplotlib.pyplot.plot([minLeft, maxRight], [minLeft, maxRight], color="black", linestyle="--")

    matplotlib.pyplot.xlim([minLeft, maxRight])
    matplotlib.pyplot.ylim([minLeft, maxRight])
    
    # shortestLeftSide = max(xl[0], yl[0])
    # shortestRightSide = min(xl[1], yl[1])
    # matplotlib.pyplot.plot([shortestLeftSide, shortestRightSide], [shortestLeftSide, shortestRightSide], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 4

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], 'GPU') # Crashes on GPU

tensorflow.get_logger().setLevel('ERROR')
```

## Load packages {data-visibility="uncounted"}

<br>
<br>

```{python}
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping

%load_ext watermark
%watermark -p numpy,pandas,tensorflow
```

# Tensors {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Shapes of data

![Illustration of tensors of different rank.](medium-tensor-rank.png)

::: footer
Source: Paras Patidar (2019), [Tensors — Representation of Data In Neural Networks](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b), Medium article.
:::

## Shapes of photos

![A photo is a rank 3 tensor.](rgb-channels.png)

::: footer
Source: Kim et al (2021), [Data Hiding Method for Color AMBTC Compressed Images Using Color Difference](https://www.mdpi.com/applsci/applsci-11-03418/article_deploy/html/images/applsci-11-03418-g001.png), Applied Sciences.
:::

## The `axis` argument in numpy

Starting with a $(3, 4)$-shaped matrix:
```{python}
X = np.arange(12).reshape(3,4)
X
```

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (4,)$.
```{python}
X.sum(axis=0)
```
:::
::: column
`axis=1`: $(3, 4) \leadsto (3,)$.
```{python}
X.prod(axis=1)
```
:::
:::

The return value's rank is one less than the input's rank.

::: {.callout-important}
The `axis` parameter tells us which dimension is removed.
:::

## Using `axis` & `keepdims`

With `keepdims=True`, the rank doesn't change.

```{python}
X = np.arange(12).reshape(3,4)
X
```

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (1, 4)$.
```{python}
X.sum(axis=0, keepdims=True)
```
:::
::: column
`axis=1`: $(3, 4) \leadsto (3, 1)$.
```{python}
X.prod(axis=1, keepdims=True)
```
:::
:::

::: columns
::: column
```{python}
#| error: true
X / X.sum(axis=1)
```
:::
::: column
```{python}
X / X.sum(axis=1, keepdims=True)
```
:::
:::

## The rank of a time series

Say we had $n$ observations of a time series $x_1, x_2, \dots, x_n$. 

This $\mathbf{x} = (x_1, \dots, x_n)$ would have shape $(n,)$ & rank 1.

If instead we had a batch of $b$ time series'

$$
\mathbf{X} = \begin{pmatrix}
x_7 & x_8 & \dots & x_{7+n-1} \\
x_2 & x_3 & \dots & x_{2+n-1} \\
\vdots & \vdots & \ddots & \vdots \\
x_3 & x_4 & \dots & x_{3+n-1} \\
\end{pmatrix}  \,,
$$

the batch $\mathbf{X}$ would have shape $(b, n)$ & rank 2.

## Multivariate time series

::: columns
::: {.column width="35%"}

<center>

```{python}
#| echo: false
from IPython.display import Markdown
t = range(4)
x = [f"$x_{i}$" for i in t]
y = [f"$y_{i}$" for i in t]
df = pandas.DataFrame({"$x$": x, "$y$": y})
df.index.name='$t$'
Markdown(df.to_markdown())
```

</center>

:::
::: {.column width="65%"}
Say $n$ observations of the $m$ time series, would be a shape $(n, m)$ matrix of rank 2.

In Keras, a batch of $b$ of these time series has shape $(b, n, m)$ and has rank 3.
:::
:::

::: {.callout-note}
Use $\mathbf{x}_t \in \mathbb{R}^{1 \times m}$ to denote the vector of all time series at time $t$.
Here, $\mathbf{x}_t = (x_t, y_t)$.
:::


# Recurrent Neural Networks {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Basic facts of RNNs

- A recurrent neural network is a type of neural network that is designed to process sequences of data (e.g. time series, sentences).
- A recurrent neural network is any network that contains a recurrent layer.
- A recurrent layer is a layer that processes data in a sequence.
- An RNN can have one or more recurrent layers.

## Diagram of an RNN

The RNN processes each data in the sequence one by one, while keeping memory of what came before.

![Illustration of *unrolling the network through time*.](rnn-unrolled.svg)

Common RNN structures are: LSTM and GRU cells.

::: footer
Source: Marcus Lautier (2022).
:::


## Australian House Price Indices

```{python}
#|echo: false
housePrices = pd.read_csv("sa3-capitals-house-price-index.csv", index_col=0)
housePrices.index = pd.to_datetime(housePrices.index)
housePrices.index.name = "Date"
housePrices = housePrices[housePrices.index > pd.to_datetime("1990")]
housePrices.columns = ["Brisbane", "East_Bris", "North_Bris", "West_Bris",
  "Melbourne", "North_Syd", "Sydney"]
housePrices.plot(legend=False);
```

## Percentage changes {.smaller}

```{python}
changes = housePrices.pct_change().dropna()
changes.round(2)
```

```{python}
#| echo: false
pandas.options.display.max_rows = 7
``` 

## Percentage changes 


```{python}
#| eval: false
changes.plot();
```
```{python}
#| echo: false
changes.plot(lw=1);
matplotlib.pyplot.legend(bbox_to_anchor=(0.5, 1.0), loc="lower center", frameon=False, ncol=3);
```

## The size of the changes

::: columns
::: column
```{python}
changes.mean()
```

```{python}
changes *= 100
```

```{python}
changes.mean()
```
:::
::: column
```{python}
#| echo: false
# set_square_figures() 
```
```{python}
changes.plot(legend=False);
```
:::
:::

## Split _without_ shuffling

```{python}
numTrain = int(0.6 * len(changes))
numVal = int(0.2 * len(changes))
numTest = len(changes) - numTrain - numVal
print(f"# Train: {numTrain}, # Val: {numVal}, # Test: {numTest}")
```
```{python}
#| echo: false
changes.iloc[:numTrain,0].plot(c=colors[0], lw=1, alpha=0.5);
changes.iloc[numTrain:(numTrain+numVal),0].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(numTrain+numVal):,0].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[:numTrain,1:].plot(c=colors[0], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[numTrain:(numTrain+numVal),1:].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(numTrain+numVal):,1:].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
plt.legend(["Train", "Val", "Test"], frameon=False, ncol=3);
```

```{python}
#| echo: false
set_rectangular_figures() 
```

## Subsequences of a time series

Keras has a built-in method for converting a time series into subsequences/chunks.

```{python}
from tensorflow.keras.utils import timeseries_dataset_from_array

integers = range(10)                                
dummyDataset = timeseries_dataset_from_array(
    data=integers[:-3],                                 
    targets=integers[3:],                               
    sequence_length=3,                                      
    batch_size=2,                                           
)

for inputs, targets in dummyDataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

::: footer
Source: Code snippet in Chapter 10 of Chollet.
:::

# Predicting Sydney House Prices {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}


## Creating dataset objects

::: columns
::: column
```{python}
# Num. of input time series.
numTS = changes.shape[1]

# How many prev. months to use.
seqLength = 6

# Predict the next month ahead.
ahead = 1

# The index of the first target.
delay = (seqLength+ahead-1)
```
:::
::: column
```{python}
# Which suburb to predict.
targetSuburb = changes["Sydney"]

trainDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    end_index=numTrain)
```
:::
:::

::: columns
::: column
```{python}
valDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    start_index=numTrain,
    end_index=numTrain+numVal)
```
:::
::: column
```{python}
testDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    start_index=numTrain+numVal)
```
:::
:::

## Converting `Dataset` to numpy

The `Dataset` object can be handed to Keras directly, but if we really need a numpy array, we can run:
```{python}
X_train = np.concatenate(list(trainDS.map(lambda x, y: x)))
y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
```

The shape of our training set is now:

```{python}
X_train.shape
```

```{python}
y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
y_val = np.concatenate(list(valDS.map(lambda x, y: y)))
y_test = np.concatenate(list(testDS.map(lambda x, y: y)))
```

## A dense network

```{python}
from tensorflow.keras.layers import Input, Flatten
random.seed(1)
modelDense = Sequential([
    Input(shape=(seqLength, numTS)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(1, activation="linear")
])
modelDense.compile(loss="mse", optimizer="adam")
print(f"This model has {modelDense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelDense.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
from tensorflow.keras.utils import plot_model
plot_model(modelDense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelDense.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelDense.predict(valDS, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="Dense")
plt.legend(frameon=False);
```

## A `SimpleRNN` layer

```{python}
from tensorflow.keras.layers import SimpleRNN

random.seed(1)

modelSimple = Sequential([
    SimpleRNN(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])
modelSimple.compile(loss="mse", optimizer="adam")
print(f"This model has {modelSimple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelSimple.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelSimple.evaluate(valDS, verbose=0)
```

## Plot the model
```{python}
plot_model(modelSimple, show_shapes=True)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelSimple.predict(valDS, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="SimpleRNN")
plt.legend(frameon=False);
```

## A `LSTM` layer

```{python}
from tensorflow.keras.layers import LSTM

random.seed(1)

modelLSTM = Sequential([
    LSTM(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])

modelLSTM.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelLSTM.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelLSTM.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelLSTM.predict(valDS, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="LSTM")
plt.legend(frameon=False);
```

## A `GRU` layer

```{python}
from tensorflow.keras.layers import GRU

random.seed(1)

modelGRU = Sequential([
    GRU(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])

modelGRU.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelGRU.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelGRU.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelGRU.predict(valDS, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="GRU")
plt.legend(frameon=False);
```

## Two `GRU` layers

```{python}
random.seed(1)

modelTwoGRUs = Sequential([
    GRU(50, input_shape=(seqLength, numTS), return_sequences=True),
    GRU(50),
    Dense(1, activation="linear")
])

modelTwoGRUs.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelTwoGRUs.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelTwoGRUs.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelTwoGRUs.predict(valDS, verbose=0)
# plt.scatter(y_val, y_pred)
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="2 GRUs")
plt.legend(frameon=False);
```

## Compare the models

```{python}
#| echo: false
models = [modelDense, modelSimple, modelLSTM, modelGRU, modelTwoGRUs]
modelNames = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mseVal = {name: model.evaluate(valDS, verbose=0) for name, model in zip(modelNames, models)}
valResults = pd.DataFrame({
    "Model": mseVal.keys(), "MSE": mseVal.values()
})
valResults.sort_values("MSE", ascending=False)
```

The network with a single GRU layer is the best. 

```{python}
modelGRU.evaluate(testDS, verbose=0)
```

## Test set

```{python}
#| echo: false
y_pred = modelGRU.predict(testDS, verbose=0)
# plt.scatter(y_test, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_test, label="Sydney")
plt.plot(y_pred, label="GRU")
plt.legend(frameon=False);
```

# Predicting Multiple Time Series {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Creating dataset objects

::: columns
::: column
Change the `targets` argument to include all the suburbs.
:::
::: column
```{python}
trainDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    end_index=numTrain)
```
:::
:::

::: columns
::: column
```{python}
valDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    start_index=numTrain,
    end_index=numTrain+numVal)
```
:::
::: column
```{python}
testDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    start_index=numTrain+numVal)
```
:::
:::

## Converting `Dataset` to numpy

The shape of our training set is now:

```{python}
X_train = np.concatenate(list(trainDS.map(lambda x, y: x)))
X_train.shape
```

```{python}
Y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
Y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
Y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
Y_val = np.concatenate(list(valDS.map(lambda x, y: y)))
Y_test = np.concatenate(list(testDS.map(lambda x, y: y)))
```

## A dense network

```{python}
random.seed(1)
modelDense = Sequential([
    Input(shape=(seqLength, numTS)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(numTS, activation="linear")
])
modelDense.compile(loss="mse", optimizer="adam")
print(f"This model has {modelDense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelDense.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
plot_model(modelDense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelDense.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelDense.predict(valDS, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="Dense")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="Dense")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="Dense")
plt.legend(frameon=False);
```
:::
:::



## A `SimpleRNN` layer

```{python}
random.seed(1)

modelSimple = Sequential([
    SimpleRNN(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])
modelSimple.compile(loss="mse", optimizer="adam")
print(f"This model has {modelSimple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelSimple.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelSimple.evaluate(valDS, verbose=0)
```

## Plot the model
```{python}
plot_model(modelSimple, show_shapes=True)
```


## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelSimple.predict(valDS, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="SimpleRNN")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="SimpleRNN")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="SimpleRNN")
plt.legend(frameon=False);
```
:::
:::


## A `LSTM` layer

```{python}
random.seed(1)

modelLSTM = Sequential([
    LSTM(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])

modelLSTM.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelLSTM.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelLSTM.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelLSTM.predict(valDS, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="LSTM")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="LSTM")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="LSTM")
plt.legend(frameon=False);
```
:::
:::

## A `GRU` layer

```{python}
random.seed(1)

modelGRU = Sequential([
    GRU(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])

modelGRU.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelGRU.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelGRU.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelGRU.predict(valDS, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.legend(frameon=False);
```
:::
:::

## Two `GRU` layers

```{python}
random.seed(1)

modelTwoGRUs = Sequential([
    GRU(50, input_shape=(seqLength, numTS), return_sequences=True),
    GRU(50),
    Dense(numTS, activation="linear")
])

modelTwoGRUs.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelTwoGRUs.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelTwoGRUs.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelTwoGRUs.predict(valDS, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="2 GRUs")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="2 GRUs")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="2 GRUs")
plt.legend(frameon=False);
```
:::
:::

## Compare the models

```{python}
#| echo: false
models = [modelDense, modelSimple, modelLSTM, modelGRU, modelTwoGRUs]
modelNames = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mseVal = {name: model.evaluate(valDS, verbose=0) for name, model in zip(modelNames, models)}
valResults = pd.DataFrame({
    "Model": mseVal.keys(), "MSE": mseVal.values()
})
valResults.sort_values("MSE", ascending=False)
```

The network with two GRU layers is the best. 

```{python}
modelTwoGRUs.evaluate(testDS, verbose=0)
```

## Test set

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelTwoGRUs.predict(testDS, verbose=0)
plt.scatter(Y_test, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_test[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_test[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_test[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.legend(frameon=False);
```
:::
:::

## Quiz

Say $X$ is a batch of time series with shape $(4,3,2)$.
```{python}
#| echo: false
X = np.arange(4*3*2).reshape(4,3,2)
#X.shape
```

::: columns
:::: column
```{python}
#| eval: false
X[0,0,0]
```
::::: fragment
```{python}
#| echo: false
X[0,0,0]
```
:::::
::::

:::: column
```{python}
#| eval: false
X[0,1,2]
```
::::: fragment
```{python}
#| echo: false
#| error: true
X[0,1,2]
```
:::::
::::
:::


::: columns
:::: column
```{python}
#| eval: false
X[1].shape
```
::::: fragment
```{python}
#| echo: false
X[1].shape
```
:::::
::::

:::: column
```{python}
#| eval: false
X[:,-1,:].shape
```
::::: fragment
```{python}
#| echo: false
X[:,-1,:].shape
```
:::::
::::
:::


::: columns
:::: column
```{python}
#| eval: false
np.max(X, axis=1) 
```
::::: fragment
```{python}
#| echo: false
np.max(X, axis=1)
```
:::::
::::
:::: column
```{python}
#| eval: false
#| error: true
X.median(axis=2)
```
::::: fragment
```{python}
#| echo: false
#| error: true
X.median(axis=2)
```
:::::
::::
:::

## Quiz

Given a batch of time series in `X`, how would you get get:

1. A $(b, n)$-shaped matrix which is the time series holding the point-wise average of the original $m$ time series'?
2. A $(b, n, 1)$-shaped tensor of the minimum of each time series?
3. A $(b,)$-shaped vector of the average of each observation?


# SimpleRNN {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Recurrence relation

<br>

> A recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones. More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form
> 
> $$ u_n = \psi(n, u_{n-1}) \quad \text{ for } \quad n > 0.$$

<br>

__Example__: Factorial $n! = n (n-1)!$ for $n > 0$ given $0! = 1$.

::: footer
Source: Wikipedia, [Recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation#Definition). 
:::

## A SimpleRNN cell.

![Diagram of a SimpleRNN cell.](colah-LSTM3-SimpleRNN.png)

<br>

All the outputs before the final one are often discarded.

<!-- ![Notation for the diagram.](colah-LSTM2-notation.png) -->

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

## SimpleRNN

Say each prediction is a vector of size $d$, so $\mathbf{y}_t \in \mathbb{R}^{1 \times d}$.

Then the main equation of a SimpleRNN, given $\mathbf{y}_0 = \mathbf{0}$, is

$$ \mathbf{y}_t = \psi\bigl( \mathbf{x}_t \mathbf{W}_x + \mathbf{y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$

Here,
$$
\begin{aligned}
&\mathbf{x}_t \in \mathbb{R}^{1 \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{y}_{t-1} \in \mathbb{R}^{1 \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

## SimpleRNN (in batches)

Say we operate on batches of size $b$, then $\mathbf{Y}_t \in \mathbb{R}^{b \times d}$.

Then the main equation of a SimpleRNN, given $\mathbf{Y}_0 = \mathbf{0}$, is

$$ \mathbf{Y}_t = \psi\bigl( \mathbf{X}_t \mathbf{W}_x + \mathbf{Y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$


Here,
$$
\begin{aligned}
&\mathbf{X}_t \in \mathbb{R}^{b \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{Y}_{t-1} \in \mathbb{R}^{b \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

::: fragment
::: {.callout-note}
Remember, $\mathbf{X} \in \mathbb{R}^{b \times n \times m}$, $\mathbf{Y} \in \mathbb{R}^{b \times d}$, and $\mathbf{X}_t$ is equivalent to `X[:, t, :]`.
:::
:::

## Simple Keras demo

```{python}
numObs = 4
numTimeSteps = 3
numTimeSeries = 2

X = np.arange(numObs*numTimeSteps*numTimeSeries).astype(np.float32) \
        .reshape([numObs, numTimeSteps, numTimeSeries])

outputSize = 1
y = np.array([0, 0, 1, 1])
```

::: columns
::: column
```{python}
X[:2]
```
:::
::: column
```{python}
X[2:]
```
:::
:::


## Keras' SimpleRNN

As usual, the `SimpleRNN` is just a layer in Keras. 

```{python}
random.seed(1234)
model = Sequential([
  SimpleRNN(outputSize, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", metrics=["accuracy"])

hist = model.fit(X, y, epochs=500, verbose=False)
model.evaluate(X, y, verbose=False)
```

The predicted probabilities on the training set are:

```{python}
model.predict(X, verbose=0)
```

## SimpleRNN weights
```{python}
model.get_weights()
```

```{python}
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

W_x, W_y, b = model.get_weights()

Y = np.zeros((numObs, outputSize), dtype=np.float32)
for t in range(numTimeSteps):
    X_t = X[:, t, :]
    z = X_t @ W_x + Y @ W_y + b
    Y = sigmoid(z)

Y
```

# Recurrent Neural Networks {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Input of an RNN {.smaller}

- Recurrent neural network (RNN) is used to handle input data that occurs in one-dimensional sequence.
  - Natural language data such as sentences.
  - Price data such as financial time series, stock prices.
  - Temperature data.
  - Time-stamped medical events.
  - Weekly sales.
  - Hourly electricity consumption.
  - Human activity pattern.
- We talk about time steps when discussing RNN, but it can be any steps in a sequence of ordered data.
  
## Applications {.smaller}

- Forecasting: revenue forecast, weather forecast, predict disease rate from medical history, etc. 
- Classification: given a time series of the activities of a visitor on a website, classify whether the visitor is a bot or a human.
- Event detection: given a continuous data stream, identify the occurrence of a specific event. Example: Detect utterances like "Hey Alexa" from an audio stream.
- Anomaly detection: given a continuous data stream, detect anything unusual happening. This is typically an unsupervised learning problem. Example: Detect unusual activity on the corporate network.

## Recurrent neural network

For a single neuron, output of a recurrent layer is:

$$
\mathbf{Y}_t = \psi\bigl( \mathbf{X}_t \mathbf{W}_x + \mathbf{Y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) .
$$

Densely connected neural networks and convnet have no memory, which means each input is processed independently with no state (memory) kept between inputs. The recurrent neuron has a form of memory because the output at step $t$ depends on all the inputs from the previous steps. We call the part of a neural network that preserves some state across time steps a *memory cell* or a *cell*.

## Recurrent neural network {.smaller}

Note: the same weights are shared across the time steps of the recurrent network. A lot of resource will be required if these parameters are not shared. This also allows generalization when the model is used on a sequence of arbitrary length.

::: {layout-ncol=2}
![One layer of recurrent neurons unrolled through time.](Geron-recurrentneuronlayer-blur.png)

![_Deep RNN_ unrolled through time.](Geron-recurrentneurondeep-blur.png)
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 15.
:::

## Input and output sequences

![Categories of recurrent neural networks: sequence to sequence, sequence to vector, vector to sequence, encoder-decoder network.](Geron-rnnType-blur.png)

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 15.
:::


## Input and output sequences

- Sequence to sequence: Useful for predicting time series such as using prices over the last $N$ days to output the prices shifted one day into the future (i.e. from $N-1$ days ago to tomorrow.)
- Sequence to vector: ignore all outputs in the previous time steps except for the last one. Example: give a sentiment score to a sequence of words corresponding to a movie review.

## Input and output sequences {.smaller}

- Vector to sequence: feed the network the same input vector over and over at each time step and let it output a sequence. Example: given that the input is an image, find a caption for it. The image is treated as an input vector (pixels in an image do not follow a sequence). The caption is a sequence of textual description of the image. A dataset containing images and their descriptions is the input of the RNN.
- The Encoder-Decoder: The encoder is a sequence-to-vector network. The decoder is a vector-to-sequence network. Example: Feed the network a sequence in one language. Use the encoder to convert the sentence into a single vector representation. The decoder decodes this vector into the translation of the sentence in another language.

## LSTM internals

![Diagram of an LSTM cell.](colah-LSTM3-chain.png)
![Notation for the diagram.](colah-LSTM2-notation.png)

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

## GRU internals

![Diagram of a GRU cell.](colah-LSTM3-var-GRU.png)

<br>

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

{{< include _temperature-forecast-rnn-example.qmd >}}

# {data-visibility="uncounted"} 

<h2>Glossary</h2>

- dimensions (tensor)
- GRU
- LSTM
- rank (tensor)
- recurrent neural networks
- SimpleRNN

```{python}
#| echo: false
!rm model.png
```

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
