<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-contrib/reveal-auto-agenda-0.0.3/reveal-auto-agenda.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.557">

  <meta name="author" content="Patrick Laub">
  <title>AI for Actuaries - Distributional Regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #e9ecef; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #0040ff; font-weight: bold; } /* Alert */
    code span.an { color: #4090c0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #336699; } /* Attribute */
    code span.bn { color: #0033a0; } /* BaseN */
    code span.bu { color: #0055cc; } /* BuiltIn */
    code span.cf { color: #0055cc; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #002080; } /* Constant */
    code span.co { color: #4090c0; font-style: italic; } /* Comment */
    code span.cv { color: #4090c0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #3060a0; font-style: italic; } /* Documentation */
    code span.dt { color: #0044bb; } /* DataType */
    code span.dv { color: #0033a0; } /* DecVal */
    code span.er { color: #0040ff; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0033a0; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #0055cc; font-weight: bold; } /* Import */
    code span.in { color: #4090c0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #0055cc; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #0055cc; } /* Other */
    code span.pp { color: #336699; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #336699; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #4090c0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
</head>
<body class="quarto-light">
<script>
  let selectedAnnoteEl;
</script>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Distributional Regression</h1>
  <p class="subtitle">ACTL3143 &amp; ACTL5111 Deep Learning for Actuaries</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Patrick Laub 
</div>
</div>
</div>

</section>
<section>
<section id="introduction" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Introduction</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="neural-networks-and-confidence" class="slide level2">
<h2>Neural networks and confidence</h2>
<p>Say we have a neural network that classifies ducks from rabbits.</p>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="dalle2-duck.webp"></p>
<figcaption>A duck in the training set</figcaption>
</figure>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="dalle2-rabbit.webp"></p>
<figcaption>A rabbit in this training set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="new-data-can-be-different" class="slide level2 smaller">
<h2>New data can be different</h2>
<div class="columns">
<div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="dalle2-duck2.webp"></p>
<figcaption>Predict this is a duck</figcaption>
</figure>
</div>
</div><div class="column" style="width:7.5%;">

</div><div class="column fragment" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="755230_poster.jpg"></p>
<figcaption>Misclassify this as a rabbit</figcaption>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>We could do better if we collected more images of ducks with rabbit ears.</p>
</div>
<div class="footer">
<p>Source: Olga Telnova, <a href="https://www.posterlounge.co.uk/p/755230.html">Cute Duck with Bunny Ears</a>, Posterlounge, accessed on July 16 2024.</p>
</div>
</section>
<section id="new-data-can-be-challenging" class="slide level2 smaller">
<h2>New data can be challenging</h2>
<div class="columns">
<div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="dalle2-rabbit2.webp"></p>
<figcaption>Predict this is a rabbit</figcaption>
</figure>
</div>
</div><div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="duck-rabbit.png"></p>
<figcaption>Predict this is a ???</figcaption>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>This is inherently difficult, extra training data won’t help.</p>
</div>
<div class="footer">
<p>Source: <a href="https://commons.wikimedia.org/wiki/Category:Rabbit%E2%80%93duck_illusion#/media/File:Canard-lapin_retouch%C3%A9.jpg">Wikimedia Commons</a></p>
</div>
</section>
<section id="classifiers-give-us-a-probability" class="slide level2">
<h2>Classifiers give us a probability</h2>
<p>This is already a big step up compared to regression models.</p>
<p>However, neural networks’ “probabilities” can be overconfident.</p>

<img data-src="example-of-overconfidence.png" class="r-stretch quarto-figure-center"><p class="caption">We <a href="https://pat-laub.github.io/DeepLearningForActuaries/Computer-Vision/computer-vision.html#confidence-of-predictions">already saw</a> a case of this.</p><p>See Guo et al.&nbsp;(2017), <a href="https://arxiv.org/pdf/1706.04599">On Calibration of Modern Neural Networks</a>.</p>
</section>
<section id="key-idea" class="slide level2">
<h2>Key idea</h2>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="Density_forecasting_2.jpeg"></p>
<figcaption>An example of distributional forecasting over the All Ordinaries Index</figcaption>
</figure>
</div>
</div><div class="column">
<ul>
<li>Earlier machine learning models focused on point estimates.</li>
<li>However, in many applications, we need to understand the distribution of the response variable.</li>
<li>Each prediction becomes a <em>distribution</em> over the possible outcomes</li>
</ul>
</div>
</div>
<div class="footer">
<p>Source: Tomasz Woźniak (2024), <a href="https://www.linkedin.com/posts/tomaszwwozniak_rstats-densityforecasting-activity-7171005952463134721-ZsHl?utm_source=share&amp;utm_medium=member_desktop">LinkedIn Post</a>, accessed on July 15 2024.</p>
</div>
</section></section>
<section>
<section id="traditional-regression" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Traditional Regression</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="notation" class="slide level2">
<h2>Notation</h2>
<ul>
<li>scalars are denoted by lowercase letters, e.g., <span class="math inline">y</span>,</li>
<li>vectors are denoted by bold lowercase letters, e.g., <span class="math display">\boldsymbol{y} = (y_1, \ldots, y_n) ,</span></li>
<li>random variables are denoted by capital letters, e.g., <span class="math inline">Y</span></li>
<li>random vectors are denoted by bold capital letters, e.g., <span class="math display">\boldsymbol{X} = (X_1, \ldots, X_p) ,</span></li>
<li>matrices are denoted by bold uppercase non-italics letters, e.g., <span class="math display">\mathbf{X} = \begin{pmatrix} x_{11} &amp; \cdots &amp; x_{1p} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; \cdots &amp; x_{np} \end{pmatrix} .</span></li>
</ul>
</section>
<section id="regression-notation" class="slide level2">
<h2>Regression notation</h2>
<ul>
<li><span class="math inline">n</span> is the number of observations, <span class="math inline">p</span> is the number of features,</li>
<li>the true coefficients are <span class="math inline">\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)</span>,</li>
<li><span class="math inline">\beta_0</span> is the intercept, <span class="math inline">\beta_1, \ldots, \beta_p</span> are the coefficients,</li>
<li><span class="math inline">\widehat{\boldsymbol{\beta}}</span> is the estimated coefficient vector,</li>
<li><span class="math inline">\boldsymbol{x}_i = (1, x_{i1}, x_{i2}, \ldots, x_{ip})</span> is the feature vector for the <span class="math inline">i</span>th observation,</li>
<li><span class="math inline">y_i</span> is the response variable for the <span class="math inline">i</span>th observation,</li>
<li><span class="math inline">\hat{y}_i</span> is the predicted value for the <span class="math inline">i</span>th observation,</li>
<li>probability density functions (p.d.f.), probability mass functions (p.m.f.), cumulative distribution functions (c.d.f.).</li>
</ul>
</section>
<section id="traditional-regression-1" class="slide level2">
<h2>Traditional Regression</h2>
<p>Multiple linear regression assumes the data-generating process is</p>
<p><span class="math display">Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \varepsilon</span></p>
<p>where <span class="math inline">\varepsilon \sim \mathcal{N}(0, \sigma^2)</span>.</p>
<p>We estimate the coefficients <span class="math inline">\beta_0, \beta_1, \ldots, \beta_p</span> by minimising the sum of squared residuals or mean squared error</p>
<p><span class="math display">\text{RSS} := \sum_{i=1}^n (y_i - \hat{y}_i)^2
, \quad \text{MSE} := \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 ,
</span></p>
<p>where <span class="math inline">\hat{y}_i</span> is the predicted value for the <span class="math inline">i</span>th observation.</p>
</section>
<section id="visualising-the-distribution-of-each-y" class="slide level2">
<h2>Visualising the distribution of each <span class="math inline">Y</span></h2>
<div id="3b44af55" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="co"># Generate sample data for linear regression</span></span>
<span id="cb1-2"><a></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-3"><a></a>X_toy <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb1-4"><a></a>np.random.shuffle(X_toy)</span>
<span id="cb1-5"><a></a></span>
<span id="cb1-6"><a></a>beta_0 <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-7"><a></a>beta_1 <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-8"><a></a>y_toy <span class="op">=</span> beta_0 <span class="op">+</span> beta_1 <span class="op">*</span> X_toy <span class="op">+</span> np.random.normal(scale<span class="op">=</span><span class="dv">2</span>, size<span class="op">=</span>X_toy.shape)</span>
<span id="cb1-9"><a></a>sigma_toy <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Assuming a standard deviation for the normal distribution</span></span>
<span id="cb1-10"><a></a></span>
<span id="cb1-11"><a></a><span class="co"># Fit a simple linear regression model</span></span>
<span id="cb1-12"><a></a>coefficients <span class="op">=</span> np.polyfit(X_toy, y_toy, <span class="dv">1</span>)</span>
<span id="cb1-13"><a></a>predicted_y <span class="op">=</span> np.polyval(coefficients, X_toy)</span>
<span id="cb1-14"><a></a></span>
<span id="cb1-15"><a></a><span class="co"># Plot the data points and the fitted line</span></span>
<span id="cb1-16"><a></a>plt.scatter(X_toy, y_toy, label<span class="op">=</span><span class="st">'Data Points'</span>)</span>
<span id="cb1-17"><a></a>plt.plot(X_toy, predicted_y, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Fitted Line'</span>)</span>
<span id="cb1-18"><a></a></span>
<span id="cb1-19"><a></a><span class="co"># Draw the normal distribution bell curve sideways at each data point</span></span>
<span id="cb1-20"><a></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X_toy)):</span>
<span id="cb1-21"><a></a>    mu <span class="op">=</span> predicted_y[i]</span>
<span id="cb1-22"><a></a>    y_values <span class="op">=</span> np.linspace(mu <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy, mu <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy, <span class="dv">100</span>)</span>
<span id="cb1-23"><a></a>    x_values <span class="op">=</span> stats.norm.pdf(y_values, mu, sigma_toy) <span class="op">+</span> X_toy[i]</span>
<span id="cb1-24"><a></a>    plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-25"><a></a></span>
<span id="cb1-26"><a></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb1-27"><a></a>plt.ylabel(<span class="st">'$y$'</span>)</span>
<span id="cb1-28"><a></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-4-output-1.png" class="r-stretch"></section>
<section id="the-probabilistic-view" class="slide level2">
<h2>The probabilistic view</h2>
<p><span class="math display">Y_i \sim \mathcal{N}(\mu_i, \sigma^2)</span></p>
<p>where <span class="math inline">\mu_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}</span>, and the <span class="math inline">\sigma^2</span> is known.</p>
<p>The <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span> normal distribution has p.d.f.</p>
<p><span class="math display">f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) .</span></p>
<p>The likelihood function is</p>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)
</span> <span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
</span></p>
<p>Perform maximum likelihood estimation to find <span class="math inline">\boldsymbol{\beta}</span>.</p>
</section>
<section id="the-predicted-distributions" class="slide level2">
<h2>The predicted distributions</h2>
<div id="4373b48d" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>y_pred <span class="op">=</span> np.polyval(coefficients, X_toy[:<span class="dv">4</span>])</span>
<span id="cb2-2"><a></a></span>
<span id="cb2-3"><a></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>))</span>
<span id="cb2-4"><a></a></span>
<span id="cb2-5"><a></a>x_min <span class="op">=</span> y_pred[:<span class="dv">4</span>].min() <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb2-6"><a></a>x_max <span class="op">=</span> y_pred[:<span class="dv">4</span>].max() <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb2-7"><a></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb2-8"><a></a></span>
<span id="cb2-9"><a></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb2-10"><a></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb2-11"><a></a>    mu <span class="op">=</span> y_pred[i]</span>
<span id="cb2-12"><a></a>    y_grid <span class="op">=</span> stats.norm.pdf(x_grid, mu, sigma_toy)</span>
<span id="cb2-13"><a></a>    ax.plot(x_grid, y_grid)</span>
<span id="cb2-14"><a></a>    ax.set_ylabel(<span class="ss">f'$f(y ; </span><span class="ch">\\</span><span class="ss">boldsymbol</span><span class="ch">{{</span><span class="ss">x</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">)$'</span>)</span>
<span id="cb2-15"><a></a>    ax.set_xticks([y_pred[i]], labels<span class="op">=</span>[<span class="vs">r'$\mu_{'</span> <span class="op">+</span> <span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="vs">r'}$'</span>])</span>
<span id="cb2-16"><a></a>    ax.plot(y_toy[i], <span class="dv">0</span>, <span class="st">'r|'</span>)</span>
<span id="cb2-17"><a></a></span>
<span id="cb2-18"><a></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-5-output-1.png" class="r-stretch"></section>
<section id="the-machine-learning-view" class="slide level2">
<h2>The machine learning view</h2>
<p>The negative log-likelihood <span class="math inline">\text{NLL}(\boldsymbol{\beta}) := -\ell(\boldsymbol{\beta})</span> is to be minimised:</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta})
= \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
</span></p>
<p>As <span class="math inline">\sigma^2</span> is fixed, minimising NLL is equivalent to minimising MSE:</p>
<p><span class="math display">
\begin{aligned}
\widehat{\boldsymbol{\beta}}
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{NLL}(\boldsymbol{\beta}) \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{1}{n} \sum_{i=1}^n \Bigl( y_i - \hat{y}_i(\boldsymbol{x}_i; \boldsymbol{\beta}) \Bigr)^2 \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{MSE}\bigl( \boldsymbol{y}, \hat{\boldsymbol{y}}(\boldsymbol{\mathbf{X}}; \boldsymbol{\beta}) \bigr).
\end{aligned}
</span></p>
</section>
<section id="generalised-linear-model-glm" class="slide level2">
<h2>Generalised Linear Model (GLM)</h2>
<p>The GLM is often characterised by the mean prediction:</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = g^{-1} \left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)
</span></p>
<p>where <span class="math inline">g</span> is the link function.</p>
<p>Common GLM distributions for the response variable include:</p>
<ul>
<li>Normal distribution with identity link (just MLR)</li>
<li>Bernoulli distribution with logit link (logistic regression)</li>
<li>Poisson distribution with log link (Poisson regression)</li>
<li>Gamma distribution with log link</li>
</ul>
</section>
<section id="logistic-regression" class="slide level2">
<h2>Logistic regression</h2>
<p>A Bernoulli distribution with parameter <span class="math inline">p</span> has p.m.f.</p>
<p><span class="math display">
f(y)\ =\ \begin{cases}
p &amp; \text{if } y = 1 \\
1 - p &amp; \text{if } y = 0
\end{cases}
\ =\ p^y (1 - p)^{1 - y}.
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> follows a Bernoulli distribution with parameter</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \frac{1}{1 + \exp\left(-\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)} = \mathbb{P}(Y=1|\boldsymbol{X}=\boldsymbol{x}).
</span></p>
<p>The likelihood function, using <span class="math inline">\mu_i := \mu(\boldsymbol{x}_i; \boldsymbol{\beta})</span>, is</p>
<p><span class="math display">
L(\boldsymbol{\beta})
\ =\ \prod_{i=1}^n \begin{cases}
\mu_i &amp; \text{if } y_i = 1 \\
1 - \mu_i &amp; \text{if } y_i = 0
\end{cases}
\ =\ \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i} .
</span></p>
</section>
<section id="binary-cross-entropy-loss" class="slide level2">
<h2>Binary cross-entropy loss</h2>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i}
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = -\sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
<p>The binary cross-entropy loss is basically identical: <span class="math display">
\text{BCE}(\boldsymbol{y}, \boldsymbol{\mu}) = - \frac{1}{n} \sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
</section>
<section id="poisson-regression" class="slide level2">
<h2>Poisson regression</h2>
<p>A Poisson distribution with rate <span class="math inline">\lambda</span> has p.m.f. <span class="math display">
f(y) = \frac{\lambda^y \exp(-\lambda)}{y!}.
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is Poisson distributed with parameter</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right) .
</span></p>
<p>The likelihood function is</p>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{ \mu_i^{y_i} \exp(-\mu_i) }{y_i!}
</span> <span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( -\mu_i + y_i \log(\mu_i) - \log(y_i!) \Bigr).
</span></p>
</section>
<section id="poisson-loss" class="slide level2">
<h2>Poisson loss</h2>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) + \log(y_i!) \Bigr) .
</span></p>
<p>The Poisson loss is</p>
<p><span class="math display">
\text{Poisson}(\boldsymbol{y}, \boldsymbol{\mu}) = \frac{1}{n} \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) \Bigr).
</span></p>
</section>
<section id="gamma-regression" class="slide level2">
<h2>Gamma regression</h2>
<p>A gamma distribution with mean <span class="math inline">\mu</span> and dispersion <span class="math inline">\phi</span> has p.d.f. <span class="math display">
f(y; \mu, \phi) = \frac{(\mu \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y^{\frac{1}{\phi} - 1} \mathrm{e}^{-\frac{y}{\mu \phi}}
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is gamma distributed with a dispersion of <span class="math inline">\phi</span> and a mean of <span class="math inline">\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)</span>.</p>
<p>The likelihood function is <span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{(\mu_i \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y_i^{\frac{1}{\phi} - 1} \exp\left(-\frac{y_i}{\mu_i \phi}\right)
</span></p>
<p><span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ -\frac{1}{\phi} \log(\mu_i \phi) - \log \Gamma\left(\frac{1}{\phi}\right) + \left(\frac{1}{\phi} - 1\right) \log(y_i) - \frac{y_i}{\mu_i \phi} \right].
</span></p>
</section>
<section id="gamma-loss" class="slide level2">
<h2>Gamma loss</h2>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i \phi) + \log \Gamma\left(\frac{1}{\phi}\right) - \left(\frac{1}{\phi} - 1\right) \log(y_i) + \frac{y_i}{\mu_i \phi} \right].
</span></p>
<p>Since <span class="math inline">\phi</span> is a nuisance parameter <span class="math display">
\text{NLL}(\boldsymbol{\beta})
= \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i) + \frac{y_i}{\mu_i \phi} \right] + \text{const}
\propto \sum_{i=1}^n \left[ \log(\mu_i) + \frac{y_i}{\mu_i} \right].
</span></p>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>As <span class="math inline">\log(\mu_i) = \log(y_i) - \log(y_i / \mu_i)</span>, we could write an alternative version <span class="math display">
\text{NLL}(\boldsymbol{\beta})
\propto \sum_{i=1}^n \left[ \log(y_i) - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) + \frac{y_i}{\mu_i} \right]
\propto \sum_{i=1}^n \left[ \frac{y_i}{\mu_i} - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) \right].
</span></p>
</div>
</div>
</div>
</section>
<section id="why-do-actuaries-use-glms" class="slide level2">
<h2>Why do actuaries use GLMs?</h2>
<ul>
<li>GLMs are interpretable.</li>
<li>GLMs are flexible (can handle different types of response variables).</li>
<li>We get the full distribution of the response variable, not just the mean.</li>
</ul>
<p>This last point is particularly important for analysing worst-case scenarios.</p>
</section></section>
<section>
<section id="stochastic-forecasts" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Stochastic Forecasts</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="stock-price-forecasting" class="slide level2">
<h2>Stock price forecasting</h2>
<div id="9badf710" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="kw">def</span> lagged_timeseries(df, target, window<span class="op">=</span><span class="dv">30</span>):</span>
<span id="cb3-2"><a></a>    lagged <span class="op">=</span> pd.DataFrame()</span>
<span id="cb3-3"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(window, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-4"><a></a>        lagged[<span class="ss">f"T-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> df[target].shift(i)</span>
<span id="cb3-5"><a></a>    lagged[<span class="st">"T"</span>] <span class="op">=</span> df[target].values</span>
<span id="cb3-6"><a></a>    <span class="cf">return</span> lagged</span>
<span id="cb3-7"><a></a></span>
<span id="cb3-8"><a></a></span>
<span id="cb3-9"><a></a>stocks <span class="op">=</span> pd.read_csv(<span class="st">"../Time-Series-And-Recurrent-Neural-Networks/aus_fin_stocks.csv"</span>)</span>
<span id="cb3-10"><a></a>stocks[<span class="st">"Date"</span>] <span class="op">=</span> pd.to_datetime(stocks[<span class="st">"Date"</span>])</span>
<span id="cb3-11"><a></a>stocks <span class="op">=</span> stocks.set_index(<span class="st">"Date"</span>)</span>
<span id="cb3-12"><a></a>_ <span class="op">=</span> stocks.pop(<span class="st">"ASX200"</span>)</span>
<span id="cb3-13"><a></a>stock <span class="op">=</span> stocks[[<span class="st">"CBA"</span>]]</span>
<span id="cb3-14"><a></a>stock <span class="op">=</span> stock.ffill()</span>
<span id="cb3-15"><a></a></span>
<span id="cb3-16"><a></a>df_lags <span class="op">=</span> lagged_timeseries(stock, <span class="st">"CBA"</span>, <span class="dv">40</span>)</span>
<span id="cb3-17"><a></a></span>
<span id="cb3-18"><a></a><span class="co"># Split the data in time</span></span>
<span id="cb3-19"><a></a>X_train <span class="op">=</span> df_lags.loc[:<span class="st">"2018"</span>]</span>
<span id="cb3-20"><a></a>X_val <span class="op">=</span> df_lags.loc[<span class="st">"2019"</span>]</span>
<span id="cb3-21"><a></a>X_test <span class="op">=</span> df_lags.loc[<span class="st">"2020"</span>:]</span>
<span id="cb3-22"><a></a></span>
<span id="cb3-23"><a></a><span class="co"># Remove any with NAs and split into X and y</span></span>
<span id="cb3-24"><a></a>X_train <span class="op">=</span> X_train.dropna()</span>
<span id="cb3-25"><a></a>X_val <span class="op">=</span> X_val.dropna()</span>
<span id="cb3-26"><a></a>X_test <span class="op">=</span> X_test.dropna()</span>
<span id="cb3-27"><a></a></span>
<span id="cb3-28"><a></a>y_train <span class="op">=</span> X_train.pop(<span class="st">"T"</span>)</span>
<span id="cb3-29"><a></a>y_val <span class="op">=</span> X_val.pop(<span class="st">"T"</span>)</span>
<span id="cb3-30"><a></a>y_test <span class="op">=</span> X_test.pop(<span class="st">"T"</span>)</span>
<span id="cb3-31"><a></a></span>
<span id="cb3-32"><a></a>X_train <span class="op">=</span> X_train <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-33"><a></a>X_val <span class="op">=</span> X_val <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-34"><a></a>X_test <span class="op">=</span> X_test <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-35"><a></a>y_train <span class="op">=</span> y_train <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-36"><a></a>y_val <span class="op">=</span> y_val <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-37"><a></a>y_test <span class="op">=</span> y_test <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb3-38"><a></a></span>
<span id="cb3-39"><a></a>lr <span class="op">=</span> LinearRegression()</span>
<span id="cb3-40"><a></a>lr.fit(X_train, y_train)<span class="op">;</span></span>
<span id="cb3-41"><a></a></span>
<span id="cb3-42"><a></a>stocks.plot()</span>
<span id="cb3-43"><a></a>plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb3-44"><a></a>plt.legend(loc<span class="op">=</span><span class="st">"upper center"</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>), ncol<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-6-output-1.png" class="r-stretch"></section>
<section id="noisy-auto-regressive-forecast" class="slide level2">
<h2>Noisy auto-regressive forecast</h2>
<div id="11d904fd" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="kw">def</span> noisy_autoregressive_forecast(model, X_val, sigma, suppress<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-2"><a></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a></a><span class="co">    Generate a multi-step forecast using the given model.</span></span>
<span id="cb4-4"><a></a><span class="co">    """</span></span>
<span id="cb4-5"><a></a>    multi_step <span class="op">=</span> pd.Series(index<span class="op">=</span>X_val.index, name<span class="op">=</span><span class="st">"Multi Step"</span>)</span>
<span id="cb4-6"><a></a></span>
<span id="cb4-7"><a></a>    <span class="co"># Initialize the input data for forecasting</span></span>
<span id="cb4-8"><a></a>    input_data <span class="op">=</span> X_val.iloc[<span class="dv">0</span>].values.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-9"><a></a></span>
<span id="cb4-10"><a></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(multi_step)):</span>
<span id="cb4-11"><a></a>        <span class="co"># Ensure input_data has the correct feature names</span></span>
<span id="cb4-12"><a></a>        input_df <span class="op">=</span> pd.DataFrame(input_data, columns<span class="op">=</span>X_val.columns)</span>
<span id="cb4-13"><a></a>        <span class="cf">if</span> suppress:</span>
<span id="cb4-14"><a></a>            next_value <span class="op">=</span> model.predict(input_df, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-15"><a></a>        <span class="cf">else</span>:</span>
<span id="cb4-16"><a></a>            next_value <span class="op">=</span> model.predict(input_df)</span>
<span id="cb4-17"><a></a></span>
<span id="cb4-18"><a></a>        next_value <span class="op">+=</span> np.random.normal(<span class="dv">0</span>, sigma)</span>
<span id="cb4-19"><a></a></span>
<span id="cb4-20"><a></a>        multi_step.iloc[i] <span class="op">=</span> next_value</span>
<span id="cb4-21"><a></a></span>
<span id="cb4-22"><a></a>        <span class="co"># Append that prediction to the input for the next forecast</span></span>
<span id="cb4-23"><a></a>        <span class="cf">if</span> i <span class="op">+</span> <span class="dv">1</span> <span class="op">&lt;</span> <span class="bu">len</span>(multi_step):</span>
<span id="cb4-24"><a></a>            input_data <span class="op">=</span> np.append(input_data[:, <span class="dv">1</span>:], next_value).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-25"><a></a></span>
<span id="cb4-26"><a></a>    <span class="cf">return</span> multi_step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="original-forecast" class="slide level2">
<h2>Original forecast</h2>
<div id="0c86910a" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a>lr_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="792c7402" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>stock.loc[lr_forecast.index, <span class="st">"AR Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_forecast</span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a><span class="kw">def</span> plot_forecasts(stock):</span>
<span id="cb6-4"><a></a>    stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].plot()</span>
<span id="cb6-5"><a></a>    plt.axvline(<span class="st">"2019"</span>, color<span class="op">=</span><span class="st">"black"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb6-6"><a></a>    plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb6-7"><a></a>    plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb6-8"><a></a></span>
<span id="cb6-9"><a></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-9-output-1.png" class="r-stretch"><div id="bd61dad0" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>residuals <span class="op">=</span> y_train.loc[<span class="st">"2015"</span>:] <span class="op">-</span> lr.predict(X_train.loc[<span class="st">"2015"</span>:])</span>
<span id="cb7-2"><a></a>sigma <span class="op">=</span> np.std(residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="with-noise" class="slide level2">
<h2>With noise</h2>
<div id="1c3404aa" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb8-2"><a></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a07bba61" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb9-2"><a></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-12-output-1.png" class="r-stretch"></section>
<section id="with-noise-1" class="slide level2" data-visibility="uncounted">
<h2>With noise</h2>
<div id="ce0d7011" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>np.random.seed(<span class="dv">2</span>)</span>
<span id="cb10-2"><a></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="29ed001a" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb11-2"><a></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-14-output-1.png" class="r-stretch"></section>
<section id="with-noise-2" class="slide level2" data-visibility="uncounted">
<h2>With noise</h2>
<div id="bb3a072f" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>np.random.seed(<span class="dv">3</span>)</span>
<span id="cb12-2"><a></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ce986825" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb13-2"><a></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-16-output-1.png" class="r-stretch"></section>
<section id="many-noisy-forecasts" class="slide level2" data-visibility="uncounted">
<h2>Many noisy forecasts</h2>
<div id="05c45dc6" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>num_forecasts <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb14-2"><a></a>forecasts <span class="op">=</span> []</span>
<span id="cb14-3"><a></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_forecasts):</span>
<span id="cb14-4"><a></a>    forecasts.append(noisy_autoregressive_forecast(lr, X_val, sigma) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb14-5"><a></a>noisy_forecasts <span class="op">=</span> pd.concat(forecasts, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-6"><a></a>noisy_forecasts.index <span class="op">=</span> X_val.index</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2f6e1b4e" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a>noisy_forecasts.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].plot(legend<span class="op">=</span><span class="va">False</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb15-2"><a></a>plt.ylabel(<span class="st">"Stock Price"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-18-output-1.png" class="r-stretch"></section>
<section id="prediction-intervals" class="slide level2">
<h2>95% “prediction intervals”</h2>
<div id="b5b2b15c" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="co"># Calculate quantiles for the forecasts</span></span>
<span id="cb16-2"><a></a>lower_quantile <span class="op">=</span> noisy_forecasts.quantile(<span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-3"><a></a>upper_quantile <span class="op">=</span> noisy_forecasts.quantile(<span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-4"><a></a>mean_forecast <span class="op">=</span> noisy_forecasts.mean(axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="38deccf2" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a><span class="co"># Plot the mean forecast</span></span>
<span id="cb17-2"><a></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb17-3"><a></a></span>
<span id="cb17-4"><a></a>plt.plot(stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].index, stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>][<span class="st">"CBA"</span>], label<span class="op">=</span><span class="st">"CBA"</span>)</span>
<span id="cb17-5"><a></a></span>
<span id="cb17-6"><a></a>plt.plot(mean_forecast, label<span class="op">=</span><span class="st">"Mean"</span>)</span>
<span id="cb17-7"><a></a></span>
<span id="cb17-8"><a></a><span class="co"># Plot the quantile-based shaded area</span></span>
<span id="cb17-9"><a></a>plt.fill_between(mean_forecast.index, </span>
<span id="cb17-10"><a></a>                 lower_quantile, </span>
<span id="cb17-11"><a></a>                 upper_quantile, </span>
<span id="cb17-12"><a></a>                 color<span class="op">=</span><span class="st">"grey"</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb17-13"><a></a></span>
<span id="cb17-14"><a></a><span class="co"># Plot settings</span></span>
<span id="cb17-15"><a></a>plt.axvline(pd.Timestamp(<span class="st">"2019-01-01"</span>), color<span class="op">=</span><span class="st">"black"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb17-16"><a></a>plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb17-17"><a></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb17-18"><a></a>plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb17-19"><a></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-20-output-1.png" class="r-stretch"></section>
<section id="residuals" class="slide level2">
<h2>Residuals</h2>
<div class="columns">
<div class="column">
<div id="2f3aa554" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a>y_pred <span class="op">=</span> lr.predict(X_train)</span>
<span id="cb18-2"><a></a>residuals <span class="op">=</span> y_train <span class="op">-</span> y_pred</span>
<span id="cb18-3"><a></a>residuals <span class="op">-=</span> np.mean(residuals)</span>
<span id="cb18-4"><a></a>residuals <span class="op">/=</span> np.std(residuals)</span>
<span id="cb18-5"><a></a>stats.shapiro(residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/plaub/miniconda3/envs/ai2025/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: scipy.stats.shapiro: For N &gt; 5000, computed p-value may not be accurate. Current N is 6872.
  res = hypotest_fun_out(*samples, **kwds)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>ShapiroResult(statistic=0.9038043962162375, pvalue=2.2563968137519478e-54)</code></pre>
</div>
</div>
<div class="fragment">
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Probably should model the log-returns instead of the stock prices.</p>
</div>
</div>
</div>
</div>
</div><div class="column">
<div id="ea52b81b" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>plt.hist(residuals, bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-2"><a></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb21-3"><a></a>plt.xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb21-4"><a></a>plt.plot(x, stats.norm.pdf(x, <span class="dv">0</span>, <span class="dv">1</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-23-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-q-plot-and-p-p-plot" class="slide level2" data-visibility="uncounted">
<h2>Q-Q plot and P-P plot</h2>
<div class="columns">
<div class="column">
<div id="d4df7e47" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a>sm.qqplot(residuals, line<span class="op">=</span><span class="st">"45"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-24-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="3e51c602" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>sm.ProbPlot(residuals).ppplot(line<span class="op">=</span><span class="st">"45"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-25-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="residuals-against-time" class="slide level2">
<h2>Residuals against time</h2>
<div id="6b56337a" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>plt.plot(y_train.index, residuals)</span>
<span id="cb24-2"><a></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb24-3"><a></a>plt.ylabel(<span class="st">"Standardised Residuals"</span>)</span>
<span id="cb24-4"><a></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-27-output-1.png" class="r-stretch"><p>Heteroskedasticity!</p>
</section></section>
<section>
<section id="glms-and-neural-networks" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>GLMs and Neural Networks</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="french-motor-claim-sizes" class="slide level2">
<h2>French motor claim sizes</h2>
<div id="aba4ccf9" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a>sev <span class="op">=</span> pd.read_csv(<span class="st">'freMTPL2sev.csv'</span>)</span>
<span id="cb25-2"><a></a>cov <span class="op">=</span> pd.read_csv(<span class="st">'freMTPL2freq.csv'</span>).drop(columns<span class="op">=</span>[<span class="st">'ClaimNb'</span>])</span>
<span id="cb25-3"><a></a>sev <span class="op">=</span> pd.merge(sev, cov, on<span class="op">=</span><span class="st">'IDpol'</span>, how<span class="op">=</span><span class="st">'left'</span>).drop(columns<span class="op">=</span>[<span class="st">"IDpol"</span>]).dropna()</span>
<span id="cb25-4"><a></a>sev</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe caption-top" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ClaimAmount</th>
<th data-quarto-table-cell-role="th">Exposure</th>
<th data-quarto-table-cell-role="th">VehPower</th>
<th data-quarto-table-cell-role="th">VehAge</th>
<th data-quarto-table-cell-role="th">DrivAge</th>
<th data-quarto-table-cell-role="th">BonusMalus</th>
<th data-quarto-table-cell-role="th">VehBrand</th>
<th data-quarto-table-cell-role="th">VehGas</th>
<th data-quarto-table-cell-role="th">Area</th>
<th data-quarto-table-cell-role="th">Density</th>
<th data-quarto-table-cell-role="th">Region</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>995.20</td>
<td>0.59</td>
<td>11.0</td>
<td>0.0</td>
<td>39.0</td>
<td>56.0</td>
<td>B12</td>
<td>Diesel</td>
<td>D</td>
<td>778.0</td>
<td>Picardie</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1128.12</td>
<td>0.95</td>
<td>4.0</td>
<td>1.0</td>
<td>49.0</td>
<td>50.0</td>
<td>B12</td>
<td>Regular</td>
<td>E</td>
<td>2354.0</td>
<td>Ile-de-France</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">26637</td>
<td>767.55</td>
<td>0.43</td>
<td>6.0</td>
<td>0.0</td>
<td>67.0</td>
<td>50.0</td>
<td>B2</td>
<td>Diesel</td>
<td>C</td>
<td>142.0</td>
<td>Languedoc-Roussillon</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">26638</td>
<td>1500.00</td>
<td>0.28</td>
<td>7.0</td>
<td>2.0</td>
<td>36.0</td>
<td>60.0</td>
<td>B12</td>
<td>Diesel</td>
<td>D</td>
<td>1732.0</td>
<td>Rhone-Alpes</td>
</tr>
</tbody>
</table>

<p>26444 rows × 11 columns</p>
</div>
</div>
</div>
</section>
<section id="preprocessing" class="slide level2">
<h2>Preprocessing</h2>
<div id="3fa266cd" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb26-2"><a></a>  sev.drop(<span class="st">"ClaimAmount"</span>, axis<span class="op">=</span><span class="dv">1</span>), sev[<span class="st">"ClaimAmount"</span>], random_state<span class="op">=</span><span class="dv">2023</span>)</span>
<span id="cb26-3"><a></a>ct <span class="op">=</span> make_column_transformer((OrdinalEncoder(), [<span class="st">"Area"</span>, <span class="st">"VehGas"</span>]),</span>
<span id="cb26-4"><a></a>    (<span class="st">"drop"</span>, [<span class="st">"VehBrand"</span>, <span class="st">"Region"</span>]), remainder<span class="op">=</span>StandardScaler())</span>
<span id="cb26-5"><a></a>X_train <span class="op">=</span> ct.fit_transform(X_train)</span>
<span id="cb26-6"><a></a>X_test <span class="op">=</span> ct.transform(X_test)</span>
<span id="cb26-7"><a></a>plt.hist(y_train[y_train <span class="op">&lt;</span> <span class="dv">5000</span>], bins<span class="op">=</span><span class="dv">30</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-29-output-1.png" class="r-stretch"></section>
<section id="doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal" class="slide level2">
<h2>Doesn’t prove that <span class="math inline">Y | \boldsymbol{X} = \boldsymbol{x}</span> is multimodal</h2>
<div id="63b5d3e6" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a><span class="co"># Make some example where the distribution is multimodal because of a binary covariate which separates the means of the two distributions</span></span>
<span id="cb27-2"><a></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb27-3"><a></a></span>
<span id="cb27-4"><a></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-5"><a></a></span>
<span id="cb27-6"><a></a>x_min <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-7"><a></a>x_max <span class="op">=</span> y_train.max()</span>
<span id="cb27-8"><a></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb27-9"><a></a></span>
<span id="cb27-10"><a></a><span class="co"># Simulate some data from an exponential distribution which has Pr(X &lt; 1000) = 0.9</span></span>
<span id="cb27-11"><a></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb27-12"><a></a>p <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-13"><a></a>lambda_ <span class="op">=</span> <span class="op">-</span>np.log(p) <span class="op">/</span> <span class="dv">1000</span> </span>
<span id="cb27-14"><a></a>mu <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_</span>
<span id="cb27-15"><a></a>y_1 <span class="op">=</span> np.random.exponential(scale<span class="op">=</span>mu, size<span class="op">=</span>n)</span>
<span id="cb27-16"><a></a></span>
<span id="cb27-17"><a></a><span class="co"># Pick a truncated normal distribution with a mean of 1100 and std of 250 (truncated to be positive)</span></span>
<span id="cb27-18"><a></a>mu <span class="op">=</span> <span class="dv">1100</span></span>
<span id="cb27-19"><a></a>sigma <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb27-20"><a></a>y_2 <span class="op">=</span> stats.truncnorm.rvs((<span class="dv">0</span> <span class="op">-</span> mu) <span class="op">/</span> sigma, (np.inf <span class="op">-</span> mu) <span class="op">/</span> sigma, loc<span class="op">=</span>mu, scale<span class="op">=</span>sigma, size<span class="op">=</span>n)</span>
<span id="cb27-21"><a></a></span>
<span id="cb27-22"><a></a><span class="co"># Combine y_1 and y_2 for the final histogram</span></span>
<span id="cb27-23"><a></a>y <span class="op">=</span> np.concatenate([y_1, y_2])</span>
<span id="cb27-24"><a></a></span>
<span id="cb27-25"><a></a><span class="co"># Determine common bins</span></span>
<span id="cb27-26"><a></a>bins <span class="op">=</span> np.histogram_bin_edges(y, bins<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb27-27"><a></a></span>
<span id="cb27-28"><a></a></span>
<span id="cb27-29"><a></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb27-30"><a></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb27-31"><a></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-32"><a></a>        ax.hist(y_1, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>colors[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb27-33"><a></a>        ax.set_ylabel(<span class="ss">f'$f(y | x = 1)$'</span>)</span>
<span id="cb27-34"><a></a></span>
<span id="cb27-35"><a></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb27-36"><a></a>        ax.hist(y_2, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>colors[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb27-37"><a></a>        ax.set_ylabel(<span class="ss">f'$f(y | x = 2)$'</span>)</span>
<span id="cb27-38"><a></a></span>
<span id="cb27-39"><a></a>    <span class="cf">else</span>:</span>
<span id="cb27-40"><a></a>        ax.hist(y, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-41"><a></a>        ax.set_ylabel(<span class="ss">f'$f(y)$'</span>)</span>
<span id="cb27-42"><a></a></span>
<span id="cb27-43"><a></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-30-output-1.png" class="r-stretch"></section>
<section id="gamma-glm" class="slide level2">
<h2>Gamma GLM</h2>
<p>Suppose a fitted gamma GLM model has</p>
<ul>
<li>a log link function <span class="math inline">g(x)=\log(x)</span> and</li>
<li>regression coefficients <span class="math inline">\boldsymbol{\beta}=(\beta_0, \beta_1, \beta_2, \beta_3)</span>.</li>
</ul>
<p>Then, it estimates the conditional mean of <span class="math inline">Y</span> given a new instance <span class="math inline">\boldsymbol{x}=(1, x_1, x_2, x_3)</span> as follows: <span class="math display">
    \mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}] = g^{-1}(\langle \boldsymbol{\beta}, \boldsymbol{x}\rangle) = \exp\big(\beta_0 + \beta_1 x_1 + beta_2 x_2 + \beta_3 x_3 \big).
</span></p>
<p>A GLM can model any other exponential family distribution using an appropriate link function <span class="math inline">g</span>.</p>
</section>
<section id="gamma-glm-loss" class="slide level2">
<h2>Gamma GLM loss</h2>
<p>If <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is a gamma r.v. with mean <span class="math inline">\mu(\boldsymbol{x}; \boldsymbol{\beta})</span> and dispersion parameter <span class="math inline">\phi</span>, we can minimise the negative log-likelihood (NLL) <span class="math display">
    \text{NLL} \propto \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})} + \text{const},
</span> i.e., we ignore the dispersion parameter <span class="math inline">\phi</span> while estimating the regression coefficients.</p>
</section>
<section id="fitting-steps" class="slide level2">
<h2>Fitting Steps</h2>
<p>Step 1. Use the advanced second derivative iterative method to find the regression coefficients: <span class="math display">
    \widehat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{arg\,min}} \ \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})}
</span></p>
<p>Step 2. Estimate the dispersion parameter: <span class="math display">
    \phi = \frac{1}{n-p}\sum_{i=1}^{n}\frac{\bigl(y_i-\mu(\boldsymbol{x}_i; \boldsymbol{\beta})\bigr)^2}{\mu(\boldsymbol{x}_i; \boldsymbol{\beta} )^2}
</span></p>
<p>(Here, <span class="math inline">p</span> is the number of coefficients in the model. If this <span class="math inline">p</span> doesn’t include the intercept, then <span class="math inline">p</span> should be use <span class="math inline">\frac{1}{n-(p+1)}</span>.)</p>
</section>
<section id="code-gamma-glm" class="slide level2">
<h2>Code: Gamma GLM</h2>
<p>In Python, we can fit a gamma GLM as follows:</p>
<div id="48a62dbf" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb28-2"><a></a></span>
<span id="cb28-3"><a></a><span class="co"># Add a column of ones to include an intercept in the model</span></span>
<span id="cb28-4"><a></a>X_train_design <span class="op">=</span> sm.add_constant(X_train)</span>
<span id="cb28-5"><a></a></span>
<span id="cb28-6"><a></a><span class="co"># Create a Gamma GLM with a log link function</span></span>
<span id="cb28-7"><a></a>gamma_glm <span class="op">=</span> sm.GLM(y_train, X_train_design,                   </span>
<span id="cb28-8"><a></a>            family<span class="op">=</span>sm.families.Gamma(sm.families.links.Log()))</span>
<span id="cb28-9"><a></a></span>
<span id="cb28-10"><a></a><span class="co"># Fit the model</span></span>
<span id="cb28-11"><a></a>gamma_glm <span class="op">=</span> gamma_glm.fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="b18419cd" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a>gamma_glm.params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>const                    7.786576
ordinalencoder__Area    -0.073226
                           ...   
remainder__BonusMalus    0.157204
remainder__Density       0.010539
Length: 9, dtype: float64</code></pre>
</div>
</div>
</div><div class="column">
<div id="296d82cc" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a><span class="co"># Dispersion Parameter</span></span>
<span id="cb31-2"><a></a>mus <span class="op">=</span> gamma_glm.predict(X_train_design)</span>
<span id="cb31-3"><a></a>residuals <span class="op">=</span> y_train <span class="op">-</span> mus</span>
<span id="cb31-4"><a></a>dof <span class="op">=</span> (<span class="bu">len</span>(y_train)<span class="op">-</span>X_train_design.shape[<span class="dv">1</span>])</span>
<span id="cb31-5"><a></a>phi_glm <span class="op">=</span> np.sum(residuals<span class="op">**</span><span class="dv">2</span><span class="op">/</span>mus<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>dof</span>
<span id="cb31-6"><a></a><span class="bu">print</span>(phi_glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>59.63363123735805</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="ann-can-feed-into-a-glm" class="slide level2">
<h2>ANN can feed into a GLM</h2>

<img data-src="richman-glm-and-ann.png" class="r-stretch quarto-figure-center"><p class="caption">Combining GLM &amp; ANN.</p><div class="footer">
<p>Source: Ronald Richman (2022), Mind the Gap - Safely Incorporating Deep Learning Models into the Actuarial Toolkit, IFoA seminar, Slide 14.</p>
</div>
</section></section>
<section>
<section id="combined-actuarial-neural-network" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Combined Actuarial Neural Network</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="cann" class="slide level2">
<h2>CANN</h2>
<p>The Combined Actuarial Neural Network is a novel actuarial neural network architecture proposed by <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525">Schelldorfer and Wüthrich (2019)</a>. We summarise the CANN approach as follows:</p>
<ul>
<li>Find the coefficients <span class="math inline">\boldsymbol{\beta}</span> of the GLM with a link function <span class="math inline">g(\cdot)</span>.</li>
<li>Find the weights <span class="math inline">\boldsymbol{w}_{\text{CANN}}</span> of a neural network <span class="math inline">\mathcal{M}_{\text{CANN}}:\mathbb{R}^{p}\to\mathbb{R}</span>.</li>
<li>Given a new instance <span class="math inline">\boldsymbol{x}</span>, we have <span class="math display">\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}] = g^{-1}\Big( \langle\boldsymbol{\beta}, \boldsymbol{x}\rangle + \mathcal{M}_{\text{CANN}}(\boldsymbol{x};\boldsymbol{w}_{\text{CANN}})\Big).</span></li>
</ul>
</section>
<section id="shifting-the-predicted-distributions" class="slide level2">
<h2>Shifting the predicted distributions</h2>
<div id="ce6c6b92" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a><span class="co"># Ensure reproducibility</span></span>
<span id="cb33-2"><a></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb33-3"><a></a></span>
<span id="cb33-4"><a></a><span class="co"># Make a 4x1 grid of plots</span></span>
<span id="cb33-5"><a></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-6"><a></a></span>
<span id="cb33-7"><a></a><span class="co"># Define the x-axis</span></span>
<span id="cb33-8"><a></a>x_min <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-9"><a></a>x_max <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb33-10"><a></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb33-11"><a></a></span>
<span id="cb33-12"><a></a><span class="co"># Plot a few gamma distribution pdfs with different means.</span></span>
<span id="cb33-13"><a></a><span class="co"># Then plot gamma distributions with shifted means and the same dispersion parameter.</span></span>
<span id="cb33-14"><a></a>glm_means <span class="op">=</span> [<span class="dv">1000</span>, <span class="dv">3000</span>, <span class="dv">2000</span>, <span class="dv">4000</span>]</span>
<span id="cb33-15"><a></a>cann_means <span class="op">=</span> [<span class="dv">1500</span>, <span class="dv">1400</span>, <span class="dv">3000</span>, <span class="dv">5000</span>]</span>
<span id="cb33-16"><a></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb33-17"><a></a>    ax.plot(x_grid, stats.gamma.pdf(x_grid, a<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>glm_means[i]<span class="op">/</span><span class="dv">2</span>), label<span class="op">=</span><span class="ss">f'GLM'</span>)</span>
<span id="cb33-18"><a></a>    ax.plot(x_grid, stats.gamma.pdf(x_grid, a<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>cann_means[i]<span class="op">/</span><span class="dv">2</span>), label<span class="op">=</span><span class="ss">f'CANN'</span>)</span>
<span id="cb33-19"><a></a>    ax.set_ylabel(<span class="ss">f'$f(y | x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">)$'</span>)</span>
<span id="cb33-20"><a></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-21"><a></a>        ax.legend([<span class="st">"GLM"</span>, <span class="st">"CANN"</span>], loc<span class="op">=</span><span class="st">"upper right"</span>, ncol<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-34-output-1.png" class="r-stretch"></section>
<section id="architecture" class="slide level2">
<h2>Architecture</h2>

<img data-src="cann-architecture.png" class="r-stretch quarto-figure-center"><p class="caption">The CANN architecture.</p><div class="footer">
<p>Source: Schelldorfer and Wüthrich (2019), <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525">Nesting Classical Actuarial Models into Neural Networks</a>, SSRN, Figure 8.</p>
</div>
</section>
<section id="code-architecture" class="slide level2">
<h2>Code: Architecture</h2>
<div id="b298d890" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb34-2"><a></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>X_train.shape[<span class="dv">1</span>:])</span>
<span id="cb34-3"><a></a></span>
<span id="cb34-4"><a></a><span class="co"># GLM part (won't be updated during training)</span></span>
<span id="cb34-5"><a></a>glm_weights <span class="op">=</span> gamma_glm.params.iloc[<span class="dv">1</span>:].values.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb34-6"><a></a>glm_bias <span class="op">=</span> gamma_glm.params.iloc[<span class="dv">0</span>]  </span>
<span id="cb34-7"><a></a>glm_part <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'linear'</span>, trainable<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb34-8"><a></a>                     kernel_initializer<span class="op">=</span>Constant(glm_weights),</span>
<span id="cb34-9"><a></a>                     bias_initializer<span class="op">=</span>Constant(glm_bias))(inputs)</span>
<span id="cb34-10"><a></a></span>
<span id="cb34-11"><a></a><span class="co"># Neural network part</span></span>
<span id="cb34-12"><a></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'leaky_relu'</span>)(inputs)</span>
<span id="cb34-13"><a></a>nn_part <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'linear'</span>)(x) </span>
<span id="cb34-14"><a></a></span>
<span id="cb34-15"><a></a><span class="co"># Combine GLM and CANN estimates</span></span>
<span id="cb34-16"><a></a>mu <span class="op">=</span> keras.ops.exp(glm_part <span class="op">+</span> nn_part)</span>
<span id="cb34-17"><a></a>cann <span class="op">=</span> Model(inputs, mu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since this CANN predicts gamma distributions, we use the gamma NLL loss function.</p>
<div id="46b68692" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a><span class="kw">def</span> cann_negative_log_likelihood(y_true, y_pred):</span>
<span id="cb35-2"><a></a>    <span class="cf">return</span> keras.ops.mean(keras.ops.log(y_pred) <span class="op">+</span> y_true<span class="op">/</span>y_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="code-model-training" class="slide level2">
<h2>Code: Model Training</h2>
<div id="80137361" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a>cann.compile(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span>cann_negative_log_likelihood)</span>
<span id="cb36-2"><a></a>hist <span class="op">=</span> cann.fit(X_train, y_train,</span>
<span id="cb36-3"><a></a>    epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb36-4"><a></a>    callbacks<span class="op">=</span>[EarlyStopping(patience<span class="op">=</span><span class="dv">10</span>)],  </span>
<span id="cb36-5"><a></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb36-6"><a></a>    batch_size<span class="op">=</span><span class="dv">64</span>, </span>
<span id="cb36-7"><a></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Find the dispersion parameter.</p>
<div id="ea0c0573" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a>mus <span class="op">=</span> cann.predict(X_train, verbose<span class="op">=</span><span class="dv">0</span>).flatten()</span>
<span id="cb37-2"><a></a>residuals <span class="op">=</span> y_train <span class="op">-</span> mus</span>
<span id="cb37-3"><a></a>dof <span class="op">=</span> (<span class="bu">len</span>(y_train)<span class="op">-</span>(X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb37-4"><a></a>phi_cann <span class="op">=</span> np.sum(residuals<span class="op">**</span><span class="dv">2</span><span class="op">/</span>mus<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> dof</span>
<span id="cb37-5"><a></a><span class="bu">print</span>(phi_cann)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>31.171623242378978</code></pre>
</div>
</div>
</section></section>
<section>
<section id="mixture-density-network" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Mixture Density Network</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="mixture-distribution" class="slide level2">
<h2>Mixture Distribution</h2>
<p>Given a finite set of resulting random variables <span class="math inline">(Y_1, \ldots, Y_{K})</span>, one can generate a multinomial random variable <span class="math inline">Y\sim \text{Multinomial}(1, \boldsymbol{\pi})</span>. Meanwhile, <span class="math inline">Y</span> can be regarded as a mixture of <span class="math inline">Y_1, \ldots, Y_{K}</span>, i.e., <span class="math display">
  Y = \begin{cases}
      Y_1 &amp; \text{w.p. } \pi_1, \\
      \vdots &amp; \vdots\\
      Y_K &amp; \text{w.p. } \pi_K, \\
  \end{cases}
</span> where we define a set of finite set of weights <span class="math inline">\boldsymbol{\pi}=(\pi_{1} \ldots, \pi_{K})</span> such that <span class="math inline">\pi_k \ge 0</span> for <span class="math inline">k \in \{1, \ldots, K\}</span> and <span class="math inline">\sum_{k=1}^{K}\pi_k=1</span>.</p>
</section>
<section id="mixture-distribution-1" class="slide level2">
<h2>Mixture Distribution</h2>
<p>Let <span class="math inline">f_{Y_k|\boldsymbol{X}}</span> and <span class="math inline">F_{Y_k|\boldsymbol{X}}</span> be the p.d.f. and the c.d.f of <span class="math inline">Y_k|\boldsymbol{X}</span> for all <span class="math inline">k \in \{1, \ldots, K\}</span>.</p>
<p>The random variable <span class="math inline">Y|\boldsymbol{X}</span>, which mixes <span class="math inline">Y_k|\boldsymbol{X}</span>’s with weights <span class="math inline">\pi_k</span>’s, has the density function <span class="math display">
    f_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) f_{k}(y|\boldsymbol{x}),
</span> and the cumulative density function <span class="math display">
    F_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) F_{k}(y|\boldsymbol{x}).
</span></p>
</section>
<section id="mixture-density-network-1" class="slide level2">
<h2>Mixture Density Network</h2>
<p>A mixture density network (MDN) <span class="math inline">\mathcal{M}_{\boldsymbol{w}^*}</span> outputs each distribution component’s mixing weights and parameters of <span class="math inline">Y</span> given the input features <span class="math inline">\boldsymbol{x}</span>, i.e., <span class="math display">
    \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})=(\boldsymbol{\pi}(\boldsymbol{x};\boldsymbol{w}^*), \boldsymbol{\theta}(\boldsymbol{x};\boldsymbol{w}^*)),
</span> where <span class="math inline">\boldsymbol{w}^*</span> is the networks’ weights found by minimising the following negative log-likelihood loss function <span class="math display">
    \mathcal{L}(\mathcal{D}, \boldsymbol{\theta})= - \sum_{i=1}^{n} \log f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w}^*),
</span> where <span class="math inline">\mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}</span> is the training dataset.</p>
</section>
<section id="mixture-density-network-2" class="slide level2">
<h2>Mixture Density Network</h2>

<img data-src="MDN.png" class="r-stretch quarto-figure-center"><p class="caption">An MDN that outputs the parameters for a <span class="math inline">K</span> component mixture distribution. <span class="math inline">\boldsymbol{\theta}_k(\boldsymbol{x}; \boldsymbol{w}^*)= (\theta_{k,1}(\boldsymbol{x}; \boldsymbol{w}^*), \ldots, \theta_{k,|\boldsymbol{\theta}_k|}(\boldsymbol{x}; \boldsymbol{w}^*))</span> consists of the parameter estimates for the <span class="math inline">k</span>th mixture component.</p></section>
<section id="model-specification" class="slide level2">
<h2>Model Specification</h2>
<p>Suppose there are two types of claims:</p>
<ul>
<li>Type I: <span class="math inline">Y_1|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_1(\boldsymbol{x}), \beta_1(\boldsymbol{x}))</span> and,</li>
<li>Type II: <span class="math inline">Y_2|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_2(\boldsymbol{x}), \beta_2(\boldsymbol{x}))</span>.</li>
</ul>
<p>The density of the actual claim amount <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> follows <span class="math display">
    \begin{align*}
        f_{Y|\boldsymbol{X}}(y|\boldsymbol{x})
        &amp;= \pi_1(\boldsymbol{x})\cdot \frac{\beta_1(\boldsymbol{x})^{\alpha_1(\boldsymbol{x})}}{\Gamma(\alpha_1(\boldsymbol{x}))}\mathrm{e}^{-\beta_1(\boldsymbol{x})y}y^{\alpha_1(\boldsymbol{x})-1} \\
        &amp;\quad + (1-\pi_1(\boldsymbol{x}))\cdot \frac{\beta_2(\boldsymbol{x})^{\alpha_2(\boldsymbol{x})}}{\Gamma(\alpha_2(\boldsymbol{x}))}\mathrm{e}^{-\beta_2(\boldsymbol{x})y}y^{\alpha_2(\boldsymbol{x})-1}.
    \end{align*}
</span> where <span class="math inline">\pi_1(\boldsymbol{x})</span> is the probability of a Type I claim given <span class="math inline">\boldsymbol{x}</span>.</p>
</section>
<section id="output" class="slide level2">
<h2>Output</h2>
<p>The aim is to find the optimum weights <span class="math display">
    \boldsymbol{w}^* = \underset{w}{\text{arg\,min}} \ \mathcal{L}(\mathcal{D}, \boldsymbol{w})
</span> for the Gamma mixture density network <span class="math inline">\mathcal{M}_{\boldsymbol{w}^*}</span> that outputs the mixing weights, shapes and scales of <span class="math inline">Y</span> given the input features <span class="math inline">\boldsymbol{x}</span>, i.e., <span class="math display">
    \begin{align*}
        \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})
        = ( &amp;\pi_1(\boldsymbol{x}; \boldsymbol{w}^*),
             \pi_2(\boldsymbol{x}; \boldsymbol{w}^*), \\
            &amp;\alpha_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \alpha_2(\boldsymbol{x}; \boldsymbol{w}^*), \\
            &amp;\beta_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \beta_2(\boldsymbol{x}; \boldsymbol{w}^*)
        ).
    \end{align*}
</span></p>
</section>
<section id="architecture-1" class="slide level2">
<h2>Architecture</h2>

<img data-src="Gamma_MDN.png" class="r-stretch quarto-figure-center"><p class="caption">We demonstrate the structure of a gamma MDN that outputs the parameters for a gamma mixture with two components.</p></section>
<section id="code-import-legacy-keras-for-now" class="slide level2">
<h2>Code: Import “legacy” Keras (for now)</h2>
<div id="6d243cbb" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a><span class="im">import</span> tf_keras</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img data-src="keras_tfp_github_issue.png"> <img data-src="keras_tfp_github_issue_comment.png"></p>
<div class="footer">
<p>Source: Tensorflow Probability GitHub, <a href="https://github.com/tensorflow/probability/issues/1774#issuecomment-1841706103">Keras 3 breaks Tensorflow Probability upon import</a>, issue #1774.</p>
</div>
</section>
<section id="code-architecture-1" class="slide level2">
<h2>Code: Architecture</h2>
<p>The following code resembles the architecture of the architecture of the gamma MDN from the previous slide.</p>
<div id="2044f1d3" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a><span class="co"># Ensure reproducibility</span></span>
<span id="cb40-2"><a></a>random.seed(<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb40-3"><a></a></span>
<span id="cb40-4"><a></a>inputs <span class="op">=</span> tf_keras.layers.Input(shape<span class="op">=</span>X_train.shape[<span class="dv">1</span>:])</span>
<span id="cb40-5"><a></a></span>
<span id="cb40-6"><a></a><span class="co"># Two hidden layers </span></span>
<span id="cb40-7"><a></a>x <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb40-8"><a></a>x <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb40-9"><a></a></span>
<span id="cb40-10"><a></a>pis <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x) <span class="co"># Mixing weights</span></span>
<span id="cb40-11"><a></a>alphas <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'exponential'</span>)(x) <span class="co"># Shape parameters</span></span>
<span id="cb40-12"><a></a>betas <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'exponential'</span>)(x) <span class="co"># Scale parameters</span></span>
<span id="cb40-13"><a></a>out <span class="op">=</span> tf_keras.layers.Concatenate(axis<span class="op">=</span><span class="dv">1</span>)([pis, alphas, betas]) <span class="co"># shape = (None, 6)</span></span>
<span id="cb40-14"><a></a></span>
<span id="cb40-15"><a></a>gamma_mdn <span class="op">=</span> tf_keras.Model(inputs, out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="loss-function" class="slide level2">
<h2>Loss Function</h2>
<p>The negative log-likelihood loss function is given by</p>
<p><span class="math display">
    \mathcal{L}(\mathcal{D}, \boldsymbol{w})
    = - \frac{1}{n} \sum_{i=1}^{n} \log \  f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w})
</span> where the <span class="math inline">f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w})</span> is defined by <span class="math display">
\begin{align*}
    &amp;\pi_1(\boldsymbol{x};\boldsymbol{w})\cdot \frac{\beta_1(\boldsymbol{x};\boldsymbol{w})^{\alpha_1(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_1(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_1(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_1(\boldsymbol{x};\boldsymbol{w})-1} \\
    &amp; \quad + (1-\pi_1(\boldsymbol{x};\boldsymbol{w}))\cdot \frac{\beta_2(\boldsymbol{x};\boldsymbol{w})^{\alpha_2(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_2(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_2(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_2(\boldsymbol{x};\boldsymbol{w})-1}
\end{align*}
</span></p>
</section>
<section id="code-loss-training" class="slide level2">
<h2>Code: Loss &amp; training</h2>
<p><code>tensorflow_probability</code> to the rescue.</p>
<div id="4459e184" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a></a><span class="im">import</span> tensorflow_probability <span class="im">as</span> tfp</span>
<span id="cb41-2"><a></a>tfd <span class="op">=</span> tfp.distributions</span>
<span id="cb41-3"><a></a></span>
<span id="cb41-4"><a></a><span class="kw">def</span> gamma_mixture_nll(y_true, y_pred):   </span>
<span id="cb41-5"><a></a>    K <span class="op">=</span> y_pred.shape[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">3</span></span>
<span id="cb41-6"><a></a>    pis <span class="op">=</span> y_pred[:, :K]                                                    </span>
<span id="cb41-7"><a></a>    alphas <span class="op">=</span> y_pred[:, K:<span class="dv">2</span><span class="op">*</span>K]                                               </span>
<span id="cb41-8"><a></a>    betas <span class="op">=</span> y_pred[:, <span class="dv">2</span><span class="op">*</span>K:<span class="dv">3</span><span class="op">*</span>K]</span>
<span id="cb41-9"><a></a>    mixture_distribution <span class="op">=</span> tfd.MixtureSameFamily(</span>
<span id="cb41-10"><a></a>        mixture_distribution<span class="op">=</span>tfd.Categorical(probs<span class="op">=</span>pis),</span>
<span id="cb41-11"><a></a>        components_distribution<span class="op">=</span>tfd.Gamma(alphas, betas))</span>
<span id="cb41-12"><a></a>    <span class="cf">return</span> <span class="op">-</span>tf_keras.backend.mean(mixture_distribution.log_prob(y_true))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5d5fc127" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a>gamma_mdn.compile(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span>gamma_mixture_nll)</span>
<span id="cb42-2"><a></a></span>
<span id="cb42-3"><a></a>hist <span class="op">=</span> gamma_mdn.fit(X_train, y_train,</span>
<span id="cb42-4"><a></a>    epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb42-5"><a></a>    callbacks<span class="op">=</span>[tf_keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">10</span>)],  </span>
<span id="cb42-6"><a></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb42-7"><a></a>    batch_size<span class="op">=</span><span class="dv">64</span>, </span>
<span id="cb42-8"><a></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="metrics-for-distributional-regression" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Metrics for Distributional Regression</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="proper-scoring-rules" class="slide level2">
<h2>Proper Scoring Rules</h2>
<dl>
<dt>Definition</dt>
<dd>
<p>A <em>scoring rule</em> is the equivalent of a loss function for distributional regression.</p>
<p>Denote <span class="math inline">S(F, y)</span> to be the score given to the forecasted distribution <span class="math inline">F</span> and an observation <span class="math inline">y \in \mathbb{R}</span>.</p>
</dd>
<dt>Definition</dt>
<dd>
<p>A scoring rule is called <em>proper</em> if <span class="math display">
\mathbb{E}_{Y \sim Q} S(Q, Y) \le \mathbb{E}_{Y \sim Q} S(F, Y)
</span> for all <span class="math inline">F</span> and <span class="math inline">Q</span> distributions.</p>
<p>It is called <em>strictly proper</em> if equality holds only if <span class="math inline">F = Q</span>.</p>
</dd>
</dl>
</section>
<section id="example-proper-scoring-rules" class="slide level2">
<h2>Example Proper Scoring Rules</h2>
<dl>
<dt>Logarithmic Score (NLL)</dt>
<dd>
<p>The logarithmic score is defined as <span class="math display">
    \mathrm{LogS}(f, y) = - \log f(y),
</span> where <span class="math inline">f</span> is the predictive density.</p>
</dd>
<dt>Continuous Ranked Probability Score (CRPS)</dt>
<dd>
<p>The continuous ranked probability score is defined as <span class="math display">
    \mathrm{crps}(F, y) = \int_{-\infty}^{\infty} (F(t) - {1}_{t\ge y})^2 \ \mathrm{d}t,
</span> where <span class="math inline">F</span> is the predicted c.d.f.</p>
</dd>
</dl>
<div class="footer">
<p>See, e.g., Taggert (2023), <a href="https://nla.gov.au/nla.obj-3160938615/view">Estimation of CRPS for precipitation forecasts…</a>, BoM Research Report 079.</p>
</div>
</section>
<section id="likelihoods" class="slide level2">
<h2>Likelihoods</h2>
<div id="05ff641c" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a></a>y_pred <span class="op">=</span> np.polyval(coefficients, X_toy[:<span class="dv">4</span>])</span>
<span id="cb43-2"><a></a>y_pred[<span class="dv">2</span>] <span class="op">*=</span> <span class="fl">1.1</span></span>
<span id="cb43-3"><a></a>sigma_preds <span class="op">=</span> sigma_toy <span class="op">*</span> np.array([<span class="fl">1.0</span>, <span class="fl">3.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb43-4"><a></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-5"><a></a></span>
<span id="cb43-6"><a></a>x_min <span class="op">=</span> y_pred[:<span class="dv">4</span>].min() <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb43-7"><a></a>x_max <span class="op">=</span> y_pred[:<span class="dv">4</span>].max() <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb43-8"><a></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb43-9"><a></a></span>
<span id="cb43-10"><a></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb43-11"><a></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb43-12"><a></a>    y_grid <span class="op">=</span> stats.norm.pdf(x_grid, y_pred[i], sigma_preds[i])</span>
<span id="cb43-13"><a></a>    ax.plot(x_grid, y_grid)</span>
<span id="cb43-14"><a></a>    ax.plot([y_toy[i], y_toy[i]], [<span class="dv">0</span>, stats.norm.pdf(y_toy[i], y_pred[i], sigma_preds[i])], color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb43-15"><a></a>    ax.scatter([y_toy[i]], [stats.norm.pdf(y_toy[i], y_pred[i], sigma_preds[i])], color<span class="op">=</span><span class="st">'red'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb43-16"><a></a>    ax.set_title(<span class="ss">f'$f(y ; </span><span class="ch">\\</span><span class="ss">boldsymbol</span><span class="ch">{{</span><span class="ss">x</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">)$'</span>)</span>
<span id="cb43-17"><a></a>    ax.set_xticks([y_pred[i]], labels<span class="op">=</span>[<span class="vs">r'$\mu_{'</span> <span class="op">+</span> <span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="vs">r'}$'</span>])</span>
<span id="cb43-18"><a></a>    <span class="co"># ax.set_ylim(0, 0.25)</span></span>
<span id="cb43-19"><a></a></span>
<span id="cb43-20"><a></a>    <span class="co"># Turn off the y axes</span></span>
<span id="cb43-21"><a></a>    ax.yaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb43-22"><a></a>    </span>
<span id="cb43-23"><a></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-45-output-1.png" class="r-stretch"></section>
<section id="code-nll" class="slide level2">
<h2>Code: NLL</h2>
<div id="1d325851" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a></a><span class="kw">def</span> gamma_nll(mean, dispersion, y):</span>
<span id="cb44-2"><a></a>    <span class="co"># Calculate shape and scale parameters from mean and dispersion</span></span>
<span id="cb44-3"><a></a>    shape <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> dispersion<span class="op">;</span> scale <span class="op">=</span> mean <span class="op">*</span> dispersion</span>
<span id="cb44-4"><a></a></span>
<span id="cb44-5"><a></a>    <span class="co"># Create a gamma distribution object</span></span>
<span id="cb44-6"><a></a>    gamma_dist <span class="op">=</span> stats.gamma(a<span class="op">=</span>shape, scale<span class="op">=</span>scale)</span>
<span id="cb44-7"><a></a>    </span>
<span id="cb44-8"><a></a>    <span class="cf">return</span> <span class="op">-</span>np.mean(gamma_dist.logpdf(y))</span>
<span id="cb44-9"><a></a></span>
<span id="cb44-10"><a></a><span class="co"># GLM</span></span>
<span id="cb44-11"><a></a>X_test_design <span class="op">=</span> sm.add_constant(X_test)</span>
<span id="cb44-12"><a></a>mus <span class="op">=</span> gamma_glm.predict(X_test_design)</span>
<span id="cb44-13"><a></a>nll_glm <span class="op">=</span> gamma_nll(mus, phi_glm, y_test)</span>
<span id="cb44-14"><a></a></span>
<span id="cb44-15"><a></a><span class="co"># CANN</span></span>
<span id="cb44-16"><a></a>mus <span class="op">=</span> cann.predict(X_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-17"><a></a>nll_cann <span class="op">=</span> gamma_nll(mus, phi_cann, y_test)</span>
<span id="cb44-18"><a></a></span>
<span id="cb44-19"><a></a><span class="co"># MDN</span></span>
<span id="cb44-20"><a></a>nll_mdn <span class="op">=</span> gamma_mdn.evaluate(X_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-comparisons" class="slide level2">
<h2>Model Comparisons</h2>
<div id="21ce0528" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a></a><span class="bu">print</span>(<span class="ss">f'GLM: </span><span class="sc">{</span>round(nll_glm, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb45-2"><a></a><span class="bu">print</span>(<span class="ss">f'CANN: </span><span class="sc">{</span>round(nll_cann, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb45-3"><a></a><span class="bu">print</span>(<span class="ss">f'MDN: </span><span class="sc">{</span>round(nll_mdn, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GLM: 11.02
CANN: 10.44
MDN: 8.67</code></pre>
</div>
</div>
</section></section>
<section>
<section id="aleatoric-and-epistemic-uncertainty" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Aleatoric and Epistemic Uncertainty</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="categories-of-uncertainty" class="slide level2">
<h2>Categories of uncertainty</h2>
<p>There are two major categories of uncertainty in statistical or machine learning:</p>
<ul>
<li>Aleatoric uncertainty</li>
<li>Epistemic uncertainty</li>
</ul>
<p>Since there is no consensus on the definitions of aleatoric and epistemic uncertainty, we provide the most acknowledged definitions in the following slides.</p>
</section>
<section id="aleatoric-uncertainty" class="slide level2">
<h2>Aleatoric Uncertainty</h2>
<dl>
<dt>Qualitative Definition</dt>
<dd>
<p><em>Aleatoric uncertainty refers to the statistical variability and inherent noise with data distribution that modelling cannot explain.</em></p>
</dd>
<dt>Quantitative Definition</dt>
<dd>
<p><span class="math display">\text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}) = \mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}],</span>i.e., if <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma^2)</span>, the aleatoric uncertainty would be <span class="math inline">\sigma^2</span>. Simply, it is the conditional variance of the response variable <span class="math inline">Y</span> given features/covariates <span class="math inline">\boldsymbol{x}</span>.</p>
</dd>
</dl>
</section>
<section id="epistemic-uncertainty" class="slide level2">
<h2>Epistemic Uncertainty</h2>
<dl>
<dt>Qualitative Definition</dt>
<dd>
<p><em>Epistemic uncertainty refers to the lack of knowledge, limited data information, parameter errors and model errors.</em></p>
</dd>
<dt>Quantitative Definition</dt>
<dd>
<p><span class="math display">\text{Epi}(Y|\boldsymbol{X}=\boldsymbol{x}) = \text{Uncertainty}(Y|\boldsymbol{X}=\boldsymbol{x}) - \text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}),</span></p>
</dd>
</dl>
<p>i.e., the total uncertainty subtracting the aleatoric uncertainty <span class="math inline">\mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}]</span> would be the epistemic uncertainty.</p>
</section>
<section id="sources-of-uncertainty" class="slide level2">
<h2>Sources of uncertainty</h2>
<p><em>If you decide to predict the claim amount of an individual using a deep learning model, which source(s) of uncertainty are you dealing with?</em></p>
<ol type="1">
<li>The inherent variability of the data-generating process <span class="math inline">\rightarrow</span> aleatoric uncertainty.</li>
<li>Parameter error <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
<li>Model error <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
<li>Data uncertainty <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
</ol>
</section></section>
<section>
<section id="avoiding-overfitting" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Avoiding Overfitting</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="traditional-regularisation" class="slide level2">
<h2>Traditional regularisation</h2>
<p>Say all the <span class="math inline">m</span> weights (excluding biases) are in the vector <span class="math inline">\boldsymbol{\theta}</span>. If we change the loss function to <span class="math display">
\text{Loss}_{1:n}
= \frac{1}{n} \sum_{i=1}^n \text{Loss}_i
  + \lambda \sum_{j=1}^{m} \left| \theta_j \right|
</span></p>
<p>this would be using <span class="math inline">L^1</span> regularisation. A loss like</p>
<p><span class="math display">
\text{Loss}_{1:n}
= \frac{1}{n} \sum_{i=1}^n \text{Loss}_i
  + \lambda \sum_{j=1}^{m} \theta_j^2
</span></p>
<p>is called <span class="math inline">L^2</span> regularisation.</p>
</section>
<section id="regularisation-in-keras" class="slide level2">
<h2>Regularisation in Keras</h2>
<div id="4190cd45" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb47-2"><a></a>features, target <span class="op">=</span> fetch_california_housing(as_frame<span class="op">=</span><span class="va">True</span>, return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-3"><a></a></span>
<span id="cb47-4"><a></a>NUM_FEATURES <span class="op">=</span> <span class="bu">len</span>(features.columns)</span>
<span id="cb47-5"><a></a></span>
<span id="cb47-6"><a></a>X_main, X_test, y_main, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb47-7"><a></a>    features, target, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb47-8"><a></a>)</span>
<span id="cb47-9"><a></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb47-10"><a></a>    X_main, y_main, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb47-11"><a></a>)</span>
<span id="cb47-12"><a></a></span>
<span id="cb47-13"><a></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb47-14"><a></a>X_train_sc <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb47-15"><a></a>X_val_sc <span class="op">=</span> scaler.transform(X_val)</span>
<span id="cb47-16"><a></a>X_test_sc <span class="op">=</span> scaler.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e1372f9c" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a></a><span class="im">from</span> keras.regularizers <span class="im">import</span> L1, L2</span>
<span id="cb48-2"><a></a></span>
<span id="cb48-3"><a></a><span class="kw">def</span> l1_model(regulariser_strength<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb48-4"><a></a>  random.seed(<span class="dv">123</span>)</span>
<span id="cb48-5"><a></a>  model <span class="op">=</span> Sequential([</span>
<span id="cb48-6"><a></a>      Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>,</span>
<span id="cb48-7"><a></a>        kernel_regularizer<span class="op">=</span>L1(regulariser_strength)),</span>
<span id="cb48-8"><a></a>      Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb48-9"><a></a>  ])</span>
<span id="cb48-10"><a></a></span>
<span id="cb48-11"><a></a>  model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb48-12"><a></a>  model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">4</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb48-13"><a></a>  <span class="cf">return</span> model</span>
<span id="cb48-14"><a></a></span>
<span id="cb48-15"><a></a><span class="kw">def</span> l2_model(regulariser_strength<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb48-16"><a></a>  random.seed(<span class="dv">123</span>)</span>
<span id="cb48-17"><a></a>  model <span class="op">=</span> Sequential([</span>
<span id="cb48-18"><a></a>      Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>,</span>
<span id="cb48-19"><a></a>        kernel_regularizer<span class="op">=</span>L2(regulariser_strength)),</span>
<span id="cb48-20"><a></a>      Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb48-21"><a></a>  ])</span>
<span id="cb48-22"><a></a></span>
<span id="cb48-23"><a></a>  model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb48-24"><a></a>  model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">10</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb48-25"><a></a>  <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="weights-before-after-l2" class="slide level2">
<h2>Weights before &amp; after <span class="math inline">L^2</span></h2>
<div class="columns">
<div class="column">
<div id="1e5a473e" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a></a>model <span class="op">=</span> l2_model(<span class="fl">0.0</span>)</span>
<span id="cb49-2"><a></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb49-3"><a></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb49-4"><a></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-51-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="6e7ea059" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a></a>model <span class="op">=</span> l2_model(<span class="fl">1.0</span>)</span>
<span id="cb51-2"><a></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb51-3"><a></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb51-4"><a></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-52-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="weights-before-after-l1" class="slide level2">
<h2>Weights before &amp; after <span class="math inline">L^1</span></h2>
<div class="columns">
<div class="column">
<div id="1e8bb0a2" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a>model <span class="op">=</span> l1_model(<span class="fl">0.0</span>)</span>
<span id="cb53-2"><a></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb53-3"><a></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-4"><a></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-53-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="6b6802ef" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a></a>model <span class="op">=</span> l1_model(<span class="fl">1.0</span>)</span>
<span id="cb55-2"><a></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb55-3"><a></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb55-4"><a></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 36</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-54-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="early-stopping-regularisation" class="slide level2">
<h2>Early-stopping regularisation</h2>
<blockquote>
<p>A very different way to regularize iterative learning algorithms such as gradient descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping… It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch”.</p>
</blockquote>
<blockquote>
<p>Alternatively, you can try building a model with slightly more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting too much. Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.</p>
</blockquote>
<div class="footer">
<p>Source: Géron (2019), Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition, Chapters 4 and 10.</p>
</div>
</section></section>
<section>
<section id="dropout" class="title-slide slide level1 agenda-slide center">
<h1>Dropout</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="dropout-1" class="slide level2">
<h2>Dropout</h2>

<img data-src="dropout.png" class="r-stretch quarto-figure-center"><p class="caption">An example of neurons dropped during training.</p><div class="footer">
<p>Sources: Marcus Lautier (2022).</p>
</div>
</section>
<section id="dropout-quote-1" class="slide level2">
<h2>Dropout quote #1</h2>
<blockquote>
<p>It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them.</p>
</blockquote>
<div class="footer">
<p>Source: Aurélien Géron (2019), <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, p.&nbsp;366</p>
</div>
</section>
<section id="dropout-quote-2" class="slide level2">
<h2>Dropout quote #2</h2>
<blockquote>
<p>The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.</p>
</blockquote>
<div class="footer">
<p>Source: Aurélien Géron (2019), <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, p.&nbsp;366</p>
</div>
</section>
<section id="code-dropout" class="slide level2">
<h2>Code: Dropout</h2>
<p>Dropout is just another layer in Keras.</p>
<div id="49992806" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a></a><span class="im">from</span> keras.layers <span class="im">import</span> Dropout</span>
<span id="cb57-2"><a></a></span>
<span id="cb57-3"><a></a>random.seed(<span class="dv">2</span>)<span class="op">;</span> </span>
<span id="cb57-4"><a></a></span>
<span id="cb57-5"><a></a>model <span class="op">=</span> Sequential([</span>
<span id="cb57-6"><a></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb57-7"><a></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb57-8"><a></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb57-9"><a></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb57-10"><a></a>    Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb57-11"><a></a>])</span>
<span id="cb57-12"><a></a></span>
<span id="cb57-13"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb57-14"><a></a>model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">4</span>, verbose<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="code-dropout-after-training" class="slide level2">
<h2>Code: Dropout after training</h2>
<p>Making predictions is the same as any other model:</p>
<div class="columns">
<div class="column">
<div id="9582c005" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a></a>model.predict(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb58-2"><a></a>                  verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>array([[1.0587903],
       [1.2814349],
       [0.9994641]], dtype=float32)</code></pre>
</div>
</div>
</div><div class="column">
<div id="c26dccfa" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a></a>model.predict(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb60-2"><a></a>                  verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>array([[1.0587903],
       [1.2814349],
       [0.9994641]], dtype=float32)</code></pre>
</div>
</div>
</div>
</div>
<p>We can make the model think it is still training:</p>
<div class="columns">
<div class="column">
<div id="6201b811" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a></a>model(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb62-2"><a></a>    training<span class="op">=</span><span class="va">True</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([[1.082524  ],
       [0.74211466],
       [1.1583111 ]], dtype=float32)</code></pre>
</div>
</div>
</div><div class="column">
<div id="60699403" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a></a>model(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb64-2"><a></a>    training<span class="op">=</span><span class="va">True</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>array([[1.0132376],
       [1.2697867],
       [0.7800578]], dtype=float32)</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="dropout-limitation" class="slide level2">
<h2>Dropout Limitation</h2>
<ul>
<li>Increased Training Time: Since dropout introduces noise into the training process, it can make the training process slower.</li>
<li>Sensitivity to Dropout Rates: the performance of dropout is highly dependent on the chosen dropout rate.</li>
</ul>
</section>
<section id="accidental-dropout-dead-neurons" class="slide level2">
<h2>Accidental dropout (“dead neurons”)</h2>
<p>My first ANN for California housing</p>
<div class="columns">
<div class="column">
<p><br></p>
<div id="29403b44" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb66-2"><a></a></span>
<span id="cb66-3"><a></a>model <span class="op">=</span> Sequential([</span>
<span id="cb66-4"><a></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb66-5"><a></a>    Dense(<span class="dv">1</span>)</span>
<span id="cb66-6"><a></a>])</span>
<span id="cb66-7"><a></a></span>
<span id="cb66-8"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb66-9"><a></a>hist <span class="op">=</span> model.fit(X_train, y_train,</span>
<span id="cb66-10"><a></a>        epochs<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb66-11"><a></a>hist.history[<span class="st">"loss"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>[25089.478515625,
 12.956840515136719,
 13.395628929138184,
 7.074816703796387,
 5.800348281860352]</code></pre>
</div>
</div>
</div><div class="column">
<div id="a6c36369" class="cell" data-execution_count="61">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="distributional-regression_files/figure-revealjs/cell-62-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="find-dead-relu-neurons" class="slide level2">
<h2>Find dead ReLU neurons</h2>
<div id="3ecd18ae" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a></a>acts <span class="op">=</span> model.layers[<span class="dv">0</span>](X_train).numpy()</span>
<span id="cb68-2"><a></a><span class="bu">print</span>(X_train.shape, acts.shape)</span>
<span id="cb68-3"><a></a>acts[:<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(12384, 8) (12384, 30)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>array([[261.45804 , 502.33704 ,  93.642815, ..., 537.5486  , 325.7366  ,
        398.9944  ],
       [ 18.983934,  52.906696,   0.      , ...,  28.361092,  10.988863,
         58.194595],
       [266.2954  , 517.58154 ,  98.64307 , ..., 553.68    , 336.69986 ,
        411.61124 ]], dtype=float32)</code></pre>
</div>
</div>
<div id="f5b3e85b" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a></a>dead <span class="op">=</span> acts.mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb71-2"><a></a>np.sum(dead)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>7</code></pre>
</div>
</div>
<div id="c4a183ad" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a></a>idx <span class="op">=</span> np.where(dead)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb73-2"><a></a>acts[:, idx<span class="op">-</span><span class="dv">1</span>:idx<span class="op">+</span><span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>array([[ 0.     ,  0.     ,  0.     ],
       [18.99187,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       ...,
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ],
       [ 0.     ,  0.     ,  0.     ]], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="trying-different-seeds" class="slide level2">
<h2>Trying different seeds</h2>
<p>Create a function which counts the number of dead ReLU neurons in the first hidden layer for a given seed:</p>
<div id="86dd3817" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a></a><span class="kw">def</span> count_dead(seed):</span>
<span id="cb75-2"><a></a>    random.seed(seed)</span>
<span id="cb75-3"><a></a>    hidden <span class="op">=</span> Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb75-4"><a></a>    acts <span class="op">=</span> hidden(X_train).numpy()</span>
<span id="cb75-5"><a></a>    <span class="cf">return</span> np.sum(acts.mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">==</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we can try out different seeds:</p>
<div id="2e33f372" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a></a>num_dead <span class="op">=</span> [count_dead(seed) <span class="cf">for</span> seed <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1_000</span>)]</span>
<span id="cb76-2"><a></a>np.median(num_dead)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>5.0</code></pre>
</div>
</div>
</section>
<section id="look-at-distribution-of-dead-relus" class="slide level2">
<h2>Look at distribution of dead ReLUs</h2>
<div id="f812e49a" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a></a>labels, counts <span class="op">=</span> np.unique(num_dead, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-2"><a></a>plt.bar(labels, counts, align<span class="op">=</span><span class="st">'center'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="distributional-regression_files/figure-revealjs/cell-70-output-1.png" class="r-stretch"></section></section>
<section>
<section id="ensembles" class="title-slide slide level1 agenda-slide center">
<h1>Ensembles</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Introduction</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Traditional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Stochastic Forecasts</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>GLMs and Neural Networks</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Combined Actuarial Neural Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Mixture Density Network</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Metrics for Distributional Regression</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Aleatoric and Epistemic Uncertainty</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Avoiding Overfitting</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Dropout</p>
</div></li>
<li><div class="agenda-active">
<p>Ensembles</p>
</div></li>
</ul>
</div>
</section>
<section id="ensembles-1" class="slide level2">
<h2>Ensembles</h2>

<img data-src="ensemble.png" class="r-stretch quarto-figure-center"><p class="caption">Combine many models to get better predictions.</p><div class="footer">
<p>Source: Marcus Lautier (2022).</p>
</div>
</section>
<section id="deep-ensembles" class="slide level2">
<h2>Deep Ensembles</h2>
<p>Train <span class="math inline">M</span> neural networks with different random initial weights independently (even in parallel).</p>
<div id="2d72e9e4" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a></a><span class="kw">def</span> build_model(seed):</span>
<span id="cb79-2"><a></a>    random.seed(seed)</span>
<span id="cb79-3"><a></a>    model <span class="op">=</span> Sequential([</span>
<span id="cb79-4"><a></a>        Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb79-5"><a></a>        Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb79-6"><a></a>    ])</span>
<span id="cb79-7"><a></a>    model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb79-8"><a></a></span>
<span id="cb79-9"><a></a>    es <span class="op">=</span> EarlyStopping(restore_best_weights<span class="op">=</span><span class="va">True</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb79-10"><a></a>    model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb79-11"><a></a>        callbacks<span class="op">=</span>[es], validation_data<span class="op">=</span>(X_val_sc, y_val), verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb79-12"><a></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8dd13f8e" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a></a>M <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb80-2"><a></a>seeds <span class="op">=</span> <span class="bu">range</span>(M)</span>
<span id="cb80-3"><a></a>models <span class="op">=</span> []</span>
<span id="cb80-4"><a></a><span class="cf">for</span> seed <span class="kw">in</span> seeds:</span>
<span id="cb80-5"><a></a>    models.append(build_model(seed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="deep-ensembles-ii" class="slide level2">
<h2>Deep Ensembles II</h2>
<p>Say the trained weights by <span class="math inline">\boldsymbol{w}^{(1)}, \ldots, \boldsymbol{w}^{(M)}</span>, then we get predictions <span class="math inline">\bigl\{ \hat{y}(\boldsymbol{x}; \boldsymbol{w}^{(m)}) \bigr\}_{m=1}^{M}</span></p>
<div id="82748dbf" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a></a>y_preds <span class="op">=</span> []</span>
<span id="cb81-2"><a></a><span class="cf">for</span> model <span class="kw">in</span> models:</span>
<span id="cb81-3"><a></a>    y_preds.append(model.predict(X_test_sc, verbose<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb81-4"><a></a></span>
<span id="cb81-5"><a></a>y_preds <span class="op">=</span> np.array(y_preds)</span>
<span id="cb81-6"><a></a>y_preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>array([[[3.2801466 ],
        [0.76298356],
        [2.4068608 ],
        ...,
        [2.3385763 ],
        [2.1730225 ],
        [1.096715  ]],

       [[3.1832185 ],
        [0.72296774],
        [2.5727806 ],
        ...,
        [2.3812106 ],
        [2.27971   ],
        [1.06247   ]],

       [[3.0994337 ],
        [0.77855957],
        [2.6037261 ],
        ...,
        [2.5923505 ],
        [2.354589  ],
        [1.2019893 ]]], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="package-versions" class="slide level2 appendix" data-visibility="uncounted">
<h2>Package Versions</h2>
<div id="8ec903ff" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a></a><span class="im">from</span> watermark <span class="im">import</span> watermark</span>
<span id="cb83-2"><a></a><span class="bu">print</span>(watermark(python<span class="op">=</span><span class="va">True</span>, packages<span class="op">=</span><span class="st">"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tensorflow_probability,tf_keras"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Python implementation: CPython
Python version       : 3.11.11
IPython version      : 8.32.0

keras                 : 3.8.0
matplotlib            : 3.10.0
numpy                 : 1.26.4
pandas                : 2.2.3
seaborn               : 0.13.2
scipy                 : 1.13.1
torch                 : 2.5.1+cu124
tensorflow            : 2.18.0
tensorflow_probability: 0.25.0
tf_keras              : 2.18.0
</code></pre>
</div>
</div>
</section>
<section id="glossary" class="slide level2 appendix" data-visibility="uncounted">
<h2>Glossary</h2>
<div class="columns">
<div class="column">
<ul>
<li>aleatoric and epistemic uncertainty</li>
<li>combined actuarial neural network</li>
<li>dead ReLU</li>
<li>deep ensembles</li>
<li>distributional forecasts</li>
<li>dropout</li>
</ul>
</div><div class="column">
<ul>
<li>generalised linear model</li>
<li>mixture density network</li>
<li>mixture distribution</li>
<li>Monte Carlo dropout</li>
<li>proper scoring rule</li>
</ul>
</div>
</div>

<div class="quarto-auto-generated-content">
<p><img src="../unsw-logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":4,"grid":false,"background":["rgba(255,255,255,0.0)","https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"]},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1000,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
            // Handle positioning of the toggle
        window.addEventListener(
          "resize",
          throttle(() => {
            elRect = undefined;
            if (selectedAnnoteEl) {
              selectCodeLines(selectedAnnoteEl);
            }
          }, 10)
        );
        function throttle(fn, ms) {
        let throttle = false;
        let timer;
          return (...args) => {
            if(!throttle) { // first call gets through
                fn.apply(this, args);
                throttle = true;
            } else { // all the others get throttled
                if(timer) clearTimeout(timer); // cancel #2
                timer = setTimeout(() => {
                  fn.apply(this, args);
                  timer = throttle = false;
                }, ms);
            }
          };
        }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>