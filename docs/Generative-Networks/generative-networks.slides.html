<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-contrib/reveal-auto-agenda-0.0.3/reveal-auto-agenda.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.557">

  <meta name="author" content="Patrick Laub">
  <title>AI for Actuaries - Generative Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {  background-color: #e9ecef; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #0040ff; font-weight: bold; } /* Alert */
    code span.an { color: #4090c0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #336699; } /* Attribute */
    code span.bn { color: #0033a0; } /* BaseN */
    code span.bu { color: #0055cc; } /* BuiltIn */
    code span.cf { color: #0055cc; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #002080; } /* Constant */
    code span.co { color: #4090c0; font-style: italic; } /* Comment */
    code span.cv { color: #4090c0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #3060a0; font-style: italic; } /* Documentation */
    code span.dt { color: #0044bb; } /* DataType */
    code span.dv { color: #0033a0; } /* DecVal */
    code span.er { color: #0040ff; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #0033a0; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #0055cc; font-weight: bold; } /* Import */
    code span.in { color: #4090c0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #0055cc; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #0055cc; } /* Other */
    code span.pp { color: #336699; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #336699; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #4090c0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
</head>
<body class="quarto-light">
<script>
  let selectedAnnoteEl;
</script>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Generative Networks</h1>
  <p class="subtitle">ACTL3143 &amp; ACTL5111 Deep Learning for Actuaries</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Patrick Laub 
</div>
</div>
</div>

</section>
<section>
<section id="text-generation" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Text Generation</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="generative-deep-learning" class="slide level2">
<h2>Generative deep learning</h2>
<ul>
<li>Using AI as augmented intelligence rather than artificial intelligence.</li>
<li>Use of deep learning to augment creative activities such as writing, music and art, to <em>generate</em> new things.</li>
<li>Some applications: text generation, deep dreaming, neural style transfer, variational autoencoders and generative adversarial networks.</li>
</ul>
</section>
<section id="text-generation-1" class="slide level2">
<h2>Text generation</h2>
<blockquote>
<p>Generating sequential data is the closest computers get to dreaming.</p>
</blockquote>
<ul>
<li>Generate sequence data: Train a model to predict the next token or next few tokens in a sentence, using previous tokens as input.</li>
<li>A network that models the probability of the next tokens given the previous ones is called a <em>language model</em>.</li>
</ul>
<aside class="notes">
<p>GPT-3 is a 175 billion parameter text-generation model trained by the startup OpenAI on a large text corpus of digitally available books, Wikipedia and web crawling. GPT-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Source: Alex Graves (2013), <a href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a></p>
</div>
</section>
<section id="word-level-language-model" class="slide level2">
<h2>Word-level language model</h2>

<img data-src="word-level-language-model.png" class="r-stretch quarto-figure-center"><p class="caption">Diagram of a word-level language model.</p><div class="footer">
<p>Source: Marcus Lautier (2022).</p>
</div>
</section>
<section id="character-level-language-model" class="slide level2">
<h2>Character-level language model</h2>

<img data-src="tensorflow-text_generation_sampling.png" class="r-stretch quarto-figure-center"><p class="caption">Diagram of a character-level language model (Char-RNN)</p><div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="useful-for-speech-recognition" class="slide level2">
<h2>Useful for speech recognition</h2>
<div id="fig-speech-recognition" class="quarto-figure quarto-figure-center quarto-float">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-speech-recognition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>RNN output</th>
<th>Decoded Transcription</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>what is the weather like in bostin right now</td>
<td>what is the weather like in boston right now</td>
</tr>
<tr class="even">
<td>prime miniter nerenr modi</td>
<td>prime minister narendra modi</td>
</tr>
<tr class="odd">
<td>arther n tickets for the game</td>
<td>are there any tickets for the game</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speech-recognition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Examples of transcriptions directly from the RNN with errors that are fixed by addition of a language model.
</figcaption>
</figure>
</div>
<div class="footer">
<p>Source: Hannun et al.&nbsp;(2014), <a href="https://arxiv.org/pdf/1412.5567.pdf">Deep Speech: Scaling up end-to-end speech recognition</a>, arXiv:1412.5567, Table 1.</p>
</div>
</section>
<section id="generating-shakespeare-i" class="slide level2">
<h2>Generating Shakespeare I</h2>
<blockquote>
<div class="line-block">ROMEO:<br>
Why, sir, what think you, sir?<br>
<br>
AUTOLYCUS:<br>
A dozen; shall I be deceased.<br>
The enemy is parting with your general,<br>
As bias should still combit them offend<br>
That Montague is as devotions that did satisfied;<br>
But not they are put your pleasure.</div>
</blockquote>
<div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="generating-shakespeare-ii" class="slide level2" data-visibility="uncounted">
<h2>Generating Shakespeare II</h2>
<blockquote>
<div class="line-block">DUKE OF YORK:<br>
Peace, sing! do you must be all the law;<br>
And overmuting Mercutio slain;<br>
And stand betide that blows which wretched shame;<br>
Which, I, that have been complaints me older hours.<br>
<br>
LUCENTIO:<br>
What, marry, may shame, the forish priest-lay estimest you, sir,<br>
Whom I will purchase with green limits o’ the commons’ ears!</div>
</blockquote>
<div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="generating-shakespeare-iii" class="slide level2" data-visibility="uncounted">
<h2>Generating Shakespeare III</h2>
<blockquote>
<div class="line-block">ANTIGONUS:<br>
To be by oath enjoin’d to this. Farewell!<br>
The day frowns more and more: thou’rt like to have<br>
A lullaby too rough: I never saw<br>
The heavens so dim by day. A savage clamour!<br>
<br>
[Exit, pursued by a bear]</div>
</blockquote>
</section></section>
<section>
<section id="sampling-strategy" class="title-slide slide level1 agenda-slide center">
<h1>Sampling strategy</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="sampling-strategy-1" class="slide level2">
<h2>Sampling strategy</h2>
<ul>
<li><em>Greedy sampling</em> will choose the token with the highest probability. It makes the resulting sentence repetitive and predictable.</li>
<li><em>Stochastic sampling</em>: if a word has probability 0.3 of being next in the sentence according to the model, we’ll choose it 30% of the time. But the result is still not interesting enough and still quite predictable.</li>
<li>Use a <em>softmax temperature</em> to control the randomness. More randomness results in more surprising and creative sentences.</li>
</ul>
</section>
<section id="softmax-temperature" class="slide level2">
<h2>Softmax temperature</h2>
<ul>
<li>The softmax temperature is a parameter that controls the randomness of the next token.</li>
<li>The formula is: <span class="math display"> \text{softmax}_\text{temperature}(x) = \frac{\exp(x / \text{temperature})}{\sum_i \exp(x_i / \text{temperature})} </span></li>
</ul>
</section>
<section id="i-am-a" class="slide level2">
<h2>“I am a” …</h2>

<img data-src="generative-networks_files/figure-revealjs/cell-4-output-1.png" class="r-stretch"><div class="footer">
<p>Idea inspired by Mehta (2023), <a href="https://shivammehta25.github.io/posts/temperature-in-language-models-open-ai-whisper-probabilistic-machine-learning/">The need for sampling temperature and differences between whisper, GPT-3, and probabilistic model’s temperature</a></p>
</div>
</section>
<section id="generating-laub-temp-0.01" class="slide level2">
<h2>Generating Laub (temp = 0.01)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> be different situation. So, next one is what they rective that each commit to be able to learn some relationships from the course, and that is part of the image that it’s very clese and black problems that you’re trying to fit the neural network to do there instead of like a specific though shef series of layers mean about full of the chosen the baseline of car was in the right, but that’s an important facts and it’s a very small summary with very scrort by the beginning of the sentence.</p>
</blockquote>
</section>
<section id="generating-laub-temp-0.25" class="slide level2" data-visibility="uncounted">
<h2>Generating Laub (temp = 0.25)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> decreas before model that we that we have to think about it, this mightsks better, for chattely the same project, because you might use the test set because it’s to be picked up the things that I wanted to heard of things that I like that even real you and you’re using the same thing again now because we need to understand what it’s doing the same thing but instead of putting it in particular week, and we can say that’s a thing I mainly link it’s three columns.</p>
</blockquote>
</section>
<section id="generating-laub-temp-0.5" class="slide level2" data-visibility="uncounted">
<h2>Generating Laub (temp = 0.5)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> probably the adw n wait lots of ngobs teulagedation to calculate the gradient and then I’ll be less than one layer the next slide will br input over and over the threshow you ampaigey the one that we want to apply them quickly. So, here this is the screen here the main top kecw onct three thing to told them, and the output is a vertical variables and Marceparase of things that you’re moving the blurring and that just data set is to maybe kind of categorical variants here but there’s more efficiently not basically replace that with respect to the best and be the same thing.</p>
</blockquote>
</section>
<section id="generating-laub-temp-1" class="slide level2" data-visibility="uncounted">
<h2>Generating Laub (temp = 1)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> put it different shates to touch on last week, so I want to ask what are you object frod current. They don’t have any zero into it, things like that which mistakes. 10 claims that the average version was relden distever ditgs and Python for the whole term wo long right to really. The name of these two options. There are in that seems to be modified version. If you look at when you’re putting numbers into your, that that’s over. And I went backwards, up, if they’rina functional pricing working with.</p>
</blockquote>
</section>
<section id="generating-laub-temp-1.5" class="slide level2" data-visibility="uncounted">
<h2>Generating Laub (temp = 1.5)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> put it could be bedinnth. Lowerstoriage nruron. So rochain the everything that I just sGiming. If there was a large. It’s gonua draltionation. Tow many, up, would that black and 53% that’s girter thankAty will get you jast typically stickK thing. But maybe. Anyway, I’m going to work on this libry two, past, at shit citcs jast pleming to memorize overcamples like pre pysing, why wareed to smart a one in this reportbryeccuriay.</p>
</blockquote>
</section>
<section id="copilots-conversation-style" class="slide level2">
<h2>Copilot’s “Conversation Style”</h2>

<img data-src="copilot-conversation-style.jpg" class="r-stretch quarto-figure-center"><p class="caption">This is (probably) just the ‘temperature’ knob under the hood.</p></section>
<section id="generate-the-most-likely-sequence" class="slide level2">
<h2>Generate the most likely sequence</h2>

<img data-src="chatbot.png" class="r-stretch quarto-figure-center"><p class="caption">An example sequence-to-sequence chatbot model.</p><div class="footer">
<p>Source: Payne (2021), <a href="https://www.width.ai/post/what-is-beam-search">What is beam search</a>, Width.ai blog.</p>
</div>
</section>
<section id="beam-search" class="slide level2">
<h2>Beam search</h2>

<img data-src="beam-search.png" class="r-stretch quarto-figure-center"><p class="caption">Illustration of a beam search.</p><div class="footer">
<p>Source: Doshi (2021), <a href="https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24">Foundations of NLP Explained Visually: Beam Search, How It Works</a>, towardsdatascience.com.</p>
</div>
</section></section>
<section>
<section id="transformers" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Transformers</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="transformer-architecture" class="slide level2">
<h2>Transformer architecture</h2>
<blockquote>
<p>GPT makes use of a mechanism known as attention, which removes the need for recurrent layers (e.g., LSTMs). It works like an information retrieval system, utilizing queries, keys, and values to decide how much information it wants to extract from each input token.</p>
<p>Attention heads can be grouped together to form what is known as a multihead attention layer. These are then wrapped up inside a Transformer block, which includes layer normalization and skip connections around the attention layer. Transformer blocks can be stacked to create very deep neural networks.</p>
</blockquote>
<p>Highly recommended viewing: Iulia Turk (2021), <a href="https://www.youtube.com/watch?v=LE3NfEULV6k"><em>Transfer learning and Transformer models</em></a>, ML Tech Talks.</p>
<div class="footer">
<p>Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O’Reilly Media, Chapter 9.</p>
</div>
</section>
<section id="transformers-package" class="slide level2">
<h2>🤗 Transformers package</h2>
<div id="754d40d9" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> transformers</span>
<span id="cb1-2"><a></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb1-3"><a></a>generator <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"gpt2"</span>, revision<span class="op">=</span><span class="st">"6c0e608"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bf068a2b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>transformers.set_seed(<span class="dv">1</span>)</span>
<span id="cb2-2"><a></a><span class="bu">print</span>(generator(<span class="st">"It's the holidays so I'm going to enjoy"</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>It's the holidays so I'm going to enjoy playing in there."

Advertisement

But how many other holiday-goers would want to join his team?


"They wouldn't know if I would be there, not that I'm</code></pre>
</div>
</div>
<div id="dcdde527" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a>transformers.set_seed(<span class="dv">2</span>)</span>
<span id="cb4-2"><a></a><span class="bu">print</span>(generator(<span class="st">"It's the holidays so I'm going to enjoy"</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>It's the holidays so I'm going to enjoy it. It's also a good holiday or we're going to go back and play soccer."

If Murgatroyd are to sign a deal with the club this summer, it is</code></pre>
</div>
</div>
</section>
<section id="reading-the-course-profile" class="slide level2">
<h2>Reading the course profile</h2>
<div id="9b9a0f46" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a>context <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb6-2"><a></a><span class="st">StoryWall Formative Discussions: An initial StoryWall, worth 2%, is due by noon on June 3. The following StoryWalls are worth 4</span><span class="sc">% e</span><span class="st">ach (taking the best 7 of 9) and are due at noon on the following dates:</span></span>
<span id="cb6-3"><a></a><span class="st">The project will be submitted in stages: draft due at noon on July 1 (10%), recorded presentation due at noon on July 22 (15%), final report due at noon on August 1 (15%).</span></span>
<span id="cb6-4"><a></a></span>
<span id="cb6-5"><a></a><span class="st">As a student at UNSW you are expected to display academic integrity in your work and interactions. Where a student breaches the UNSW Student Code with respect to academic integrity, the University may take disciplinary action under the Student Misconduct Procedure. To assure academic integrity, you may be required to demonstrate reasoning, research and the process of constructing work submitted for assessment.</span></span>
<span id="cb6-6"><a></a><span class="st">To assist you in understanding what academic integrity means, and how to ensure that you do comply with the UNSW Student Code, it is strongly recommended that you complete the Working with Academic Integrity module before submitting your first assessment task. It is a free, online self-paced Moodle module that should take about one hour to complete.</span></span>
<span id="cb6-7"><a></a></span>
<span id="cb6-8"><a></a><span class="st">StoryWall (30%)</span></span>
<span id="cb6-9"><a></a></span>
<span id="cb6-10"><a></a><span class="st">The StoryWall format will be used for small weekly questions. Each week of questions will be released on a Monday, and most of them will be due the following Monday at midday (see assessment table for exact dates). Students will upload their responses to the question sets, and give comments on another student's submission. Each week will be worth 4%, and the grading is pass/fail, with the best 7 of 9 being counted. The first week's basic 'introduction' StoryWall post is counted separately and is worth 2%.</span></span>
<span id="cb6-11"><a></a></span>
<span id="cb6-12"><a></a><span class="st">Project (40%)</span></span>
<span id="cb6-13"><a></a></span>
<span id="cb6-14"><a></a><span class="st">Over the term, students will complete an individual project. There will be a selection of deep learning topics to choose from (this will be outlined during Week 1).</span></span>
<span id="cb6-15"><a></a></span>
<span id="cb6-16"><a></a><span class="st">The deliverables for the project will include: a draft/progress report mid-way through the term, a presentation (recorded), a final report including a written summary of the project and the relevant Python code (Jupyter notebook).</span></span>
<span id="cb6-17"><a></a></span>
<span id="cb6-18"><a></a><span class="st">Exam (30%)</span></span>
<span id="cb6-19"><a></a></span>
<span id="cb6-20"><a></a><span class="st">The exam will test the concepts presented in the lectures. For example, students will be expected to: provide definitions for various deep learning terminology, suggest neural network designs to solve risk and actuarial problems, give advice to mock deep learning engineers whose projects have hit common roadblocks, find/explain common bugs in deep learning Python code.</span></span>
<span id="cb6-21"><a></a><span class="st">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="question-answering" class="slide level2">
<h2>Question answering</h2>
<div id="fc2e7c3e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a>qa <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>, model<span class="op">=</span><span class="st">"distilbert-base-cased-distilled-squad"</span>, revision<span class="op">=</span><span class="st">"626af31"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f4a1c2e1" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>qa(question<span class="op">=</span><span class="st">"What weight is the exam?"</span>, context<span class="op">=</span>context)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>{'score': 0.5019669532775879, 'start': 2092, 'end': 2095, 'answer': '30%'}</code></pre>
</div>
</div>
<div id="d2682840" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>qa(question<span class="op">=</span><span class="st">"What topics are in the exam?"</span>, context<span class="op">=</span>context)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>{'score': 0.21276050806045532,
 'start': 1778,
 'end': 1791,
 'answer': 'deep learning'}</code></pre>
</div>
</div>
<div id="94355289" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a>qa(question<span class="op">=</span><span class="st">"When is the presentation due?"</span>, context<span class="op">=</span>context)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>{'score': 0.5296478271484375,
 'start': 1319,
 'end': 1335,
 'answer': 'Monday at midday'}</code></pre>
</div>
</div>
<div id="5fd59bf4" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a>qa(question<span class="op">=</span><span class="st">"How many StoryWall tasks are there?"</span>, context<span class="op">=</span>context)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>{'score': 0.21391020715236664, 'start': 1155, 'end': 1158, 'answer': '30%'}</code></pre>
</div>
</div>
</section>
<section id="chatgpt-is-transformer-rlhf" class="slide level2">
<h2>ChatGPT is Transformer + RLHF</h2>
<blockquote>
<p>At the time of writing, there is no official paper that describes how ChatGPT works in detail, but from the official blog post we know that it uses a technique called reinforcement learning from human feedback (RLHF) to fine-tune the GPT-3.5 model.</p>
</blockquote>
<blockquote>
<p>While ChatGPT still has many limitations (such as sometimes “hallucinating” factually incorrect information), it is a powerful example of how Transformers can be used to build generative models that can produce complex, long-ranging, and novel output that is often indistinguishable from human-generated text. The progress made thus far by models like ChatGPT serves as a testament to the potential of AI and its transformative impact on the world.</p>
</blockquote>
<div class="footer">
<p>Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O’Reilly Media, Chapter 9.</p>
</div>
</section>
<section id="chatgpt-internals" class="slide level2">
<h2>ChatGPT internals</h2>

<img data-src="ChatGPT-Diagram.png" class="r-stretch quarto-figure-center"><p class="caption">It uses a fair bit of human feedback</p><div class="footer">
<p>Source: <a href="https://openai.com/blog/chatgpt">OpenAI blog</a>.</p>
</div>
</section>
<section id="recommended-reading" class="slide level2 smaller">
<h2>Recommended reading</h2>
<ul>
<li>The Verge (2022), <a href="https://www.theverge.com/c/23194235/ai-fiction-writing-amazon-kindle-sudowrite-jasper">The Great Fiction of AI: The strange world of high-speed semi-automated genre fiction</a></li>
<li>Vaswani et al.&nbsp;(2017), <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>, NeurIPS</li>
<li>Bommasani et al.&nbsp;(2021), <a href="https://arxiv.org/pdf/2108.07258.pdf">On the Opportunities and Risks of Foundation Models</a></li>
<li>Gary Marcus (2022), <a href="https://nautil.us/deep-learning-is-hitting-a-wall-14467/">Deep Learning Is Hitting a Wall</a>, Nautilus article</li>
<li>Super Data Science episode 564, <a href="https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000556643700">Clem Delangue on Hugging Face and Transformers</a></li>
<li>Super Data Science episode 559, <a href="https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000554847681">GPT-3 for Natural Language Processing</a></li>
<li>Computerphile (2019), <a href="https://youtu.be/rURRYI66E54">AI Language Models &amp; Transformers</a> (20m)</li>
<li>Computerphile (2020), <a href="https://youtu.be/_8yVOC4ciXc">GPT3: An Even Bigger Language Model</a> (25m)</li>
<li>Nicholas Renotte (2021), <a href="https://youtu.be/JctmnczWg0U">AI Blog Post Summarization with Hugging Face Transformers…</a> (33m)</li>
<li>Seattle Applied Deep Learning (2019), <a href="https://youtu.be/S27pHKBEp30">LSTM is dead. Long Live Transformers!</a> (28m)</li>
</ul>
</section></section>
<section>
<section id="image-generation" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Image Generation</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="reverse-engineering-a-cnn" class="slide level2">
<h2>Reverse-engineering a CNN</h2>
<p>A CNN is a function <span class="math inline">f_{\boldsymbol{\theta}}(\mathbf{x})</span> that takes a vector (image) <span class="math inline">\mathbf{x}</span> and returns a vector (distribution) <span class="math inline">\widehat{\mathbf{y}}</span>.</p>
<p>Normally, we train it by modifying <span class="math inline">\boldsymbol{\theta}</span> so that</p>
<p><span class="math display"> \boldsymbol{\theta}^*\ =\  \underset{\boldsymbol{\theta}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). </span></p>
<p>However, it is possible to <em>not train</em> the network but to modify <span class="math inline">\mathbf{x}</span>, like</p>
<p><span class="math display"> \mathbf{x}^*\ =\  \underset{\mathbf{x}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). </span></p>
<p>This is very slow as we do gradient descent every single time.</p>
</section>
<section id="adversarial-examples" class="slide level2">
<h2>Adversarial examples</h2>

<img data-src="adversarial-example.png" class="r-stretch quarto-figure-center"><p class="caption">A demonstration of fast adversarial example generation applied to GoogLeNet on ImageNet. By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet’s classification of the image.</p><div class="footer">
<p>Source: Goodfellow et al.&nbsp;(2015), <a href="https://arxiv.org/pdf/1412.6572.pdf">Explaining and Harnessing Adversarial Examples</a>, ICLR.</p>
</div>
</section>
<section id="adversarial-stickers" class="slide level2">
<h2>Adversarial stickers</h2>

<img data-src="the-verge-adversarial_patch_.0.gif" class="r-stretch quarto-figure-center"><p class="caption">Adversarial stickers.</p><div class="footer">
<p>Source: The Verge (2018), <a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google">These stickers make computer vision software hallucinate things that aren’t there</a>.</p>
</div>
</section>
<section id="adversarial-text" class="slide level2">
<h2>Adversarial text</h2>
<p>“<a href="https://github.com/QData/TextAttack">TextAttack</a> 🐙 is a Python framework for adversarial attacks, data augmentation, and model training in NLP”</p>

<img data-src="https://jxmo.io/files/textattack.gif" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">Demo</p></section>
<section id="deep-dream" class="slide level2">
<h2>Deep Dream</h2>

<img data-src="deep-dream.jpeg" class="r-stretch quarto-figure-center"><p class="caption">Deep Dream is an image-modification program released by Google in 2015.</p><div class="footer">
<p>Source: Wikipedia, <a href="https://commons.wikimedia.org/wiki/File:Aurelia-aurita-3-0009.jpg">DeepDream page</a>.</p>
</div>
</section>
<section id="deepdream" class="slide level2">
<h2>DeepDream</h2>
<ul>
<li>Even though many deep learning models are black boxes, convnets are quite interpretable via visualization. Some visualization techniques are: visualizing convnet outputs shows how convnet layers transform the input, visualizing convnet filters shows what visual patterns or concept each filter is receptive to, etc.</li>
<li>The activations of the first few layers of the network carries more information about the visual contents, while deeper layers encode higher, more abstract concepts.</li>
</ul>
</section>
<section id="deepdream-1" class="slide level2">
<h2>DeepDream</h2>
<ul>
<li>Each filter is receptive to a visual pattern. To visualize a convnet filter, gradient ascent is used to maximise the response of the filter. Gradient ascent maximize a loss function and moves the image in a direction that activate the filter more strongly to enhance its reading of the visual pattern.</li>
<li>DeepDream maximizes the activation of the entire convnet layer rather than that of a specific filter, thus mixing together many visual patterns all at once.</li>
<li>DeepDream starts with an existing image, latches on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.</li>
</ul>
</section>
<section id="original" class="slide level2">
<h2>Original</h2>

<img data-src="deep-dream-melbourne-original.jpg" class="r-stretch quarto-figure-center"><p class="caption">A sunny day on the Mornington peninsula.</p></section>
<section id="transformed" class="slide level2">
<h2>Transformed</h2>

<img data-src="deep-dream-melbourne.png" class="r-stretch quarto-figure-center"><p class="caption">Deep-dreaming version.</p><div class="footer">
<p>Generated by <a href="https://keras.io/examples/generative/deep_dream/">Keras’ Deep Dream tutorial</a>.</p>
</div>
</section></section>
<section>
<section id="neural-style-transfer" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Neural style transfer</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="neural-style-transfer-1" class="slide level2">
<h2>Neural style transfer</h2>
<p>Applying the style of a reference image to a target image while conserving the content of the target image.</p>

<img data-src="neuralstyletransfer.png" class="r-stretch quarto-figure-center"><p class="caption">An example neural style transfer.</p><aside class="notes">
<ul>
<li>Style: textures, colors, visual patterns (blue-and-yellow circular brushstrokes in Vincent Van Gogh’s Starry Night)</li>
<li>Content: the higher-level macrostructure of the image (buildings in the Tübingen photograph).</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.9.</p>
</div>
</section>
<section id="goal-of-nst" class="slide level2">
<h2>Goal of NST</h2>
<p>What the model does:</p>
<ul>
<li><p>Preserve content by maintaining similar deeper layer activations between the original image and the generated image. The convnet should “see” both the original image and the generated image as containing the same things.</p></li>
<li><p>Preserve style by maintaining similar correlations within activations for both low level layers and high-level layers. Feature correlations within a layer capture textures: the generated image and the style-reference image should share the same textures at different spatial scales.</p></li>
</ul>
</section>
<section id="a-wanderer-in-greenland" class="slide level2">
<h2>A wanderer in Greenland</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Content</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja.jpg"></p>
<figcaption>Some striking young hiker in Greenland.</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Style</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="wanderer.jpg"></p>
<figcaption><em>Wanderer above the Sea of Fog</em> by Caspar David Friedrich.</figcaption>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-wanderer-in-greenland-ii" class="slide level2">
<h2>A wanderer in Greenland II</h2>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja.gif"></p>
<figcaption>Animation of NST in progress.</figcaption>
</figure>
</div>
</div><div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja-wanderer.png"></p>
<figcaption>One result of NST.</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Question</strong></p>
</div>
<div class="callout-content">
<p>How would you make this faster for one specific style image?</p>
</div>
</div>
</div>
<div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-new-style-image" class="slide level2">
<h2>A new style image</h2>

<img data-src="wave.jpg" class="r-stretch quarto-figure-center"><p class="caption">Hokusai’s Great Wave off Kanagawa</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-new-content-image" class="slide level2">
<h2>A new content image</h2>

<img data-src="qingdao.jpg" class="r-stretch quarto-figure-center"><p class="caption">The seascape in Qingdao</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="another-neural-style-transfer" class="slide level2">
<h2>Another neural style transfer</h2>

<img data-src="qwave.jpg" class="r-stretch quarto-figure-center"><p class="caption">The seascape in Qingdao in the style of Hokusai’s Great Wave off Kanagawa</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="why-is-this-important" class="slide level2">
<h2>Why is this important?</h2>
<p>Taking derivatives with respect to the input image can be a first step toward explainable AI for convolutional networks.</p>
<ul>
<li><a href="https://youtu.be/y8cwyeccuy4">Saliency maps</a></li>
<li><a href="https://youtu.be/xGZfAoh0xKs">Grad-CAM</a></li>
</ul>
</section></section>
<section>
<section id="autoencoders" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Autoencoders</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="autoencoder" class="slide level2">
<h2>Autoencoder</h2>
<p>An autoencoder takes a data/image, maps it to a latent space via an encoder module, then decodes it back to an output with the same dimensions via a decoder module.</p>

<img data-src="autoencoder.png" class="r-stretch quarto-figure-center"><p class="caption">Schematic of an autoencoder.</p><div class="footer">
<p>Source: Marcus Lautier (2022).</p>
</div>
</section>
<section id="autoencoder-ii" class="slide level2">
<h2>Autoencoder II</h2>
<ul>
<li>An autoencoder is trained by using the same image as both the input and the target, meaning an autoencoder learns to reconstruct the original inputs. Therefore it’s <em>not supervised learning</em>, but <em>self-supervised learning</em>.</li>
<li>If we impose constraints on the encoders to be low-dimensional and sparse, <em>the input data will be compressed</em> into fewer bits of information.</li>
<li>Latent space is a place that stores low-dimensional representation of data. It can be used for <em>data compression</em>, where data is compressed to a point in a latent space.</li>
<li>An image can be compressed into a latent representation, which can then be reconstructed back to a <em>slightly different image</em>.</li>
</ul>
<aside class="notes">
<p>For image editing, an image can be projected onto a latent space and moved inside the latent space in a meaningful way (which means we modify its latent representation), before being mapped back to the image space. This will edit the image and allow us to generate images that have never been seen before.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-hand-written-characters" class="slide level2">
<h2>Example: Hand-written characters</h2>
<div id="f97dd71d" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Loading the Mandarin hand-written character dataset</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a><span class="co"># Download the dataset if it hasn't already been downloaded.</span></span>
<span id="cb16-2"><a></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb16-3"><a></a><span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"mandarin-split"</span>).exists():</span>
<span id="cb16-4"><a></a>    <span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"mandarin"</span>).exists():</span>
<span id="cb16-5"><a></a>        <span class="op">!</span>wget https:<span class="op">//</span>laub.au<span class="op">/</span>data<span class="op">/</span>mandarin.<span class="bu">zip</span></span>
<span id="cb16-6"><a></a>        <span class="op">!</span>unzip mandarin.<span class="bu">zip</span></span>
<span id="cb16-7"><a></a>    </span>
<span id="cb16-8"><a></a>    <span class="im">import</span> splitfolders</span>
<span id="cb16-9"><a></a>    splitfolders.ratio(<span class="st">"mandarin"</span>, output<span class="op">=</span><span class="st">"mandarin-split"</span>,</span>
<span id="cb16-10"><a></a>        seed<span class="op">=</span><span class="dv">1337</span>, ratio<span class="op">=</span>(<span class="dv">5</span><span class="op">/</span><span class="dv">7</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">7</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">7</span>))</span>
<span id="cb16-11"><a></a></span>
<span id="cb16-12"><a></a><span class="im">from</span> keras.utils <span class="im">import</span> image_dataset_from_directory</span>
<span id="cb16-13"><a></a></span>
<span id="cb16-14"><a></a>data_dir <span class="op">=</span> <span class="st">"mandarin-split"</span></span>
<span id="cb16-15"><a></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb16-16"><a></a>img_height <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb16-17"><a></a>img_width <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb16-18"><a></a>img_size <span class="op">=</span> (img_height, img_width)</span>
<span id="cb16-19"><a></a></span>
<span id="cb16-20"><a></a>train_ds <span class="op">=</span> image_dataset_from_directory(</span>
<span id="cb16-21"><a></a>    data_dir <span class="op">+</span> <span class="st">"/train"</span>,</span>
<span id="cb16-22"><a></a>    image_size<span class="op">=</span>img_size,</span>
<span id="cb16-23"><a></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb16-24"><a></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-25"><a></a>    color_mode<span class="op">=</span><span class="st">"grayscale"</span>)</span>
<span id="cb16-26"><a></a></span>
<span id="cb16-27"><a></a>val_ds <span class="op">=</span> image_dataset_from_directory(</span>
<span id="cb16-28"><a></a>    data_dir <span class="op">+</span> <span class="st">"/val"</span>,</span>
<span id="cb16-29"><a></a>    image_size<span class="op">=</span>img_size,</span>
<span id="cb16-30"><a></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb16-31"><a></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-32"><a></a>    color_mode<span class="op">=</span><span class="st">"grayscale"</span>)</span>
<span id="cb16-33"><a></a></span>
<span id="cb16-34"><a></a>test_ds <span class="op">=</span> image_dataset_from_directory(</span>
<span id="cb16-35"><a></a>    data_dir <span class="op">+</span> <span class="st">"/test"</span>,</span>
<span id="cb16-36"><a></a>    image_size<span class="op">=</span>img_size,</span>
<span id="cb16-37"><a></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb16-38"><a></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-39"><a></a>    color_mode<span class="op">=</span><span class="st">"grayscale"</span>)</span>
<span id="cb16-40"><a></a></span>
<span id="cb16-41"><a></a>X_train <span class="op">=</span> np.concatenate(<span class="bu">list</span>(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x))) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb16-42"><a></a>y_train <span class="op">=</span> np.concatenate(<span class="bu">list</span>(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: y)))</span>
<span id="cb16-43"><a></a></span>
<span id="cb16-44"><a></a>X_val <span class="op">=</span> np.concatenate(<span class="bu">list</span>(val_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x))) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb16-45"><a></a>y_val <span class="op">=</span> np.concatenate(<span class="bu">list</span>(val_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: y)))</span>
<span id="cb16-46"><a></a></span>
<span id="cb16-47"><a></a>X_test <span class="op">=</span> np.concatenate(<span class="bu">list</span>(test_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x))) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb16-48"><a></a>y_test <span class="op">=</span> np.concatenate(<span class="bu">list</span>(test_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: y)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="columns">
<div class="column">
<div id="c17bc16c" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>plt.imshow(X_train[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-15-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="ebc22d56" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a>plt.imshow(X_train[<span class="dv">80</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-16-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-compression-game" class="slide level2">
<h2>A compression game</h2>
<div class="columns">
<div class="column">
<div id="3cf6cce1" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>plt.imshow(X_train[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span>
<span id="cb19-2"><a></a><span class="bu">print</span>(img_width <span class="op">*</span> img_height)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6400</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-17-output-2.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div class="fragment">
<blockquote>
<p><em>A 4 with a curly foot, a flat line goes across the middle of the 4, two feet come off the bottom.</em></p>
</blockquote>
<p>96 characters</p>
</div>
<div class="fragment">
<blockquote>
<p><em>A Dōng character, rotated counterclockwise 15 degrees.</em></p>
</blockquote>
<p>54 characters</p>
</div>
</div>
</div>
</section>
<section id="make-a-basic-autoencoder" class="slide level2">
<h2>Make a basic autoencoder</h2>
<div id="2cd2752c" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>num_hidden_layer <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb21-2"><a></a><span class="bu">print</span>(<span class="ss">f"Compress from </span><span class="sc">{</span>img_height <span class="op">*</span> img_width<span class="sc">}</span><span class="ss"> pixels to </span><span class="sc">{</span>num_hidden_layer<span class="sc">}</span><span class="ss"> latent variables."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Compress from 6400 pixels to 400 latent variables.</code></pre>
</div>
</div>
<div id="75156830" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb23-2"><a></a></span>
<span id="cb23-3"><a></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb23-4"><a></a>    layers.Input((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb23-5"><a></a>    layers.Flatten(),</span>
<span id="cb23-6"><a></a>    layers.Dense(num_hidden_layer, <span class="st">"relu"</span>),</span>
<span id="cb23-7"><a></a>    layers.Dense(img_height<span class="op">*</span>img_width, <span class="st">"sigmoid"</span>),</span>
<span id="cb23-8"><a></a>    layers.Reshape((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb23-9"><a></a>])</span>
<span id="cb23-10"><a></a></span>
<span id="cb23-11"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"binary_crossentropy"</span>)</span>
<span id="cb23-12"><a></a>epochs <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb23-13"><a></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">15</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-14"><a></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb23-15"><a></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-model" class="slide level2">
<h2>The model</h2>
<div id="48d0b689" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">400</span>)            │     <span style="color: #00af00; text-decoration-color: #00af00">2,560,400</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │     <span style="color: #00af00; text-decoration-color: #00af00">2,566,400</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ reshape (<span style="color: #0087ff; text-decoration-color: #0087ff">Reshape</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">15,380,402</span> (58.67 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">5,126,800</span> (19.56 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">10,253,602</span> (39.11 MB)
</pre>
</div>
</div>
<div id="968f6862" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a></a>model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>0.20443250238895416</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image" class="slide level2">
<h2>Some recovered image</h2>
<div id="3071ceac" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a></a>X_val_rec <span class="op">=</span> model(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="f4baeba1" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-23-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="38e5bf5b" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-24-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="try-downscaling-the-images-a-bit-first-2x" class="slide level2">
<h2>Try downscaling the images a bit first (2x)</h2>
<div class="columns">
<div class="column">
<div id="933abd8e" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a></a><span class="co"># Plot an original image</span></span>
<span id="cb30-2"><a></a>plt.imshow(X_train[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-25-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="92e4333c" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a></a><span class="co"># Put an image through the MaxPooling2D layer and plot the result</span></span>
<span id="cb31-2"><a></a>downscale <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb31-3"><a></a>    layers.Input((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb31-4"><a></a>    layers.MaxPooling2D(<span class="dv">2</span>),</span>
<span id="cb31-5"><a></a>])</span>
<span id="cb31-6"><a></a>plt.imshow(downscale(X_train[[<span class="dv">0</span>]])[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-26-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<div id="650b1307" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb32-2"><a></a></span>
<span id="cb32-3"><a></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb32-4"><a></a>    layers.Input((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb32-5"><a></a>    layers.MaxPooling2D(<span class="dv">2</span>),</span>
<span id="cb32-6"><a></a>    layers.Flatten(),</span>
<span id="cb32-7"><a></a>    layers.Dense(num_hidden_layer, <span class="st">"relu"</span>),</span>
<span id="cb32-8"><a></a>    layers.Dense(img_height<span class="op">*</span>img_width, <span class="st">"sigmoid"</span>),</span>
<span id="cb32-9"><a></a>    layers.Reshape((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb32-10"><a></a>])</span>
<span id="cb32-11"><a></a></span>
<span id="cb32-12"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"binary_crossentropy"</span>)</span>
<span id="cb32-13"><a></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">15</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-14"><a></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-15"><a></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="3677acc2" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a></a>model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>0.20748917758464813</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image-1" class="slide level2">
<h2>Some recovered image</h2>
<div id="f33a4e5e" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a></a>X_val_rec <span class="op">=</span> model(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="125bbad9" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-30-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="1f370387" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-31-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="invert-the-images" class="slide level2">
<h2>Invert the images</h2>
<div class="columns">
<div class="column">
<div id="33ad2e6a" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a></a>plt.imshow(<span class="dv">1</span> <span class="op">-</span> X_train[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-32-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="1a2380c4" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a></a>plt.imshow(<span class="dv">1</span> <span class="op">-</span> X_train[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-33-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="slide level2">

<div id="4e666300" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb40-2"><a></a></span>
<span id="cb40-3"><a></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb40-4"><a></a>    layers.Input((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb40-5"><a></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb40-6"><a></a>    layers.Flatten(),</span>
<span id="cb40-7"><a></a>    layers.Dense(num_hidden_layer, <span class="st">"relu"</span>),</span>
<span id="cb40-8"><a></a>    layers.Dense(img_height<span class="op">*</span>img_width, <span class="st">"sigmoid"</span>),</span>
<span id="cb40-9"><a></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb40-10"><a></a>    layers.Reshape((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb40-11"><a></a>])</span>
<span id="cb40-12"><a></a></span>
<span id="cb40-13"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"binary_crossentropy"</span>)</span>
<span id="cb40-14"><a></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">15</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-15"><a></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb40-16"><a></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<div id="a02f13ff" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_3"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ lambda (<span style="color: #0087ff; text-decoration-color: #0087ff">Lambda</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">400</span>)            │     <span style="color: #00af00; text-decoration-color: #00af00">2,560,400</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │     <span style="color: #00af00; text-decoration-color: #00af00">2,566,400</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Lambda</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ reshape_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Reshape</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">15,380,402</span> (58.67 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">5,126,800</span> (19.56 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Optimizer params: </span><span style="color: #00af00; text-decoration-color: #00af00">10,253,602</span> (39.11 MB)
</pre>
</div>
</div>
<div id="4a215330" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a></a>model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0.20058368146419525</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image-2" class="slide level2">
<h2>Some recovered image</h2>
<div id="77b8bc60" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a></a>X_val_rec <span class="op">=</span> model(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="d185b168" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-38-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="8208c0d0" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-39-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="cnn-enhanced-encoder" class="slide level2">
<h2>CNN-enhanced encoder</h2>
<div id="3690d0a6" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb47-2"><a></a>encoder <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb47-3"><a></a>    layers.Input((img_height, img_width, <span class="dv">1</span>)),</span>
<span id="cb47-4"><a></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb47-5"><a></a>    layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb47-6"><a></a>    layers.MaxPooling2D(),</span>
<span id="cb47-7"><a></a>    layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb47-8"><a></a>    layers.MaxPooling2D(),</span>
<span id="cb47-9"><a></a>    layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb47-10"><a></a>    layers.MaxPooling2D(),</span>
<span id="cb47-11"><a></a>    layers.Flatten(),</span>
<span id="cb47-12"><a></a>    layers.Dense(num_hidden_layer, <span class="st">"relu"</span>)</span>
<span id="cb47-13"><a></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<div id="9e3cc3eb" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a></a>decoder <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb48-2"><a></a>    keras.Input(shape<span class="op">=</span>(num_hidden_layer,)),</span>
<span id="cb48-3"><a></a>    layers.Dense(<span class="dv">6400</span>),</span>
<span id="cb48-4"><a></a>    layers.Reshape((<span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">16</span>)),</span>
<span id="cb48-5"><a></a>    layers.Conv2D(<span class="dv">256</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb48-6"><a></a>    layers.UpSampling2D(),</span>
<span id="cb48-7"><a></a>    layers.Conv2D(<span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb48-8"><a></a>    layers.UpSampling2D(),   </span>
<span id="cb48-9"><a></a>    layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),                 </span>
<span id="cb48-10"><a></a>    layers.Conv2D(<span class="dv">1</span>, <span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb48-11"><a></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb48-12"><a></a>])</span>
<span id="cb48-13"><a></a>model <span class="op">=</span> keras.models.Sequential([encoder, decoder])</span>
<span id="cb48-14"><a></a>model.compile(<span class="st">"adam"</span>, <span class="st">"binary_crossentropy"</span>)</span>
<span id="cb48-15"><a></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">15</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-16"><a></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb48-17"><a></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<div id="f6f08ab5" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a></a>encoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_4"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ lambda_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Lambda</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │           <span style="color: #00af00; text-decoration-color: #00af00">160</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">4,640</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">18,496</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">400</span>)            │     <span style="color: #00af00; text-decoration-color: #00af00">2,560,400</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,583,696</span> (9.86 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,583,696</span> (9.86 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</section>
<section class="slide level2">

<div id="e628e9cd" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a></a>decoder.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_5"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6400</span>)           │     <span style="color: #00af00; text-decoration-color: #00af00">2,566,400</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ reshape_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Reshape</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">20</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)    │        <span style="color: #00af00; text-decoration-color: #00af00">37,120</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling2d (<span style="color: #0087ff; text-decoration-color: #0087ff">UpSampling2D</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">256</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">40</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │       <span style="color: #00af00; text-decoration-color: #00af00">295,040</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">UpSampling2D</span>)  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">73,792</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │            <span style="color: #00af00; text-decoration-color: #00af00">65</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Lambda</span>)               │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">80</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,972,417</span> (11.34 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,972,417</span> (11.34 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
<div id="ff8a8772" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a></a>model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>0.1909608691930771</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image-3" class="slide level2">
<h2>Some recovered image</h2>
<div id="9fff52a5" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a></a>X_val_rec <span class="op">=</span> model(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="2f1eead8" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-46-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="ebe209f0" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-47-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="some-recovered-image-4" class="slide level2" data-visibility="uncounted">
<h2>Some recovered image</h2>
<div id="9a0a3a54" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a></a>X_test_rec <span class="op">=</span> model(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="8c88f15d" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a></a>plt.imshow(X_test[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-49-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="483573db" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a></a>plt.imshow(X_test_rec[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-50-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="some-recovered-image-5" class="slide level2" data-visibility="uncounted">
<h2>Some recovered image</h2>
<div class="columns">
<div class="column">
<div id="443abfe7" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a></a>plt.imshow(X_test[<span class="dv">1</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-51-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="c1ef942b" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a></a>plt.imshow(X_test_rec[<span class="dv">1</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-52-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="latent-space-vs-word-embedding" class="slide level2 smaller">
<h2>Latent space vs word embedding</h2>
<ul>
<li>We revisit the concept of word embedding, where words in the vocabulary are mapped into vector representations. Words with similar meaning should lie close to one another in the word-embedding space.</li>
<li>Latent space contains low-dimensional representation of data. Data/Images that are similar should lie close in the latent space.</li>
<li>There are pre-trained word-embedding spaces such as those for English-language movie review, German-language legal documents, etc. Semantic relationships between words differ for different tasks. Similarly, the structure of latent spaces for different data sets (humans faces, animals, etc) are different.</li>
</ul>
</section>
<section id="latent-space-vs-word-embedding-1" class="slide level2">
<h2>Latent space vs word embedding</h2>
<ul>
<li>Given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data.</li>
<li>A <strong>concept vector</strong> is a direction of variation in the data. For example there may be a smile vector such that if <span class="math inline">z</span> is the latent representation of a face, then <span class="math inline">z+s</span> is the representation of the same face, smiling. We can generate an image of the person smiling from this latent representation.</li>
</ul>
</section>
<section id="intentionally-add-noise-to-inputs" class="slide level2">
<h2>Intentionally add noise to inputs</h2>
<div class="columns">
<div class="column">
<div id="9466aa88" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a></a>mask <span class="op">=</span> rnd.random(size<span class="op">=</span>X_train.shape[<span class="dv">1</span>:]) <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb61-2"><a></a>plt.imshow(mask <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> X_train[<span class="dv">0</span>]), cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-53-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="79e48d43" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a></a>mask <span class="op">=</span> rnd.random(size<span class="op">=</span>X_train.shape[<span class="dv">1</span>:]) <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb62-2"><a></a>plt.imshow(mask <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> X_train[<span class="dv">42</span>]) <span class="op">*</span> mask, cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="generative-networks_files/figure-revealjs/cell-54-output-1.png"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="denoising-autoencoder" class="slide level2">
<h2>Denoising autoencoder</h2>
<p>Can be used to do <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629">feature engineering for supervised learning problems</a></p>
<blockquote>
<p>It is also possible to include input variables as outputs to infer missing values or just help the model “understand” the features – in fact the winning solution of a claims prediction Kaggle competition heavily used denoising autoencoders together with model stacking and ensembling – read more here.</p>
</blockquote>
<p>Jacky Poon</p>
<div class="footer">
<p>Source: Poon (2021), <a href="https://actuariesinstitute.github.io/cookbook/docs/multitasking_risk_pricing.html"><em>Multitasking Risk Pricing Using Deep Learning</em></a>, Actuaries’ Analytical Cookbook.</p>
</div>
</section></section>
<section>
<section id="variational-autoencoders" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Variational Autoencoders</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-post-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="variational-autoencoder" class="slide level2">
<h2>Variational autoencoder</h2>
<aside class="notes">
<p>A slightly different sample from the distribution in the latent space will be decoded to a slightly different image. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representation everywhere: every point in the latent space is decoded to a valid output. So the latent spaces of VAEs are continuous and highly-structured.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<img data-src="chollet-VAE.png" class="r-stretch quarto-figure-center"><p class="caption">Schematic of a variational autoencoder.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.17.</p>
</div>
</section>
<section id="vae-schematic-process" class="slide level2">
<h2>VAE schematic process</h2>

<img data-src="chollet-VAEcode.png" class="r-stretch quarto-figure-center"><p class="caption">Keras code for a VAE.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Unnumbered listing in Chapter 12.</p>
</div>
</section>
<section id="focus-on-the-decoder" class="slide level2">
<h2>Focus on the decoder</h2>

<img data-src="chollet-latentspace.png" class="r-stretch quarto-figure-center"><p class="caption">Sampling new artificial images from the latent space.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.13.</p>
</div>
</section>
<section id="exploring-the-mnist-latent-space" class="slide level2">
<h2>Exploring the MNIST latent space</h2>

<img data-src="chollet-VAEdecoded.png" class="r-stretch quarto-figure-center"><p class="caption">Example of MNIST-like images generated from the latent space.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.18.</p>
</div>
</section>
<section id="recommended-viewing" class="slide level2">
<h2>Recommended Viewing</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/7Rb4s9wNOmc" width="1000px" height="600px" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section></section>
<section>
<section id="diffusion-models" class="title-slide slide level1 agenda-slide center" data-visibility="uncounted">
<h1>Diffusion Models</h1>
<div class="agenda-heading">
<p>Lecture Outline</p>
</div>
<div class="agenda">
<ul>
<li><div class="agenda-inactive agenda-pre-active">
<p>Text Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Sampling strategy</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Transformers</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Image Generation</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Neural style transfer</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Autoencoders</p>
</div></li>
<li><div class="agenda-inactive agenda-pre-active">
<p>Variational Autoencoders</p>
</div></li>
<li><div class="agenda-active">
<p>Diffusion Models</p>
</div></li>
</ul>
</div>
</section>
<section id="using-kerascv" class="slide level2">
<h2>Using KerasCV</h2>
<iframe data-external="1" src="https://www.youtube.com/embed/pstsh2C2roc" width="1000px" height="600px" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="package-versions" class="slide level2 appendix" data-visibility="uncounted">
<h2>Package Versions</h2>
<div id="84f9436b" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a></a><span class="im">from</span> watermark <span class="im">import</span> watermark</span>
<span id="cb63-2"><a></a><span class="bu">print</span>(watermark(python<span class="op">=</span><span class="va">True</span>, packages<span class="op">=</span><span class="st">"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Python implementation: CPython
Python version       : 3.11.11
IPython version      : 8.32.0

keras     : 3.8.0
matplotlib: 3.10.0
numpy     : 1.26.4
pandas    : 2.2.3
seaborn   : 0.13.2
scipy     : 1.13.1
torch     : 2.5.1+cu124
tensorflow: 2.18.0
tf_keras  : 2.18.0
</code></pre>
</div>
</div>
</section>
<section id="glossary" class="slide level2 appendix" data-visibility="uncounted">
<h2>Glossary</h2>
<div class="columns">
<div class="column">
<ul>
<li>autoencoder (variational)</li>
<li>beam search</li>
<li>bias</li>
<li>ChatGPT (&amp; RLHF)</li>
<li>DeepDream</li>
<li>greedy sampling</li>
</ul>
</div><div class="column">
<ul>
<li>HuggingFace</li>
<li>language model</li>
<li>latent space</li>
<li>neural style transfer</li>
<li>softmax temperature</li>
<li>stochastic sampling</li>
</ul>
</div>
</div>

<div class="quarto-auto-generated-content">
<p><img src="../unsw-logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":4,"grid":false,"background":["rgba(255,255,255,0.0)","https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"]},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1000,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
            // Handle positioning of the toggle
        window.addEventListener(
          "resize",
          throttle(() => {
            elRect = undefined;
            if (selectedAnnoteEl) {
              selectCodeLines(selectedAnnoteEl);
            }
          }, 10)
        );
        function throttle(fn, ms) {
        let throttle = false;
        let timer;
          return (...args) => {
            if(!throttle) { // first call gets through
                fn.apply(this, args);
                throttle = true;
            } else { // all the others get throttled
                if(timer) clearTimeout(timer); // cancel #2
                timer = setTimeout(() => {
                  fn.apply(this, args);
                  timer = throttle = false;
                }, ms);
            }
          };
        }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>