
# Dense Layers in Matrices {data-visibility="uncounted"}

## Logistic regression

```{python}
#| echo: false
set_square_figures()
```

::: columns
::: column

Observations: $\mathbf{x}_{i,\bullet} \in \mathbb{R}^{2}$.

Target: $y_i \in \{0, 1\}$.

Predict: $\hat{y}_i = \mathbb{P}(Y_i = 1)$.

<br>

__The model__

For $\mathbf{x}_{i,\bullet} = (x_{i,1}, x_{i,2})$:
$$
z_i = x_{i,1} w_1 + x_{i,2} w_2 + b
$$

$$
\hat{y}_i = \sigma(z_i) = \frac{1}{1 + \mathrm{e}^{-z_i}} .
$$

:::
::: column

```{python}
x = np.linspace(-10, 10, 100)
y = 1/(1 + np.exp(-x))
plt.plot(x, y);
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

## Multiple observations

```{python}
data = pd.DataFrame({"x_1": [1, 3, 5], "x_2": [2, 4, 6], "y": [0, 1, 1]})
data
```

Let $w_1 = 1$, $w_2 = 2$ and $b = -10$.

```{python}
w_1 = 1; w_2 = 2; b = -10
data["x_1"] * w_1 + data["x_2"] * w_2 + b 
```

## Matrix notation

::: columns
::: column
Have $\mathbf{X} \in \mathbb{R}^{3 \times 2}$.

```{python}
X_df = data[["x_1", "x_2"]]
X = X_df.to_numpy()
X
```
:::
::: column
Let $\mathbf{w} = (w_1, w_2)^\top \in \mathbb{R}^{2 \times 1}$.

```{python}
w = np.array([[1], [2]])
w
```
:::
:::

$$
\mathbf{z} = \mathbf{X} \mathbf{w} + b , \quad \mathbf{a} = \sigma(\mathbf{z})
$$

::: columns
::: column
```{python}
z = X.dot(w) + b
z
```
:::
::: column
```{python}
1 / (1 + np.exp(-z))
```
:::
:::

## Using a softmax output

::: columns
::: column
Observations: $\mathbf{x}_{i,\bullet} \in \mathbb{R}^{2}$.
Predict: $\hat{y}_{i,j} = \mathbb{P}(Y_i = j)$.
:::
::: column
Target: $\mathbf{y}_{i,\bullet} \in \{(1, 0), (0, 1)\}$.
:::
:::

__The model__: For $\mathbf{x}_{i,\bullet} = (x_{i,1}, x_{i,2})$
$$
\begin{aligned}
z_{i,1} &= x_{i,1} w_{1,1} + x_{i,2} w_{2,1} + b_1 , \\
z_{i,2} &= x_{i,1} w_{1,2} + x_{i,2} w_{2,2} + b_2 .
\end{aligned}
$$

$$
\begin{aligned}
\hat{y}_{i,1} &= \text{Softmax}_1(\mathbf{z}_i) = \frac{\mathrm{e}^{z_{i,1}}}{\mathrm{e}^{z_{i,1}} + \mathrm{e}^{z_{i,2}}} , \\
\hat{y}_{i,2} &= \text{Softmax}_2(\mathbf{z}_i) = \frac{\mathrm{e}^{z_{i,2}}}{\mathrm{e}^{z_{i,1}} + \mathrm{e}^{z_{i,2}}} .
\end{aligned}
$$

## Multiple observations

::: columns
::: column
```{python}
#| echo: false
data = pd.DataFrame({
  "x_1": [1, 3, 5], "x_2": [2, 4, 6],
  "y_1": [1, 0, 0], "y_2": [0, 1, 1]})
```

```{python}
data
```
:::
::: column
Choose:

$w_{1,1} = 1$, $w_{2,1} = 2$,

$w_{1,2} = 3$, $w_{2,2} = 4$, and

$b_1 = -10$, $b_2 = -20$.

:::
:::

```{python}
w_11 = 1; w_21 = 2; b_1 = -10
w_12 = 3; w_22 = 4; b_2 = -20
data["x_1"] * w_11 + data["x_2"] * w_21 + b_1
```

## Matrix notation

::: columns
::: column
Have $\mathbf{X} \in \mathbb{R}^{3 \times 2}$.

```{python}
X
```
:::
::: column
$\mathbf{W}\in \mathbb{R}^{2\times2}$, $\mathbf{b}\in \mathbb{R}^{2}$

```{python}
W = np.array([[1, 3], [2, 4]])
b = np.array([-10, -20])
display(W); b
```
:::
:::

$$
  \mathbf{Z} = \mathbf{X} \mathbf{W} + \mathbf{b} , \quad \mathbf{A} = \text{Softmax}(\mathbf{Z}) .
$$

::: columns
::: column
```{python}
Z = X @ W + b
Z
```
:::
::: column
```{python}
np.exp(Z) / np.sum(np.exp(Z),
  axis=1, keepdims=True)
```
:::
:::
