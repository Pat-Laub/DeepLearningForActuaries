---
title: Generative Networks
---

```{python}
#| echo: false
#| warning: false
import os
# os.environ["KERAS_BACKEND"] = "torch"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import torch
torch.set_num_threads(1)

import matplotlib
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

def square_fig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortest_side = min(xl[1], yl[1])
    matplotlib.pyplot.plot([0, shortest_side], [0, shortest_side], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 6

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import keras
keras.utils.set_random_seed(1)
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import random
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import numpy.random as rnd
import pandas as pd
import keras
from keras import layers
```

:::

# Text Generation {visibility="uncounted"}

## Generative deep learning

- Using AI as augmented intelligence rather than artificial intelligence.
- Use of deep learning to augment creative activities such as writing, music and art, to *generate* new things.
- Some applications: text generation, deep dreaming, neural style transfer, variational autoencoders and generative adversarial networks.

## Text generation

> Generating sequential data is the closest computers get to dreaming.

- Generate sequence data: Train a model to predict the next token or next few tokens in a sentence, using previous tokens as input.
- A network that models the probability of the next tokens given the previous ones is called a *language model*.

::: {.notes}
GPT-3 is a 175 billion parameter text-generation model trained by the startup OpenAI on a large text corpus of digitally available books, Wikipedia and web crawling. GPT-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic.
:::

::: footer
Source: Alex Graves (2013), [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)
:::

## Word-level language model

![Diagram of a word-level language model.](word-level-language-model.png)

::: footer
Source: Marcus Lautier (2022).
:::

::: {.content-visible unless-format="revealjs"}
The way how word-level language models work is that, it first takes in the input text and then generates the probability distribution of the next word. This distribution tells us how likely a certain word is to be the next word. Thereafter, the model implements a appropriate sampling strategy to select the next word. Once the next word is predicted, it is appended to the input text and then passed in to the model again to predict the next word. The idea here is to predict the word after word. 
:::

## Character-level language model

![Diagram of a character-level language model (Char-RNN)](tensorflow-text_generation_sampling.png)

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::

::: {.content-visible unless-format="revealjs"}
Character-level language predtics the next character given a certain input character. They capture patterns at a much granular level and do not aim to capture semantics of words.
:::

## Useful for speech recognition

::: {#fig-speech-recognition}

| RNN output |  Decoded Transcription |
| --- | --- |
| what is the weather like in bostin right now | what is the weather like in boston right now |
| prime miniter nerenr modi | prime minister narendra modi |
| arther n tickets for the game | are there any tickets for the game |

Examples of transcriptions directly from the RNN with errors that are fixed by addition of a language model.
:::

::: footer
Source: Hannun et al. (2014), [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/pdf/1412.5567.pdf), arXiv:1412.5567, Table 1.
:::

::: {.content-visible unless-format="revealjs"}
The above example shows how RNN predictions (for sequential data processing) can be improved by fixing errors using a language model.
:::
## Generating Shakespeare I

::: {.content-visible unless-format="revealjs"}
The following is an example how a language model trained on works of Shakespeare starts predicting words after we input a string. This is an example of a character-level prediction, where we aim to predict the most likely character, not the word.
:::

> | ROMEO:
| Why, sir, what think you, sir?
| 
| AUTOLYCUS:
| A dozen; shall I be deceased.
| The enemy is parting with your general,
| As bias should still combit them offend
| That Montague is as devotions that did satisfied;
| But not they are put your pleasure.

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::

## Generating Shakespeare II {data-visibility="uncounted"}

> | DUKE OF YORK:
| Peace, sing! do you must be all the law;
| And overmuting Mercutio slain;
| And stand betide that blows which wretched shame;
| Which, I, that have been complaints me older hours.
| 
| LUCENTIO:
| What, marry, may shame, the forish priest-lay estimest you, sir,
| Whom I will purchase with green limits o' the commons' ears!

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::


## Generating Shakespeare III {data-visibility="uncounted"}

> | ANTIGONUS: 
| To be by oath enjoin'd to this. Farewell! 
| The day frowns more and more: thou'rt like to have 
| A lullaby too rough: I never saw 
| The heavens so dim by day. A savage clamour! 
|
| [Exit, pursued by a bear]

# Sampling strategy

::: {.content-visible unless-format="revealjs"}
The sampling strategy refers to the way how we pick the next word/character as the prediction after observing the distribution. There are different sampling strategies and they aim to serve different levels of trade-offs between exploration and exploitation when generating text sequences. 
:::

- *Greedy sampling* will choose the token with the highest probability. It makes the resulting sentence repetitive and predictable.
- *Stochastic sampling*: if a word has probability 0.3 of being next in the sentence according to the model, we‚Äôll choose it 30% of the time. But the result is still not interesting enough and still quite predictable.
- Use a *softmax temperature* to control the randomness. More randomness results in more surprising and creative sentences.

## Softmax temperature

- The softmax temperature is a parameter that controls the randomness of the next token.
- The formula is: $$ \text{softmax}_\text{temperature}(x) = \frac{\exp(x / \text{temperature})}{\sum_i \exp(x_i / \text{temperature})} $$

## "I am a" ...

```{python}
#| echo: false
def softmax(x, temperature=1.0):
    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))

x = np.array([0.15, 0.5, 0.1, 0.25])
x /= np.sum(x)

# Plot bar charts for the original distributions and the softmax temperature versions.
plt.figure(figsize=(6, 3))

phrase = "I am a"
next_words = ["dog", "human", "robot", "teddy"] 

plt.subplot(2, 2, 1)
temperature = 0.1
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

plt.subplot(2, 2, 2)
temperature = 1
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

temperature = 10
plt.subplot(2, 2, 3)
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

plt.subplot(2, 2, 4)
temperature = 100
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

# Clean up the layout.
plt.tight_layout()

plt.show()
```
::: footer
Idea inspired by Mehta (2023), [The need for sampling temperature and differences between whisper, GPT-3, and probabilistic model's temperature](https://shivammehta25.github.io/posts/temperature-in-language-models-open-ai-whisper-probabilistic-machine-learning/)
:::

::: {.content-visible unless-format="revealjs"}
The graphical illustration above shows how the distribution of words change with different levels of `Temp` values. Higher levels of temperatures result in less predictable(more interesting) outcomes. If we continue to increase the `Temp` levels, after a certain point, outcomes will be picked completely at random. This predictions after this point might not be meaningful. Hence, attention to the trade-off between predictability and interestingness is important when deciding the `Temp` levels.
:::


::: {.content-visible unless-format="revealjs"}
The following sections show how a neural network turned on the same dataset, and given the same starting input string _In today's lecture we will_ shall generate very different sequences of text as predictions. `Temp=0.25` may give interesting outputs compared to `Temp=0.01` and `Temp=0.50` may give interesting outputs compared to `Temp=0.25`. However, when we keep on increasing `Temp` levels, the neural network starts giving out random(meaningless) outcomes.
:::

## Generating Laub (temp = 0.01)

> _In today's lecture we will_ be different situation.
> So, next one is what they rective that each commit to be able to learn some relationships from the course, and that is part of the image that it's very clese and black problems that you're trying to fit the neural network to do there instead of like a specific though shef series of layers mean about full of the chosen the baseline of car was in the right, but that's an important facts and it's a very small summary with very scrort by the beginning of the sentence.

## Generating Laub (temp = 0.25) {data-visibility="uncounted"}

> _In today's lecture we will_ decreas before model that we that we have to think about it, this mightsks better, for chattely the same project, because you might use the test set because it's to be picked up the things that I wanted to heard of things that I like that even real you and you're using the same thing again now because we need to understand what it's doing the same thing but instead of putting it in particular week, and we can say that's a thing I mainly link it's three columns.

## Generating Laub (temp = 0.5) {data-visibility="uncounted"}

> _In today's lecture we will_ probably the adw n wait lots of ngobs teulagedation to calculate the gradient and then I'll be less than one layer the next slide will br input over and over the threshow you ampaigey the one that we want to apply them quickly. So, here this is the screen here the main top kecw onct three thing to told them, and the output is a vertical variables and Marceparase of things that you're moving the blurring and that just data set is to maybe kind of categorical variants here but there's more efficiently not basically replace that with respect to the best and be the same thing.

## Generating Laub (temp = 1) {data-visibility="uncounted"}

> _In today's lecture we will_ put it different shates to touch on last week, so I want to ask what are you object frod current.
> They don't have any zero into it, things like that which mistakes. 10 claims that the average version was relden distever ditgs and Python for the whole term wo long right to really.
> The name of these two options.
> There are in that seems to be modified version. If you look at when you're putting numbers into your, that that's over.
> And I went backwards, up, if they'rina functional pricing working with.

## Generating Laub (temp = 1.5) {data-visibility="uncounted"}

> _In today's lecture we will_ put it could be bedinnth. Lowerstoriage nruron. So rochain the everything that I just sGiming.
> If there was a large. It's gonua draltionation.
> Tow many, up, would that black and 53% that's girter thankAty will get you jast typically stickK thing.
> But maybe. Anyway, I'm going to work on this libry two, past, at shit citcs jast
> pleming to memorize overcamples like pre pysing, why wareed to smart a one in this reportbryeccuriay.

## Generate the most likely sequence

::: {.content-visible unless-format="revealjs"}
Similar to other sequence generating tasks such as generating the next word or generating the next character, generating an entire sequence of words is also useful. The task involves generating the most likely sequence after observing model predictions. 
:::

![An example sequence-to-sequence chatbot model.](chatbot.png)

::: footer
Source: Payne (2021), [What is beam search](https://www.width.ai/post/what-is-beam-search), Width.ai blog.
:::

## Beam search

::: {.content-visible unless-format="revealjs"}
Instead of trying to carry forward only the highest probable prediction, beam search carries forward several high probable predictions, and then decide the highest probable combination of predictions. Beam search helps expand the exploration horizon for predictions which can contribute to more contextually relevant model predictions. However, this comes at a certain computational complexity.
:::

![Illustration of a beam search.](beam-search.png)

::: footer
Source: Doshi (2021), [Foundations of NLP Explained Visually: Beam Search, How It Works](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24), towardsdatascience.com.
:::

{{< include _hf-transformers.qmd >}}

# Image Generation {visibility="uncounted"}

## Reverse-engineering a CNN

::: {.content-visible unless-format="revealjs"}
Reverse engineering is a process where we manipulate the inputs _x_ while keeping the loss function and the model architecture the same. This is useful in understanding the inner workings of the model, especially when we do not have access to the model architecture or the original train dataset. The idea here is to tweak/distort the input feature data and observe how model predictions vary. This provides meaningful insights in to what patterns in the input data are most critical to making model predictions. 
:::

::: {.content-visible unless-format="revealjs"}
This task however requires computing the gradients of the model's outputs with respect to all input features, hence, can be time consuming. 
:::

A CNN is a function $f_{\boldsymbol{\theta}}(\mathbf{x})$ that takes a vector (image) $\mathbf{x}$ and returns a vector (distribution) $\widehat{\mathbf{y}}$.

Normally, we train it by modifying $\boldsymbol{\theta}$ so that 

$$ \boldsymbol{\theta}^*\ =\  \underset{\boldsymbol{\theta}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). $$

However, it is possible to _not train_ the network but to modify $\mathbf{x}$, like

$$ \mathbf{x}^*\ =\  \underset{\mathbf{x}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). $$

This is very slow as we do gradient descent every single time.

## Adversarial examples

::: {.content-visible unless-format="revealjs"}
An adversarial attack refers to a small carefully created modifications to the input data that aims to trick the model in to making wrong predictions while keeping the _y_true_ same. The goal is to identify instances where subtle modifications in the input data (which are not instantaneously recognized) can lead to erroneous model predictions.
:::

![A demonstration of fast adversarial example generation applied to GoogLeNet on ImageNet. By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet‚Äôs classification of the image.](adversarial-example.png)

::: footer
Source: Goodfellow et al. (2015), [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf), ICLR.
:::

::: {.content-visible unless-format="revealjs"}
The above example shows how a small perturbation to the image of a panda led to the model predicting the image as a gibbon with high confidence. This indicates that there may be certain patterns in the data which are not clearly seen by the human eye, but the model is relying on them to make predictions. Identifying these sensitivities/vulnerabilities are important to understand how a model is making its predictions.
:::
## Adversarial stickers

![Adversarial stickers.](the-verge-adversarial_patch_.0.gif)

::: footer
Source: The Verge (2018), [These stickers make computer vision software hallucinate things that aren‚Äôt there](https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google).
:::

::: {.content-visible unless-format="revealjs"}
The above graphical illustration shows how adding a metal component changes the model predictions from _Banana_ to _toaster_ with high confidence.
:::

## Adversarial text

::: {.content-visible unless-format="revealjs"}
Adversarial attacks on text generation models help users get an understanding of the inner workings NLP models. This includes identifying input patterns that are critical to model predictions, and assessing performance of NLP models for robustness. 
:::


"[TextAttack](https://github.com/QData/TextAttack) üêô is a Python framework for adversarial attacks, data augmentation, and model training in NLP"

![Demo](https://jxmo.io/files/textattack.gif){width=80%}

## Deep Dream 

![Deep Dream is an image-modification program released by Google in 2015.](deep-dream.jpeg)

::: footer
Source: Wikipedia, [DeepDream page](https://commons.wikimedia.org/wiki/File:Aurelia-aurita-3-0009.jpg).
:::

## DeepDream

- Even though many deep learning models are black boxes, convnets are quite interpretable via visualization. Some visualization techniques are: visualizing convnet outputs shows how convnet layers transform the input, visualizing convnet filters shows what visual patterns or concept each filter is receptive to, etc.
- The activations of the first few layers of the network carries more information about the visual contents, while deeper layers encode higher, more abstract concepts.

## DeepDream

- Each filter is receptive to a visual pattern. To visualize a convnet filter, gradient ascent is used to maximize the response of the filter. Gradient ascent maximize a loss function and moves the image in a direction that activate the filter more strongly to enhance its reading of the visual pattern. 
- DeepDream maximizes the activation of the entire convnet layer rather than that of a specific filter, thus mixing together many visual patterns all at once.
- DeepDream starts with an existing image, latches on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.


## Original

![A sunny day on the Mornington peninsula.](deep-dream-melbourne-original.jpg)

## Transformed

![Deep-dreaming version.](deep-dream-melbourne.png)

::: footer
Generated by [Keras' Deep Dream tutorial](https://keras.io/examples/generative/deep_dream/).
:::

# Neural style transfer {data-visibility="uncounted"}

## Neural style transfer

Applying the style of a reference image to a target image while conserving the content of the target image.

![An example neural style transfer.](neuralstyletransfer.png)

::: {.notes}
- Style: textures, colors, visual patterns (blue-and-yellow circular brushstrokes in Vincent Van Gogh's Starry Night)
- Content: the higher-level macrostructure of the image (buildings in the T√ºbingen photograph).
:::

::: footer
Source: Fran√ßois Chollet (2021), _Deep Learning with Python_, Second Edition, Figure 12.9.
:::


## Goal of NST

What the model does:

- Preserve content by maintaining similar deeper layer activations between the original image and the generated image. The convnet should ‚Äúsee‚Äù both the original image and the generated image as containing the same things.

- Preserve style by maintaining similar correlations within activations for both low level layers and high-level layers. Feature correlations within a layer capture textures: the generated image and the style-reference image should share the same textures at different spatial scales.

## A wanderer in Greenland

::: columns
::: {.column width="50%"}
Content

![Some striking young hiker in Greenland.](ninja.jpg)
:::

::: {.column width="50%"}
Style

![_Wanderer above the Sea of Fog_ by Caspar David Friedrich.](wanderer.jpg)
:::
:::

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A wanderer in Greenland II

::: columns
::: {.column width="45%"}
![Animation of NST in progress.](ninja.gif)
:::

::: {.column width="55%"}
![One result of NST.](ninja-wanderer.png)
:::
:::

:::{.callout-tip}
## Question

How would you make this faster for one specific style image?
:::

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A new style image

![Hokusai's Great Wave off Kanagawa](wave.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A new content image

![The seascape in Qingdao](qingdao.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## Another neural style transfer

![The seascape in Qingdao in the style of Hokusai's Great Wave off Kanagawa](qwave.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::


## Why is this important?

Taking derivatives with respect to the input image can be a first step toward explainable AI for convolutional networks.

- [Saliency maps](https://youtu.be/y8cwyeccuy4)
- [Grad-CAM](https://youtu.be/xGZfAoh0xKs)

# Autoencoders {visibility="uncounted"}


## Autoencoder

An autoencoder takes a data/image, maps it to a latent space via an encoder module, then decodes it back to an output with the same dimensions via a decoder module.

::: {.content-visible unless-format="revealjs"}
They are useful in learning latent representations of the data.
:::

![Schematic of an autoencoder.](autoencoder.png)

::: footer
Source: Marcus Lautier (2022).
:::

## Autoencoder II {.smaller}

- An autoencoder is trained by using the same image as both the input and the target, meaning an autoencoder learns to reconstruct the original inputs. Therefore it's _not supervised learning_, but _self-supervised learning_.
- If we impose constraints on the encoders to be low-dimensional and sparse, _the input data will be compressed_ into fewer bits of information. 
- Latent space is a place that stores low-dimensional representation of data. It can be used for _data compression_, where data is compressed to a point in a latent space.
- An image can be compressed into a latent representation, which can then be reconstructed back to a _slightly different image_. 

::: {.notes}
For image editing, an image can be projected onto a latent space and moved inside the latent space in a meaningful way (which means we modify its latent representation), before being mapped back to the image space. This will edit the image and allow us to generate images that have never been seen before.
:::

{{< include _psam-gan.qmd >}}

{{< include _variational-autoencoder.qmd >}}

::: {.content-visible unless-format="revealjs"}
Both autoencoders and variational autoencoders aim to obtain latent representations of input data that carry same information but in a lower dimensional space. The difference between the two is that, autoencoders outputs the latent representations as vectors, while variational auto encoders first identifies the distribution of the input in the latent space, and then sample an observation from that as the vector. Autoencoders are better suited for dimensionality reduction and feature learning tasks. Variation autoencoders are better suited for generative modelling tasks and uncertainty estimation.
:::

# Diffusion Models {visibility="uncounted"}

## Using KerasCV

::: {.content-hidden unless-format="revealjs"}
{{< video https://www.youtube.com/watch?v=pstsh2C2roc width="1000px" height="600px" >}}
:::
::: {.content-visible unless-format="revealjs"}
{{< video https://www.youtube.com/watch?v=pstsh2C2roc >}}
:::

## Package Versions {.appendix data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"))
```

## Glossary {.appendix data-visibility="uncounted"}

::: columns
:::: column
- autoencoder (variational)
- beam search
- bias
- ChatGPT (& RLHF)
- DeepDream
- greedy sampling
::::
:::: column
- HuggingFace
- language model
- latent space
- neural style transfer
- softmax temperature
- stochastic sampling
::::
:::