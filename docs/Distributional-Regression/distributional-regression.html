<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick Laub">

<title>AI for Actuaries - Distributional Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Advanced-Topics/interpretability.html" rel="next">
<link href="../Labs/backpropagation-lab.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Distributional-Regression/distributional-regression.html">Module 7</a></li><li class="breadcrumb-item"><a href="../Distributional-Regression/distributional-regression.html">Distributional Regression</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">AI for Actuaries</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Module 1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Artificial-Intelligence/course-overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Artificial-Intelligence/artificial-intelligence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Artificial Intelligence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Artificial-Intelligence/python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/python-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Intro Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/python-for-data-science-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Python for Data Science</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/chess-ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: Chess AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Module 2</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tabular-Data/deep-learning-keras.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Deep Learning with Keras</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tabular-Data/categorical-variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Categorical Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tabular-Data/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Tabular-Data/project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/matplotlib-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Matplotlib</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/victorian-crash-severity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: Victorian Car Crash Severity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/french-motor-frequency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: French Motor Claim Frequency</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Module 3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Computer-Vision/computer-vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computer Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/latex-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Markdown</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/hurricane-damage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: Aerial Photos of Hurricane Damage</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Module 4</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Natural-Language-Processing/natural-language-processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/police-reports.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: Police Reports of US Car Crashes</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Module 5</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Time-Series-And-Recurrent-Neural-Networks/time-series-and-rnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time Series &amp; Recurrent Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Exercises/sydney-airport-temperature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercise: Sydney Temperature Forecasting</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Module 6</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Advanced-Tabular-Data/entity-embedding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Entity Embedding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Advanced-Tabular-Data/optimisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/forward-pass-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Forward Pass</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/optimisation-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Optimisation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/backpropagation-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Backpropagation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Module 7</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Distributional-Regression/distributional-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Distributional Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Advanced-Topics/interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interpretability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Labs/distributional-regression-lab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lab: Distributional Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Module 8</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Generative-Networks/generative-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Generative-Networks/gans.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative Adversarial Networks</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#neural-networks-and-confidence" id="toc-neural-networks-and-confidence" class="nav-link" data-scroll-target="#neural-networks-and-confidence">Neural networks and confidence</a></li>
  <li><a href="#new-data-can-be-different" id="toc-new-data-can-be-different" class="nav-link" data-scroll-target="#new-data-can-be-different">New data can be different</a></li>
  <li><a href="#new-data-can-be-challenging" id="toc-new-data-can-be-challenging" class="nav-link" data-scroll-target="#new-data-can-be-challenging">New data can be challenging</a></li>
  <li><a href="#classifiers-give-us-a-probability" id="toc-classifiers-give-us-a-probability" class="nav-link" data-scroll-target="#classifiers-give-us-a-probability">Classifiers give us a probability</a></li>
  <li><a href="#key-idea" id="toc-key-idea" class="nav-link" data-scroll-target="#key-idea">Key idea</a></li>
  </ul></li>
  <li><a href="#traditional-regression" id="toc-traditional-regression" class="nav-link" data-scroll-target="#traditional-regression">Traditional Regression</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#regression-notation" id="toc-regression-notation" class="nav-link" data-scroll-target="#regression-notation">Regression notation</a></li>
  <li><a href="#traditional-regression-1" id="toc-traditional-regression-1" class="nav-link" data-scroll-target="#traditional-regression-1">Traditional Regression</a></li>
  <li><a href="#visualising-the-distribution-of-each-y" id="toc-visualising-the-distribution-of-each-y" class="nav-link" data-scroll-target="#visualising-the-distribution-of-each-y">Visualising the distribution of each <span class="math inline">Y</span></a></li>
  <li><a href="#the-probabilistic-view" id="toc-the-probabilistic-view" class="nav-link" data-scroll-target="#the-probabilistic-view">The probabilistic view</a></li>
  <li><a href="#the-predicted-distributions" id="toc-the-predicted-distributions" class="nav-link" data-scroll-target="#the-predicted-distributions">The predicted distributions</a></li>
  <li><a href="#the-machine-learning-view" id="toc-the-machine-learning-view" class="nav-link" data-scroll-target="#the-machine-learning-view">The machine learning view</a></li>
  <li><a href="#generalised-linear-model-glm" id="toc-generalised-linear-model-glm" class="nav-link" data-scroll-target="#generalised-linear-model-glm">Generalised Linear Model (GLM)</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic regression</a></li>
  <li><a href="#binary-cross-entropy-loss" id="toc-binary-cross-entropy-loss" class="nav-link" data-scroll-target="#binary-cross-entropy-loss">Binary cross-entropy loss</a></li>
  <li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression">Poisson regression</a></li>
  <li><a href="#poisson-loss" id="toc-poisson-loss" class="nav-link" data-scroll-target="#poisson-loss">Poisson loss</a></li>
  <li><a href="#gamma-regression" id="toc-gamma-regression" class="nav-link" data-scroll-target="#gamma-regression">Gamma regression</a></li>
  <li><a href="#gamma-loss" id="toc-gamma-loss" class="nav-link" data-scroll-target="#gamma-loss">Gamma loss</a></li>
  <li><a href="#why-do-actuaries-use-glms" id="toc-why-do-actuaries-use-glms" class="nav-link" data-scroll-target="#why-do-actuaries-use-glms">Why do actuaries use GLMs?</a></li>
  </ul></li>
  <li><a href="#stochastic-forecasts" id="toc-stochastic-forecasts" class="nav-link" data-scroll-target="#stochastic-forecasts">Stochastic Forecasts</a>
  <ul class="collapse">
  <li><a href="#stock-price-forecasting" id="toc-stock-price-forecasting" class="nav-link" data-scroll-target="#stock-price-forecasting">Stock price forecasting</a></li>
  <li><a href="#noisy-auto-regressive-forecast" id="toc-noisy-auto-regressive-forecast" class="nav-link" data-scroll-target="#noisy-auto-regressive-forecast">Noisy auto-regressive forecast</a></li>
  <li><a href="#original-forecast" id="toc-original-forecast" class="nav-link" data-scroll-target="#original-forecast">Original forecast</a></li>
  <li><a href="#with-noise" id="toc-with-noise" class="nav-link" data-scroll-target="#with-noise">With noise</a></li>
  <li><a href="#with-noise-1" id="toc-with-noise-1" class="nav-link" data-scroll-target="#with-noise-1">With noise</a></li>
  <li><a href="#with-noise-2" id="toc-with-noise-2" class="nav-link" data-scroll-target="#with-noise-2">With noise</a></li>
  <li><a href="#many-noisy-forecasts" id="toc-many-noisy-forecasts" class="nav-link" data-scroll-target="#many-noisy-forecasts">Many noisy forecasts</a></li>
  <li><a href="#prediction-intervals" id="toc-prediction-intervals" class="nav-link" data-scroll-target="#prediction-intervals">95% “prediction intervals”</a></li>
  <li><a href="#residuals" id="toc-residuals" class="nav-link" data-scroll-target="#residuals">Residuals</a></li>
  <li><a href="#q-q-plot-and-p-p-plot" id="toc-q-q-plot-and-p-p-plot" class="nav-link" data-scroll-target="#q-q-plot-and-p-p-plot">Q-Q plot and P-P plot</a></li>
  <li><a href="#residuals-against-time" id="toc-residuals-against-time" class="nav-link" data-scroll-target="#residuals-against-time">Residuals against time</a></li>
  </ul></li>
  <li><a href="#glms-and-neural-networks" id="toc-glms-and-neural-networks" class="nav-link" data-scroll-target="#glms-and-neural-networks">GLMs and Neural Networks</a>
  <ul class="collapse">
  <li><a href="#french-motor-claim-sizes" id="toc-french-motor-claim-sizes" class="nav-link" data-scroll-target="#french-motor-claim-sizes">French motor claim sizes</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal" id="toc-doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal" class="nav-link" data-scroll-target="#doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal">Doesn’t prove that <span class="math inline">Y | \boldsymbol{X} = \boldsymbol{x}</span> is multimodal</a></li>
  <li><a href="#gamma-glm" id="toc-gamma-glm" class="nav-link" data-scroll-target="#gamma-glm">Gamma GLM</a></li>
  <li><a href="#gamma-glm-loss" id="toc-gamma-glm-loss" class="nav-link" data-scroll-target="#gamma-glm-loss">Gamma GLM loss</a></li>
  <li><a href="#fitting-steps" id="toc-fitting-steps" class="nav-link" data-scroll-target="#fitting-steps">Fitting Steps</a></li>
  <li><a href="#code-gamma-glm" id="toc-code-gamma-glm" class="nav-link" data-scroll-target="#code-gamma-glm">Code: Gamma GLM</a></li>
  <li><a href="#ann-can-feed-into-a-glm" id="toc-ann-can-feed-into-a-glm" class="nav-link" data-scroll-target="#ann-can-feed-into-a-glm">ANN can feed into a GLM</a></li>
  </ul></li>
  <li><a href="#combined-actuarial-neural-network" id="toc-combined-actuarial-neural-network" class="nav-link" data-scroll-target="#combined-actuarial-neural-network">Combined Actuarial Neural Network</a>
  <ul class="collapse">
  <li><a href="#cann" id="toc-cann" class="nav-link" data-scroll-target="#cann">CANN</a></li>
  <li><a href="#shifting-the-predicted-distributions" id="toc-shifting-the-predicted-distributions" class="nav-link" data-scroll-target="#shifting-the-predicted-distributions">Shifting the predicted distributions</a></li>
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#code-architecture" id="toc-code-architecture" class="nav-link" data-scroll-target="#code-architecture">Code: Architecture</a></li>
  <li><a href="#code-model-training" id="toc-code-model-training" class="nav-link" data-scroll-target="#code-model-training">Code: Model Training</a></li>
  </ul></li>
  <li><a href="#mixture-density-network" id="toc-mixture-density-network" class="nav-link" data-scroll-target="#mixture-density-network">Mixture Density Network</a>
  <ul class="collapse">
  <li><a href="#mixture-distribution" id="toc-mixture-distribution" class="nav-link" data-scroll-target="#mixture-distribution">Mixture Distribution</a></li>
  <li><a href="#mixture-distribution-1" id="toc-mixture-distribution-1" class="nav-link" data-scroll-target="#mixture-distribution-1">Mixture Distribution</a></li>
  <li><a href="#mixture-density-network-1" id="toc-mixture-density-network-1" class="nav-link" data-scroll-target="#mixture-density-network-1">Mixture Density Network</a></li>
  <li><a href="#mixture-density-network-2" id="toc-mixture-density-network-2" class="nav-link" data-scroll-target="#mixture-density-network-2">Mixture Density Network</a></li>
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification">Model Specification</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output</a></li>
  <li><a href="#architecture-1" id="toc-architecture-1" class="nav-link" data-scroll-target="#architecture-1">Architecture</a></li>
  <li><a href="#code-import-legacy-keras-for-now" id="toc-code-import-legacy-keras-for-now" class="nav-link" data-scroll-target="#code-import-legacy-keras-for-now">Code: Import “legacy” Keras (for now)</a></li>
  <li><a href="#code-architecture-1" id="toc-code-architecture-1" class="nav-link" data-scroll-target="#code-architecture-1">Code: Architecture</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#code-loss-training" id="toc-code-loss-training" class="nav-link" data-scroll-target="#code-loss-training">Code: Loss &amp; training</a></li>
  </ul></li>
  <li><a href="#metrics-for-distributional-regression" id="toc-metrics-for-distributional-regression" class="nav-link" data-scroll-target="#metrics-for-distributional-regression">Metrics for Distributional Regression</a>
  <ul class="collapse">
  <li><a href="#proper-scoring-rules" id="toc-proper-scoring-rules" class="nav-link" data-scroll-target="#proper-scoring-rules">Proper Scoring Rules</a></li>
  <li><a href="#example-proper-scoring-rules" id="toc-example-proper-scoring-rules" class="nav-link" data-scroll-target="#example-proper-scoring-rules">Example Proper Scoring Rules</a></li>
  <li><a href="#likelihoods" id="toc-likelihoods" class="nav-link" data-scroll-target="#likelihoods">Likelihoods</a></li>
  <li><a href="#code-nll" id="toc-code-nll" class="nav-link" data-scroll-target="#code-nll">Code: NLL</a></li>
  <li><a href="#model-comparisons" id="toc-model-comparisons" class="nav-link" data-scroll-target="#model-comparisons">Model Comparisons</a></li>
  </ul></li>
  <li><a href="#aleatoric-and-epistemic-uncertainty" id="toc-aleatoric-and-epistemic-uncertainty" class="nav-link" data-scroll-target="#aleatoric-and-epistemic-uncertainty">Aleatoric and Epistemic Uncertainty</a>
  <ul class="collapse">
  <li><a href="#categories-of-uncertainty" id="toc-categories-of-uncertainty" class="nav-link" data-scroll-target="#categories-of-uncertainty">Categories of uncertainty</a></li>
  <li><a href="#aleatoric-uncertainty" id="toc-aleatoric-uncertainty" class="nav-link" data-scroll-target="#aleatoric-uncertainty">Aleatoric Uncertainty</a></li>
  <li><a href="#epistemic-uncertainty" id="toc-epistemic-uncertainty" class="nav-link" data-scroll-target="#epistemic-uncertainty">Epistemic Uncertainty</a></li>
  <li><a href="#sources-of-uncertainty" id="toc-sources-of-uncertainty" class="nav-link" data-scroll-target="#sources-of-uncertainty">Sources of uncertainty</a></li>
  </ul></li>
  <li><a href="#avoiding-overfitting" id="toc-avoiding-overfitting" class="nav-link" data-scroll-target="#avoiding-overfitting">Avoiding Overfitting</a>
  <ul class="collapse">
  <li><a href="#traditional-regularisation" id="toc-traditional-regularisation" class="nav-link" data-scroll-target="#traditional-regularisation">Traditional regularisation</a></li>
  <li><a href="#regularisation-in-keras" id="toc-regularisation-in-keras" class="nav-link" data-scroll-target="#regularisation-in-keras">Regularisation in Keras</a></li>
  <li><a href="#weights-before-after-l2" id="toc-weights-before-after-l2" class="nav-link" data-scroll-target="#weights-before-after-l2">Weights before &amp; after <span class="math inline">L^2</span></a></li>
  <li><a href="#weights-before-after-l1" id="toc-weights-before-after-l1" class="nav-link" data-scroll-target="#weights-before-after-l1">Weights before &amp; after <span class="math inline">L^1</span></a></li>
  <li><a href="#early-stopping-regularisation" id="toc-early-stopping-regularisation" class="nav-link" data-scroll-target="#early-stopping-regularisation">Early-stopping regularisation</a></li>
  </ul></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a>
  <ul class="collapse">
  <li><a href="#dropout-1" id="toc-dropout-1" class="nav-link" data-scroll-target="#dropout-1">Dropout</a></li>
  <li><a href="#dropout-quote-1" id="toc-dropout-quote-1" class="nav-link" data-scroll-target="#dropout-quote-1">Dropout quote #1</a></li>
  <li><a href="#dropout-quote-2" id="toc-dropout-quote-2" class="nav-link" data-scroll-target="#dropout-quote-2">Dropout quote #2</a></li>
  <li><a href="#code-dropout" id="toc-code-dropout" class="nav-link" data-scroll-target="#code-dropout">Code: Dropout</a></li>
  <li><a href="#code-dropout-after-training" id="toc-code-dropout-after-training" class="nav-link" data-scroll-target="#code-dropout-after-training">Code: Dropout after training</a></li>
  <li><a href="#dropout-limitation" id="toc-dropout-limitation" class="nav-link" data-scroll-target="#dropout-limitation">Dropout Limitation</a></li>
  <li><a href="#accidental-dropout-dead-neurons" id="toc-accidental-dropout-dead-neurons" class="nav-link" data-scroll-target="#accidental-dropout-dead-neurons">Accidental dropout (“dead neurons”)</a></li>
  <li><a href="#find-dead-relu-neurons" id="toc-find-dead-relu-neurons" class="nav-link" data-scroll-target="#find-dead-relu-neurons">Find dead ReLU neurons</a></li>
  <li><a href="#trying-different-seeds" id="toc-trying-different-seeds" class="nav-link" data-scroll-target="#trying-different-seeds">Trying different seeds</a></li>
  <li><a href="#look-at-distribution-of-dead-relus" id="toc-look-at-distribution-of-dead-relus" class="nav-link" data-scroll-target="#look-at-distribution-of-dead-relus">Look at distribution of dead ReLUs</a></li>
  </ul></li>
  <li><a href="#ensembles" id="toc-ensembles" class="nav-link" data-scroll-target="#ensembles">Ensembles</a>
  <ul class="collapse">
  <li><a href="#ensembles-1" id="toc-ensembles-1" class="nav-link" data-scroll-target="#ensembles-1">Ensembles</a></li>
  <li><a href="#deep-ensembles" id="toc-deep-ensembles" class="nav-link" data-scroll-target="#deep-ensembles">Deep Ensembles</a></li>
  <li><a href="#deep-ensembles-ii" id="toc-deep-ensembles-ii" class="nav-link" data-scroll-target="#deep-ensembles-ii">Deep Ensembles II</a></li>
  
  
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="distributional-regression.slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Distributional-Regression/distributional-regression.html">Module 7</a></li><li class="breadcrumb-item"><a href="../Distributional-Regression/distributional-regression.html">Distributional Regression</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Distributional Regression</h1>
<p class="subtitle lead">ACTL3143 &amp; ACTL5111 Deep Learning for Actuaries</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Patrick Laub </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="acknowledgments" class="level4">
<h4 class="anchored" data-anchor-id="acknowledgments">Acknowledgments</h4>
<p>Thanks to Eric Dong for making the original version of these slides.</p>
<div id="8ff013ce" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the package imports</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.random <span class="im">as</span> rnd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> EarlyStopping</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential, Model</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Input, Dense, Concatenate</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.initializers <span class="im">import</span> Constant</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> make_column_transformer</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> set_config</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>set_config(transform_output<span class="op">=</span><span class="st">"pandas"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="introduction" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="introduction">Introduction</h2>
<section id="neural-networks-and-confidence" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks-and-confidence">Neural networks and confidence</h3>
<p>Say we have a neural network that classifies ducks from rabbits.</p>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dalle2-duck.webp" class="img-fluid figure-img"></p>
<figcaption>A duck in the training set</figcaption>
</figure>
</div>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dalle2-rabbit.webp" class="img-fluid figure-img"></p>
<figcaption>A rabbit in this training set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="new-data-can-be-different" class="level3 smaller">
<h3 class="smaller anchored" data-anchor-id="new-data-can-be-different">New data can be different</h3>
<div class="columns">
<div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dalle2-duck2.webp" class="img-fluid figure-img"></p>
<figcaption>Predict this is a duck</figcaption>
</figure>
</div>
</div><div class="column" style="width:7.5%;">

</div><div class="column fragment" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="755230_poster.jpg" class="img-fluid figure-img"></p>
<figcaption>Misclassify this as a rabbit</figcaption>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>We could do better if we collected more images of ducks with rabbit ears.</p>
</div>
<div class="footer">
<p>Source: Olga Telnova, <a href="https://www.posterlounge.co.uk/p/755230.html">Cute Duck with Bunny Ears</a>, Posterlounge, accessed on July 16 2024.</p>
</div>
</section>
<section id="new-data-can-be-challenging" class="level3 smaller">
<h3 class="smaller anchored" data-anchor-id="new-data-can-be-challenging">New data can be challenging</h3>
<div class="columns">
<div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dalle2-rabbit2.webp" class="img-fluid figure-img"></p>
<figcaption>Predict this is a rabbit</figcaption>
</figure>
</div>
</div><div class="column fragment">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="duck-rabbit.png" class="img-fluid figure-img"></p>
<figcaption>Predict this is a ???</figcaption>
</figure>
</div>
</div>
</div>
<div class="fragment">
<p>This is inherently difficult, extra training data won’t help.</p>
</div>
<div class="footer">
<p>Source: <a href="https://commons.wikimedia.org/wiki/Category:Rabbit%E2%80%93duck_illusion#/media/File:Canard-lapin_retouch%C3%A9.jpg">Wikimedia Commons</a></p>
</div>
</section>
<section id="classifiers-give-us-a-probability" class="level3">
<h3 class="anchored" data-anchor-id="classifiers-give-us-a-probability">Classifiers give us a probability</h3>
<p>This is already a big step up compared to regression models.</p>
<p>However, neural networks’ “probabilities” can be overconfident.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="example-of-overconfidence.png" class="img-fluid figure-img"></p>
<figcaption>We <a href="https://pat-laub.github.io/DeepLearningForActuaries/Computer-Vision/computer-vision.html#confidence-of-predictions">already saw</a> a case of this.</figcaption>
</figure>
</div>
<p>See Guo et al.&nbsp;(2017), <a href="https://arxiv.org/pdf/1706.04599">On Calibration of Modern Neural Networks</a>.</p>
</section>
<section id="key-idea" class="level3">
<h3 class="anchored" data-anchor-id="key-idea">Key idea</h3>
<div class="columns">
<div class="column">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Density_forecasting_2.jpeg" class="img-fluid figure-img"></p>
<figcaption>An example of distributional forecasting over the All Ordinaries Index</figcaption>
</figure>
</div>
</div><div class="column">
<ul>
<li>Earlier machine learning models focused on point estimates.</li>
<li>However, in many applications, we need to understand the distribution of the response variable.</li>
<li>Each prediction becomes a <em>distribution</em> over the possible outcomes</li>
</ul>
</div>
</div>
<div class="footer">
<p>Source: Tomasz Woźniak (2024), <a href="https://www.linkedin.com/posts/tomaszwwozniak_rstats-densityforecasting-activity-7171005952463134721-ZsHl?utm_source=share&amp;utm_medium=member_desktop">LinkedIn Post</a>, accessed on July 15 2024.</p>
</div>
</section>
</section>
<section id="traditional-regression" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="traditional-regression">Traditional Regression</h2>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<ul>
<li>scalars are denoted by lowercase letters, e.g., <span class="math inline">y</span>,</li>
<li>vectors are denoted by bold lowercase letters, e.g., <span class="math display">\boldsymbol{y} = (y_1, \ldots, y_n) ,</span></li>
<li>random variables are denoted by capital letters, e.g., <span class="math inline">Y</span></li>
<li>random vectors are denoted by bold capital letters, e.g., <span class="math display">\boldsymbol{X} = (X_1, \ldots, X_p) ,</span></li>
<li>matrices are denoted by bold uppercase non-italics letters, e.g., <span class="math display">\mathbf{X} = \begin{pmatrix} x_{11} &amp; \cdots &amp; x_{1p} \\ \vdots &amp; \ddots &amp; \vdots \\ x_{n1} &amp; \cdots &amp; x_{np} \end{pmatrix} .</span></li>
</ul>
</section>
<section id="regression-notation" class="level3">
<h3 class="anchored" data-anchor-id="regression-notation">Regression notation</h3>
<ul>
<li><span class="math inline">n</span> is the number of observations, <span class="math inline">p</span> is the number of features,</li>
<li>the true coefficients are <span class="math inline">\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)</span>,</li>
<li><span class="math inline">\beta_0</span> is the intercept, <span class="math inline">\beta_1, \ldots, \beta_p</span> are the coefficients,</li>
<li><span class="math inline">\widehat{\boldsymbol{\beta}}</span> is the estimated coefficient vector,</li>
<li><span class="math inline">\boldsymbol{x}_i = (1, x_{i1}, x_{i2}, \ldots, x_{ip})</span> is the feature vector for the <span class="math inline">i</span>th observation,</li>
<li><span class="math inline">y_i</span> is the response variable for the <span class="math inline">i</span>th observation,</li>
<li><span class="math inline">\hat{y}_i</span> is the predicted value for the <span class="math inline">i</span>th observation,</li>
<li>probability density functions (p.d.f.), probability mass functions (p.m.f.), cumulative distribution functions (c.d.f.).</li>
</ul>
</section>
<section id="traditional-regression-1" class="level3">
<h3 class="anchored" data-anchor-id="traditional-regression-1">Traditional Regression</h3>
<p>Multiple linear regression assumes the data-generating process is</p>
<p><span class="math display">Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \varepsilon</span></p>
<p>where <span class="math inline">\varepsilon \sim \mathcal{N}(0, \sigma^2)</span>.</p>
<p>We estimate the coefficients <span class="math inline">\beta_0, \beta_1, \ldots, \beta_p</span> by minimising the sum of squared residuals or mean squared error</p>
<p><span class="math display">\text{RSS} := \sum_{i=1}^n (y_i - \hat{y}_i)^2
, \quad \text{MSE} := \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 ,
</span></p>
<p>where <span class="math inline">\hat{y}_i</span> is the predicted value for the <span class="math inline">i</span>th observation.</p>
</section>
<section id="visualising-the-distribution-of-each-y" class="level3">
<h3 class="anchored" data-anchor-id="visualising-the-distribution-of-each-y">Visualising the distribution of each <span class="math inline">Y</span></h3>
<div id="bd0fdc81" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data for linear regression</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X_toy <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(X_toy)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>y_toy <span class="op">=</span> beta_0 <span class="op">+</span> beta_1 <span class="op">*</span> X_toy <span class="op">+</span> np.random.normal(scale<span class="op">=</span><span class="dv">2</span>, size<span class="op">=</span>X_toy.shape)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>sigma_toy <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Assuming a standard deviation for the normal distribution</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a simple linear regression model</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>coefficients <span class="op">=</span> np.polyfit(X_toy, y_toy, <span class="dv">1</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>predicted_y <span class="op">=</span> np.polyval(coefficients, X_toy)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points and the fitted line</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_toy, y_toy, label<span class="op">=</span><span class="st">'Data Points'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.plot(X_toy, predicted_y, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Fitted Line'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the normal distribution bell curve sideways at each data point</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X_toy)):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> predicted_y[i]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    y_values <span class="op">=</span> np.linspace(mu <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy, mu <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy, <span class="dv">100</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> stats.norm.pdf(y_values, mu, sigma_toy) <span class="op">+</span> X_toy[i]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$y$'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-probabilistic-view" class="level3">
<h3 class="anchored" data-anchor-id="the-probabilistic-view">The probabilistic view</h3>
<p><span class="math display">Y_i \sim \mathcal{N}(\mu_i, \sigma^2)</span></p>
<p>where <span class="math inline">\mu_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}</span>, and the <span class="math inline">\sigma^2</span> is known.</p>
<p>The <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span> normal distribution has p.d.f.</p>
<p><span class="math display">f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) .</span></p>
<p>The likelihood function is</p>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)
</span> <span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
</span></p>
<p>Perform maximum likelihood estimation to find <span class="math inline">\boldsymbol{\beta}</span>.</p>
</section>
<section id="the-predicted-distributions" class="level3">
<h3 class="anchored" data-anchor-id="the-predicted-distributions">The predicted distributions</h3>
<div id="d3224e3a" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.polyval(coefficients, X_toy[:<span class="dv">4</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> y_pred[:<span class="dv">4</span>].min() <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> y_pred[:<span class="dv">4</span>].max() <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> y_pred[i]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    y_grid <span class="op">=</span> stats.norm.pdf(x_grid, mu, sigma_toy)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_grid, y_grid)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="ss">f'$f(y ; </span><span class="ch">\\</span><span class="ss">boldsymbol</span><span class="ch">{{</span><span class="ss">x</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">)$'</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([y_pred[i]], labels<span class="op">=</span>[<span class="vs">r'$\mu_{'</span> <span class="op">+</span> <span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="vs">r'}$'</span>])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    ax.plot(y_toy[i], <span class="dv">0</span>, <span class="st">'r|'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-machine-learning-view" class="level3">
<h3 class="anchored" data-anchor-id="the-machine-learning-view">The machine learning view</h3>
<p>The negative log-likelihood <span class="math inline">\text{NLL}(\boldsymbol{\beta}) := -\ell(\boldsymbol{\beta})</span> is to be minimised:</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta})
= \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
</span></p>
<p>As <span class="math inline">\sigma^2</span> is fixed, minimising NLL is equivalent to minimising MSE:</p>
<p><span class="math display">
\begin{aligned}
\widehat{\boldsymbol{\beta}}
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{NLL}(\boldsymbol{\beta}) \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{1}{n} \sum_{i=1}^n \Bigl( y_i - \hat{y}_i(\boldsymbol{x}_i; \boldsymbol{\beta}) \Bigr)^2 \\
&amp;= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{MSE}\bigl( \boldsymbol{y}, \hat{\boldsymbol{y}}(\boldsymbol{\mathbf{X}}; \boldsymbol{\beta}) \bigr).
\end{aligned}
</span></p>
</section>
<section id="generalised-linear-model-glm" class="level3">
<h3 class="anchored" data-anchor-id="generalised-linear-model-glm">Generalised Linear Model (GLM)</h3>
<p>The GLM is often characterised by the mean prediction:</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = g^{-1} \left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)
</span></p>
<p>where <span class="math inline">g</span> is the link function.</p>
<p>Common GLM distributions for the response variable include:</p>
<ul>
<li>Normal distribution with identity link (just MLR)</li>
<li>Bernoulli distribution with logit link (logistic regression)</li>
<li>Poisson distribution with log link (Poisson regression)</li>
<li>Gamma distribution with log link</li>
</ul>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">Logistic regression</h3>
<p>A Bernoulli distribution with parameter <span class="math inline">p</span> has p.m.f.</p>
<p><span class="math display">
f(y)\ =\ \begin{cases}
p &amp; \text{if } y = 1 \\
1 - p &amp; \text{if } y = 0
\end{cases}
\ =\ p^y (1 - p)^{1 - y}.
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> follows a Bernoulli distribution with parameter</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \frac{1}{1 + \exp\left(-\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)} = \mathbb{P}(Y=1|\boldsymbol{X}=\boldsymbol{x}).
</span></p>
<p>The likelihood function, using <span class="math inline">\mu_i := \mu(\boldsymbol{x}_i; \boldsymbol{\beta})</span>, is</p>
<p><span class="math display">
L(\boldsymbol{\beta})
\ =\ \prod_{i=1}^n \begin{cases}
\mu_i &amp; \text{if } y_i = 1 \\
1 - \mu_i &amp; \text{if } y_i = 0
\end{cases}
\ =\ \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i} .
</span></p>
</section>
<section id="binary-cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="binary-cross-entropy-loss">Binary cross-entropy loss</h3>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i}
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = -\sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
<p>The binary cross-entropy loss is basically identical: <span class="math display">
\text{BCE}(\boldsymbol{y}, \boldsymbol{\mu}) = - \frac{1}{n} \sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
</span></p>
</section>
<section id="poisson-regression" class="level3">
<h3 class="anchored" data-anchor-id="poisson-regression">Poisson regression</h3>
<p>A Poisson distribution with rate <span class="math inline">\lambda</span> has p.m.f. <span class="math display">
f(y) = \frac{\lambda^y \exp(-\lambda)}{y!}.
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is Poisson distributed with parameter</p>
<p><span class="math display">
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right) .
</span></p>
<p>The likelihood function is</p>
<p><span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{ \mu_i^{y_i} \exp(-\mu_i) }{y_i!}
</span> <span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( -\mu_i + y_i \log(\mu_i) - \log(y_i!) \Bigr).
</span></p>
</section>
<section id="poisson-loss" class="level3">
<h3 class="anchored" data-anchor-id="poisson-loss">Poisson loss</h3>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) + \log(y_i!) \Bigr) .
</span></p>
<p>The Poisson loss is</p>
<p><span class="math display">
\text{Poisson}(\boldsymbol{y}, \boldsymbol{\mu}) = \frac{1}{n} \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) \Bigr).
</span></p>
</section>
<section id="gamma-regression" class="level3">
<h3 class="anchored" data-anchor-id="gamma-regression">Gamma regression</h3>
<p>A gamma distribution with mean <span class="math inline">\mu</span> and dispersion <span class="math inline">\phi</span> has p.d.f. <span class="math display">
f(y; \mu, \phi) = \frac{(\mu \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y^{\frac{1}{\phi} - 1} \mathrm{e}^{-\frac{y}{\mu \phi}}
</span></p>
<p>Our model is <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is gamma distributed with a dispersion of <span class="math inline">\phi</span> and a mean of <span class="math inline">\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)</span>.</p>
<p>The likelihood function is <span class="math display">
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{(\mu_i \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y_i^{\frac{1}{\phi} - 1} \exp\left(-\frac{y_i}{\mu_i \phi}\right)
</span></p>
<p><span class="math display">
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ -\frac{1}{\phi} \log(\mu_i \phi) - \log \Gamma\left(\frac{1}{\phi}\right) + \left(\frac{1}{\phi} - 1\right) \log(y_i) - \frac{y_i}{\mu_i \phi} \right].
</span></p>
</section>
<section id="gamma-loss" class="level3">
<h3 class="anchored" data-anchor-id="gamma-loss">Gamma loss</h3>
<p>The negative log-likelihood is</p>
<p><span class="math display">
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i \phi) + \log \Gamma\left(\frac{1}{\phi}\right) - \left(\frac{1}{\phi} - 1\right) \log(y_i) + \frac{y_i}{\mu_i \phi} \right].
</span></p>
<p>Since <span class="math inline">\phi</span> is a nuisance parameter <span class="math display">
\text{NLL}(\boldsymbol{\beta})
= \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i) + \frac{y_i}{\mu_i \phi} \right] + \text{const}
\propto \sum_{i=1}^n \left[ \log(\mu_i) + \frac{y_i}{\mu_i} \right].
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As <span class="math inline">\log(\mu_i) = \log(y_i) - \log(y_i / \mu_i)</span>, we could write an alternative version <span class="math display">
\text{NLL}(\boldsymbol{\beta})
\propto \sum_{i=1}^n \left[ \log(y_i) - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) + \frac{y_i}{\mu_i} \right]
\propto \sum_{i=1}^n \left[ \frac{y_i}{\mu_i} - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) \right].
</span></p>
</div>
</div>
</section>
<section id="why-do-actuaries-use-glms" class="level3">
<h3 class="anchored" data-anchor-id="why-do-actuaries-use-glms">Why do actuaries use GLMs?</h3>
<ul>
<li>GLMs are interpretable.</li>
<li>GLMs are flexible (can handle different types of response variables).</li>
<li>We get the full distribution of the response variable, not just the mean.</li>
</ul>
<p>This last point is particularly important for analysing worst-case scenarios.</p>
</section>
</section>
<section id="stochastic-forecasts" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="stochastic-forecasts">Stochastic Forecasts</h2>
<section id="stock-price-forecasting" class="level3">
<h3 class="anchored" data-anchor-id="stock-price-forecasting">Stock price forecasting</h3>
<div id="5943d715" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lagged_timeseries(df, target, window<span class="op">=</span><span class="dv">30</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    lagged <span class="op">=</span> pd.DataFrame()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(window, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        lagged[<span class="ss">f"T-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> df[target].shift(i)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    lagged[<span class="st">"T"</span>] <span class="op">=</span> df[target].values</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lagged</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>stocks <span class="op">=</span> pd.read_csv(<span class="st">"../Time-Series-And-Recurrent-Neural-Networks/aus_fin_stocks.csv"</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>stocks[<span class="st">"Date"</span>] <span class="op">=</span> pd.to_datetime(stocks[<span class="st">"Date"</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>stocks <span class="op">=</span> stocks.set_index(<span class="st">"Date"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> stocks.pop(<span class="st">"ASX200"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>stock <span class="op">=</span> stocks[[<span class="st">"CBA"</span>]]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>stock <span class="op">=</span> stock.ffill()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>df_lags <span class="op">=</span> lagged_timeseries(stock, <span class="st">"CBA"</span>, <span class="dv">40</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data in time</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> df_lags.loc[:<span class="st">"2018"</span>]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> df_lags.loc[<span class="st">"2019"</span>]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> df_lags.loc[<span class="st">"2020"</span>:]</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any with NAs and split into X and y</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train.dropna()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> X_val.dropna()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test.dropna()</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> X_train.pop(<span class="st">"T"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> X_val.pop(<span class="st">"T"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> X_test.pop(<span class="st">"T"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> X_val <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> y_val <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> LinearRegression()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>lr.fit(X_train, y_train)<span class="op">;</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>stocks.plot()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper center"</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>), ncol<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="noisy-auto-regressive-forecast" class="level3">
<h3 class="anchored" data-anchor-id="noisy-auto-regressive-forecast">Noisy auto-regressive forecast</h3>
<div id="3f3caa67" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> noisy_autoregressive_forecast(model, X_val, sigma, suppress<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Generate a multi-step forecast using the given model.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    multi_step <span class="op">=</span> pd.Series(index<span class="op">=</span>X_val.index, name<span class="op">=</span><span class="st">"Multi Step"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the input data for forecasting</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    input_data <span class="op">=</span> X_val.iloc[<span class="dv">0</span>].values.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(multi_step)):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure input_data has the correct feature names</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        input_df <span class="op">=</span> pd.DataFrame(input_data, columns<span class="op">=</span>X_val.columns)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> suppress:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            next_value <span class="op">=</span> model.predict(input_df, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            next_value <span class="op">=</span> model.predict(input_df)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        next_value <span class="op">+=</span> np.random.normal(<span class="dv">0</span>, sigma)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        multi_step.iloc[i] <span class="op">=</span> next_value</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append that prediction to the input for the next forecast</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">+</span> <span class="dv">1</span> <span class="op">&lt;</span> <span class="bu">len</span>(multi_step):</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            input_data <span class="op">=</span> np.append(input_data[:, <span class="dv">1</span>:], next_value).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> multi_step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="original-forecast" class="level3">
<h3 class="anchored" data-anchor-id="original-forecast">Original forecast</h3>
<div id="6cd42956" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lr_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="750bc21c" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>stock.loc[lr_forecast.index, <span class="st">"AR Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_forecast</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_forecasts(stock):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].plot()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    plt.axvline(<span class="st">"2019"</span>, color<span class="op">=</span><span class="st">"black"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b7d1d409" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_train.loc[<span class="st">"2015"</span>:] <span class="op">-</span> lr.predict(X_train.loc[<span class="st">"2015"</span>:])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.std(residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="with-noise" class="level3">
<h3 class="anchored" data-anchor-id="with-noise">With noise</h3>
<div id="015c6043" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ef731636" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="with-noise-1" class="level3" data-visibility="uncounted">
<h3 data-visibility="uncounted" class="anchored" data-anchor-id="with-noise-1">With noise</h3>
<div id="efcc5ce7" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ba956f31" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="with-noise-2" class="level3" data-visibility="uncounted">
<h3 data-visibility="uncounted" class="anchored" data-anchor-id="with-noise-2">With noise</h3>
<div id="6ed3df0a" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">3</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lr_noisy_forecast <span class="op">=</span> noisy_autoregressive_forecast(lr, X_val, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e04cc374" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>stock.loc[lr_noisy_forecast.index, <span class="st">"AR Noisy Linear"</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> lr_noisy_forecast</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plot_forecasts(stock)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="many-noisy-forecasts" class="level3" data-visibility="uncounted">
<h3 data-visibility="uncounted" class="anchored" data-anchor-id="many-noisy-forecasts">Many noisy forecasts</h3>
<div id="7625909a" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>num_forecasts <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>forecasts <span class="op">=</span> []</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_forecasts):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    forecasts.append(noisy_autoregressive_forecast(lr, X_val, sigma) <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>noisy_forecasts <span class="op">=</span> pd.concat(forecasts, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>noisy_forecasts.index <span class="op">=</span> X_val.index</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5105596b" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>noisy_forecasts.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].plot(legend<span class="op">=</span><span class="va">False</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Stock Price"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="prediction-intervals" class="level3">
<h3 class="anchored" data-anchor-id="prediction-intervals">95% “prediction intervals”</h3>
<div id="e32879b7" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate quantiles for the forecasts</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>lower_quantile <span class="op">=</span> noisy_forecasts.quantile(<span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>upper_quantile <span class="op">=</span> noisy_forecasts.quantile(<span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>mean_forecast <span class="op">=</span> noisy_forecasts.mean(axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d84abcce" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the mean forecast</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plt.plot(stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>].index, stock.loc[<span class="st">"2018-12"</span>:<span class="st">"2019"</span>][<span class="st">"CBA"</span>], label<span class="op">=</span><span class="st">"CBA"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.plot(mean_forecast, label<span class="op">=</span><span class="st">"Mean"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the quantile-based shaded area</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.fill_between(mean_forecast.index, </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>                 lower_quantile, </span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                 upper_quantile, </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">"grey"</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot settings</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>plt.axvline(pd.Timestamp(<span class="st">"2019-01-01"</span>), color<span class="op">=</span><span class="st">"black"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"center left"</span>, bbox_to_anchor<span class="op">=</span>(<span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Stock Price"</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="residuals" class="level3">
<h3 class="anchored" data-anchor-id="residuals">Residuals</h3>
<div class="columns">
<div class="column">
<div id="d5b07b71" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> lr.predict(X_train)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_train <span class="op">-</span> y_pred</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">-=</span> np.mean(residuals)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">/=</span> np.std(residuals)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>stats.shapiro(residuals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/plaub/miniconda3/envs/ai2024/lib/python3.11/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N &gt; 5000.
  warnings.warn("p-value may not be accurate for N &gt; 5000.")</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>ShapiroResult(statistic=0.9038059115409851, pvalue=0.0)</code></pre>
</div>
</div>
<div class="fragment callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Probably should model the log-returns instead of the stock prices.</p>
</div>
</div>
</div><div class="column">
<div id="ea0ab211" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>plt.hist(residuals, bins<span class="op">=</span><span class="dv">40</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, stats.norm.pdf(x, <span class="dv">0</span>, <span class="dv">1</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-q-plot-and-p-p-plot" class="level3" data-visibility="uncounted">
<h3 data-visibility="uncounted" class="anchored" data-anchor-id="q-q-plot-and-p-p-plot">Q-Q plot and P-P plot</h3>
<div class="columns">
<div class="column">
<div id="13e73852" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>sm.qqplot(residuals, line<span class="op">=</span><span class="st">"45"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="45a9f1d0" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>sm.ProbPlot(residuals).ppplot(line<span class="op">=</span><span class="st">"45"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="residuals-against-time" class="level3">
<h3 class="anchored" data-anchor-id="residuals-against-time">Residuals against time</h3>
<div id="91a3204b" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>plt.plot(y_train.index, residuals)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Date"</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Standardised Residuals"</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Heteroskedasticity!</p>
</section>
</section>
<section id="glms-and-neural-networks" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="glms-and-neural-networks">GLMs and Neural Networks</h2>
<section id="french-motor-claim-sizes" class="level3">
<h3 class="anchored" data-anchor-id="french-motor-claim-sizes">French motor claim sizes</h3>
<p>As <code>freMTPL2sev</code> just has Policy ID &amp; severity, we merge with <code>freMTPL2freq</code> which has Policy ID, # Claims, and other covariables.</p>
<div id="a09950c4" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="annotated-cell-19"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-19-1"><a href="#annotated-cell-19-1" aria-hidden="true" tabindex="-1"></a>sev <span class="op">=</span> pd.read_csv(<span class="st">'freMTPL2sev.csv'</span>)</span>
<span id="annotated-cell-19-2"><a href="#annotated-cell-19-2" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> pd.read_csv(<span class="st">'freMTPL2freq.csv'</span>).drop(columns<span class="op">=</span>[<span class="st">'ClaimNb'</span>])</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-19" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-19-3" class="code-annotation-target"><a href="#annotated-cell-19-3" aria-hidden="true" tabindex="-1"></a>sev <span class="op">=</span> pd.merge(sev, cov, on<span class="op">=</span><span class="st">'IDpol'</span>, how<span class="op">=</span><span class="st">'left'</span>).drop(columns<span class="op">=</span>[<span class="st">"IDpol"</span>]).dropna()</span>
<span id="annotated-cell-19-4"><a href="#annotated-cell-19-4" aria-hidden="true" tabindex="-1"></a>sev</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-19" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-19" data-code-lines="3" data-code-annotation="1">Merges the severity dataframe <code>sev</code> with the covariates in <code>covariates</code> by matching the <code>IDpol</code> column. Assigning <code>how='left'</code> ensures that all rows from the left dataset <code>sev</code> is considered, and only the matching columns from <code>covariates</code> are selected. Also drops the policy ID column and any missing values or/and NAN values.</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">ClaimAmount</th>
<th data-quarto-table-cell-role="th">Exposure</th>
<th data-quarto-table-cell-role="th">VehPower</th>
<th data-quarto-table-cell-role="th">VehAge</th>
<th data-quarto-table-cell-role="th">DrivAge</th>
<th data-quarto-table-cell-role="th">BonusMalus</th>
<th data-quarto-table-cell-role="th">VehBrand</th>
<th data-quarto-table-cell-role="th">VehGas</th>
<th data-quarto-table-cell-role="th">Area</th>
<th data-quarto-table-cell-role="th">Density</th>
<th data-quarto-table-cell-role="th">Region</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>995.20</td>
<td>0.59</td>
<td>11.0</td>
<td>0.0</td>
<td>39.0</td>
<td>56.0</td>
<td>B12</td>
<td>Diesel</td>
<td>D</td>
<td>778.0</td>
<td>Picardie</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1128.12</td>
<td>0.95</td>
<td>4.0</td>
<td>1.0</td>
<td>49.0</td>
<td>50.0</td>
<td>B12</td>
<td>Regular</td>
<td>E</td>
<td>2354.0</td>
<td>Ile-de-France</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">26637</td>
<td>767.55</td>
<td>0.43</td>
<td>6.0</td>
<td>0.0</td>
<td>67.0</td>
<td>50.0</td>
<td>B2</td>
<td>Diesel</td>
<td>C</td>
<td>142.0</td>
<td>Languedoc-Roussillon</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">26638</td>
<td>1500.00</td>
<td>0.28</td>
<td>7.0</td>
<td>2.0</td>
<td>36.0</td>
<td>60.0</td>
<td>B12</td>
<td>Diesel</td>
<td>D</td>
<td>1732.0</td>
<td>Rhone-Alpes</td>
</tr>
</tbody>
</table>

<p>26444 rows × 11 columns</p>
</div>
</div>
</div>
</div>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>Next we carry out some basic preprocessing. The column transformer first applies ordinal encoding to <code>Area</code> and <code>VehGas</code> variables, and applies standard scaling to all remaining numerical values. To simplify things, <code>VehBrand</code> and <code>Region</code> variables are dropped from the dataframe. The column transformer is then applied to both training and test sets.</p>
<div id="9ce3a30a" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  sev.drop(<span class="st">"ClaimAmount"</span>, axis<span class="op">=</span><span class="dv">1</span>), sev[<span class="st">"ClaimAmount"</span>], random_state<span class="op">=</span><span class="dv">2023</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>ct <span class="op">=</span> make_column_transformer((OrdinalEncoder(), [<span class="st">"Area"</span>, <span class="st">"VehGas"</span>]),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"drop"</span>, [<span class="st">"VehBrand"</span>, <span class="st">"Region"</span>]), remainder<span class="op">=</span>StandardScaler())</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> ct.fit_transform(X_train)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> ct.transform(X_test)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.hist(y_train[y_train <span class="op">&lt;</span> <span class="dv">5000</span>], bins<span class="op">=</span><span class="dv">30</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Plotting the empirical distribution of the target variable <code>ClaimAmount</code> help us get an understanding of the inherent variability associated with the data.</p>
<p>The following section illustrates how embedding a GLM in a neural network architecture can help us quantify the uncertainty relating to the predictions coming from the neural network. The idea is to first fit a GLM, and use the predictions from the GLM and predictions from the neural network part to define a custom loss function. This embedding presents an opportunity to compute the dispersion parameter <span class="math inline">\phi_{CANN}</span> for the neural network. </p>
<p>The idea of GLM is to find a linear combination of independent variables <span class="math inline">\boldsymbol{x}</span> and coefficients <span class="math inline">\boldsymbol{\beta}</span>, apply a non-linear transformation (<span class="math inline">g^{-1}</span>) to that linear combination and set it equal to conditional mean of the response variable <span class="math inline">Y</span> given an instance <span class="math inline">\boldsymbol{x}</span>. The non-linear transformation provides added flexibility.</p>
</section>
<section id="doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="doesnt-prove-that-y-boldsymbolx-boldsymbolx-is-multimodal">Doesn’t prove that <span class="math inline">Y | \boldsymbol{X} = \boldsymbol{x}</span> is multimodal</h3>
<div id="a4f94406" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make some example where the distribution is multimodal because of a binary covariate which separates the means of the two distributions</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> y_train.max()</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate some data from an exponential distribution which has Pr(X &lt; 1000) = 0.9</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="op">-</span>np.log(p) <span class="op">/</span> <span class="dv">1000</span> </span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> lambda_</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>y_1 <span class="op">=</span> np.random.exponential(scale<span class="op">=</span>mu, size<span class="op">=</span>n)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick a truncated normal distribution with a mean of 1100 and std of 250 (truncated to be positive)</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">1100</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>y_2 <span class="op">=</span> stats.truncnorm.rvs((<span class="dv">0</span> <span class="op">-</span> mu) <span class="op">/</span> sigma, (np.inf <span class="op">-</span> mu) <span class="op">/</span> sigma, loc<span class="op">=</span>mu, scale<span class="op">=</span>sigma, size<span class="op">=</span>n)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine y_1 and y_2 for the final histogram</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.concatenate([y_1, y_2])</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine common bins</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> np.histogram_bin_edges(y, bins<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        ax.hist(y_1, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>colors[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="ss">f'$f(y | x = 1)$'</span>)</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>        ax.hist(y_2, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>colors[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="ss">f'$f(y | x = 2)$'</span>)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        ax.hist(y, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="ss">f'$f(y)$'</span>)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gamma-glm" class="level3">
<h3 class="anchored" data-anchor-id="gamma-glm">Gamma GLM</h3>
<p>Suppose a fitted gamma GLM model has</p>
<ul>
<li>a log link function <span class="math inline">g(x)=\log(x)</span> and</li>
<li>regression coefficients <span class="math inline">\boldsymbol{\beta}=(\beta_0, \beta_1, \beta_2, \beta_3)</span>.</li>
</ul>
<p>Then, it estimates the conditional mean of <span class="math inline">Y</span> given a new instance <span class="math inline">\boldsymbol{x}=(1, x_1, x_2, x_3)</span> as follows: <span class="math display">
    \mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}] = g^{-1}(\langle \boldsymbol{\beta}, \boldsymbol{x}\rangle) = \exp\big(\beta_0 + \beta_1 x_1 + beta_2 x_2 + \beta_3 x_3 \big).
</span></p>
<p>A GLM can model any other exponential family distribution using an appropriate link function <span class="math inline">g</span>.</p>
</section>
<section id="gamma-glm-loss" class="level3">
<h3 class="anchored" data-anchor-id="gamma-glm-loss">Gamma GLM loss</h3>
<p>If <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> is a gamma r.v. with mean <span class="math inline">\mu(\boldsymbol{x}; \boldsymbol{\beta})</span> and dispersion parameter <span class="math inline">\phi</span>, we can minimise the negative log-likelihood (NLL) <span class="math display">
    \text{NLL} \propto \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})} + \text{const},
</span> i.e., we ignore the dispersion parameter <span class="math inline">\phi</span> while estimating the regression coefficients.</p>
</section>
<section id="fitting-steps" class="level3">
<h3 class="anchored" data-anchor-id="fitting-steps">Fitting Steps</h3>
<p>Step 1. Use the advanced second derivative iterative method to find the regression coefficients: <span class="math display">
    \widehat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{arg\,min}} \ \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})}
</span></p>
<p>Step 2. Estimate the dispersion parameter: <span class="math display">
    \phi = \frac{1}{n-p}\sum_{i=1}^{n}\frac{\bigl(y_i-\mu(\boldsymbol{x}_i; \boldsymbol{\beta})\bigr)^2}{\mu(\boldsymbol{x}_i; \boldsymbol{\beta} )^2}
</span></p>
<p>(Here, <span class="math inline">p</span> is the number of coefficients in the model. If this <span class="math inline">p</span> doesn’t include the intercept, then <span class="math inline">p</span> should be use <span class="math inline">\frac{1}{n-(p+1)}</span>.)</p>
</section>
<section id="code-gamma-glm" class="level3">
<h3 class="anchored" data-anchor-id="code-gamma-glm">Code: Gamma GLM</h3>
<p>In Python, we can fit a gamma GLM as follows:</p>
<div id="fac08385" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a column of ones to include an intercept in the model</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>X_train_design <span class="op">=</span> sm.add_constant(X_train)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Gamma GLM with a log link function</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>gamma_glm <span class="op">=</span> sm.GLM(y_train, X_train_design,                   </span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>            family<span class="op">=</span>sm.families.Gamma(sm.families.links.Log()))</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>gamma_glm <span class="op">=</span> gamma_glm.fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div id="6cc83e1e" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>gamma_glm.params</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>const                    7.786576
ordinalencoder__Area    -0.073226
                           ...   
remainder__BonusMalus    0.157204
remainder__Density       0.010539
Length: 9, dtype: float64</code></pre>
</div>
</div>
</div><div class="column">
<div id="fcdda5ca" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dispersion Parameter</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> gamma_glm.predict(X_train_design)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_train <span class="op">-</span> mus</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>dof <span class="op">=</span> (<span class="bu">len</span>(y_train)<span class="op">-</span>X_train_design.shape[<span class="dv">1</span>])</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>phi_glm <span class="op">=</span> np.sum(residuals<span class="op">**</span><span class="dv">2</span><span class="op">/</span>mus<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>dof</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(phi_glm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>59.63363123735805</code></pre>
</div>
</div>
</div>
</div>
<p>The above example of fitting a Gamma distribution assumes a constant dispersion, meaning that, the dispersion of claim amount is constant for all policyholders. If we believe that the constant dispersion assumption is quite strong, we can use a double GLM model. Fitting a GLM is the traditional way of modelling a claim amount.</p>
</section>
<section id="ann-can-feed-into-a-glm" class="level3">
<h3 class="anchored" data-anchor-id="ann-can-feed-into-a-glm">ANN can feed into a GLM</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="richman-glm-and-ann.png" class="img-fluid figure-img"></p>
<figcaption>Combining GLM &amp; ANN.</figcaption>
</figure>
</div>
<div class="footer">
<p>Source: Ronald Richman (2022), Mind the Gap - Safely Incorporating Deep Learning Models into the Actuarial Toolkit, IFoA seminar, Slide 14.</p>
</div>
</section>
</section>
<section id="combined-actuarial-neural-network" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="combined-actuarial-neural-network">Combined Actuarial Neural Network</h2>
<section id="cann" class="level3">
<h3 class="anchored" data-anchor-id="cann">CANN</h3>
<p>The Combined Actuarial Neural Network is a novel actuarial neural network architecture proposed by <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525">Schelldorfer and Wüthrich (2019)</a>. We summarise the CANN approach as follows:</p>
<ul>
<li>Find the coefficients <span class="math inline">\boldsymbol{\beta}</span> of the GLM with a link function <span class="math inline">g(\cdot)</span>.</li>
<li>Find the weights <span class="math inline">\boldsymbol{w}_{\text{CANN}}</span> of a neural network <span class="math inline">\mathcal{M}_{\text{CANN}}:\mathbb{R}^{p}\to\mathbb{R}</span>.</li>
<li>Given a new instance <span class="math inline">\boldsymbol{x}</span>, we have <span class="math display">\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}] = g^{-1}\Big( \langle\boldsymbol{\beta}, \boldsymbol{x}\rangle + \mathcal{M}_{\text{CANN}}(\boldsymbol{x};\boldsymbol{w}_{\text{CANN}})\Big).</span></li>
</ul>
</section>
<section id="shifting-the-predicted-distributions" class="level3">
<h3 class="anchored" data-anchor-id="shifting-the-predicted-distributions">Shifting the predicted distributions</h3>
<div id="bf2e3456" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure reproducibility</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a 4x1 grid of plots</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the x-axis</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a few gamma distribution pdfs with different means.</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Then plot gamma distributions with shifted means and the same dispersion parameter.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>glm_means <span class="op">=</span> [<span class="dv">1000</span>, <span class="dv">3000</span>, <span class="dv">2000</span>, <span class="dv">4000</span>]</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>cann_means <span class="op">=</span> [<span class="dv">1500</span>, <span class="dv">1400</span>, <span class="dv">3000</span>, <span class="dv">5000</span>]</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_grid, stats.gamma.pdf(x_grid, a<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>glm_means[i]<span class="op">/</span><span class="dv">2</span>), label<span class="op">=</span><span class="ss">f'GLM'</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_grid, stats.gamma.pdf(x_grid, a<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>cann_means[i]<span class="op">/</span><span class="dv">2</span>), label<span class="op">=</span><span class="ss">f'CANN'</span>)</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="ss">f'$f(y | x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">)$'</span>)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        ax.legend([<span class="st">"GLM"</span>, <span class="st">"CANN"</span>], loc<span class="op">=</span><span class="st">"upper right"</span>, ncol<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="cann-architecture.png" class="img-fluid figure-img"></p>
<figcaption>The CANN architecture.</figcaption>
</figure>
</div>
<div class="footer">
<p>Source: Schelldorfer and Wüthrich (2019), <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525">Nesting Classical Actuarial Models into Neural Networks</a>, SSRN, Figure 8.</p>
</div>
</section>
<section id="code-architecture" class="level3">
<h3 class="anchored" data-anchor-id="code-architecture">Code: Architecture</h3>
<div id="1fb68f35" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="annotated-cell-24"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-24-1"><a href="#annotated-cell-24-1" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="annotated-cell-24-2"><a href="#annotated-cell-24-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>X_train.shape[<span class="dv">1</span>:])</span>
<span id="annotated-cell-24-3"><a href="#annotated-cell-24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-24-4"><a href="#annotated-cell-24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># GLM part (won't be updated during training)</span></span>
<span id="annotated-cell-24-5"><a href="#annotated-cell-24-5" aria-hidden="true" tabindex="-1"></a>glm_weights <span class="op">=</span> gamma_glm.params.iloc[<span class="dv">1</span>:].values.reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="annotated-cell-24-6"><a href="#annotated-cell-24-6" aria-hidden="true" tabindex="-1"></a>glm_bias <span class="op">=</span> gamma_glm.params.iloc[<span class="dv">0</span>]  </span>
<span id="annotated-cell-24-7"><a href="#annotated-cell-24-7" aria-hidden="true" tabindex="-1"></a>glm_part <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'linear'</span>, trainable<span class="op">=</span><span class="va">False</span>,</span>
<span id="annotated-cell-24-8"><a href="#annotated-cell-24-8" aria-hidden="true" tabindex="-1"></a>                     kernel_initializer<span class="op">=</span>Constant(glm_weights),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-24" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-24-9" class="code-annotation-target"><a href="#annotated-cell-24-9" aria-hidden="true" tabindex="-1"></a>                     bias_initializer<span class="op">=</span>Constant(glm_bias))(inputs)</span>
<span id="annotated-cell-24-10"><a href="#annotated-cell-24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-24-11"><a href="#annotated-cell-24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Neural network part</span></span>
<span id="annotated-cell-24-12"><a href="#annotated-cell-24-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'leaky_relu'</span>)(inputs)</span>
<span id="annotated-cell-24-13"><a href="#annotated-cell-24-13" aria-hidden="true" tabindex="-1"></a>nn_part <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'linear'</span>)(x) </span>
<span id="annotated-cell-24-14"><a href="#annotated-cell-24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-24-15"><a href="#annotated-cell-24-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine GLM and CANN estimates</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-24" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-24-16" class="code-annotation-target"><a href="#annotated-cell-24-16" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> keras.ops.exp(glm_part <span class="op">+</span> nn_part)</span>
<span id="annotated-cell-24-17"><a href="#annotated-cell-24-17" aria-hidden="true" tabindex="-1"></a>cann <span class="op">=</span> Model(inputs, mu)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-24" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-24" data-code-lines="9" data-code-annotation="1">Adds a <code>Dense</code> layer with just one neuron, to store the model output (before inverse link function) from the GLM. The linear activation is used to make sure that the output is a linear combination of inputs. The weights are set to be non-trainable, hence the values obtained during GLM fitting will not change during the neural network training process. <code>kernel_initializer=Constant(glm_weights)</code> and <code>bias_initializer=Constant(glm_bias)</code> ensures that weights are initialized with the optimal values estimated from GLM fit.</span>
</dd>
<dt data-target-cell="annotated-cell-24" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-24" data-code-lines="16" data-code-annotation="2">Add the GLM contribution to the neural network output and exponentiate to get the mean estimate.</span>
</dd>
</dl>
</div>
</div>
<p>Since this CANN predicts gamma distributions, we use the gamma NLL loss function.</p>
<div id="7f7c03fa" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cann_negative_log_likelihood(y_true, y_pred):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> keras.ops.mean(keras.ops.log(y_pred) <span class="op">+</span> y_true<span class="op">/</span>y_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="code-model-training" class="level3">
<h3 class="anchored" data-anchor-id="code-model-training">Code: Model Training</h3>
<div id="58cc64bd" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="annotated-cell-26"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><a class="code-annotation-anchor" data-target-cell="annotated-cell-26" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-26-1" class="code-annotation-target"><a href="#annotated-cell-26-1" aria-hidden="true" tabindex="-1"></a>cann.compile(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span>cann_negative_log_likelihood)</span>
<span id="annotated-cell-26-2"><a href="#annotated-cell-26-2" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> cann.fit(X_train, y_train,</span>
<span id="annotated-cell-26-3"><a href="#annotated-cell-26-3" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="annotated-cell-26-4"><a href="#annotated-cell-26-4" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[EarlyStopping(patience<span class="op">=</span><span class="dv">10</span>)],  </span>
<span id="annotated-cell-26-5"><a href="#annotated-cell-26-5" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="annotated-cell-26-6"><a href="#annotated-cell-26-6" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>, </span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-26" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-26-7" class="code-annotation-target"><a href="#annotated-cell-26-7" aria-hidden="true" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-26" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-26" data-code-lines="1" data-code-annotation="1">Compiles the model with adam optimizer and the custom loss function</span>
</dd>
<dt data-target-cell="annotated-cell-26" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-26" data-code-lines="7" data-code-annotation="2">Fits the model (with a validation split defined inside the fit function)</span>
</dd>
</dl>
</div>
</div>
<p>Find the dispersion parameter.</p>
<div id="e4572b63" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> cann.predict(X_train, verbose<span class="op">=</span><span class="dv">0</span>).flatten()</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_train <span class="op">-</span> mus</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>dof <span class="op">=</span> (<span class="bu">len</span>(y_train)<span class="op">-</span>(X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>phi_cann <span class="op">=</span> np.sum(residuals<span class="op">**</span><span class="dv">2</span><span class="op">/</span>mus<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> dof</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(phi_cann)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>31.171623242378978</code></pre>
</div>
</div>
</section>
</section>
<section id="mixture-density-network" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="mixture-density-network">Mixture Density Network</h2>
<section id="mixture-distribution" class="level3">
<h3 class="anchored" data-anchor-id="mixture-distribution">Mixture Distribution</h3>
<p>One intuitive way to capture uncertainty using neural networks would be to estimate the parameters of the target distribution, instead of predicting the value it self. For example, suppose we want to predict <span class="math inline">y</span> coming from a Gaussian distribution. Most common method would be to predict <span class="math inline">(\hat{y})</span> directly using a single neuron at the output layer. Another possible way would be to estimate the parameters (<span class="math inline">\mu</span> and <span class="math inline">\sigma</span>) of the <span class="math inline">y</span> distribution using 2 neurons at the output layer. Estimating parameters of the distribution instead of point estimates for <span class="math inline">y</span> can help us get an idea about the uncertainty. However, assuming distributional properties at times could be too restrictive. For example, it is possible that the actual distribution of <span class="math inline">y</span> values is bimodal or multi modal. In such situations, assuming a mixture distribution is more intuitive.</p>
<p>Given a finite set of resulting random variables <span class="math inline">(Y_1, \ldots, Y_{K})</span>, one can generate a multinomial random variable <span class="math inline">Y\sim \text{Multinomial}(1, \boldsymbol{\pi})</span>. Meanwhile, <span class="math inline">Y</span> can be regarded as a mixture of <span class="math inline">Y_1, \ldots, Y_{K}</span>, i.e., <span class="math display">
  Y = \begin{cases}
      Y_1 &amp; \text{w.p. } \pi_1, \\
      \vdots &amp; \vdots\\
      Y_K &amp; \text{w.p. } \pi_K, \\
  \end{cases}
</span> where we define a set of finite set of weights <span class="math inline">\boldsymbol{\pi}=(\pi_{1} \ldots, \pi_{K})</span> such that <span class="math inline">\pi_k \ge 0</span> for <span class="math inline">k \in \{1, \ldots, K\}</span> and <span class="math inline">\sum_{k=1}^{K}\pi_k=1</span>.</p>
</section>
<section id="mixture-distribution-1" class="level3">
<h3 class="anchored" data-anchor-id="mixture-distribution-1">Mixture Distribution</h3>
<p>Let <span class="math inline">f_{Y_k|\boldsymbol{X}}</span> and <span class="math inline">F_{Y_k|\boldsymbol{X}}</span> be the p.d.f. and the c.d.f of <span class="math inline">Y_k|\boldsymbol{X}</span> for all <span class="math inline">k \in \{1, \ldots, K\}</span>.</p>
<p>The random variable <span class="math inline">Y|\boldsymbol{X}</span>, which mixes <span class="math inline">Y_k|\boldsymbol{X}</span>’s with weights <span class="math inline">\pi_k</span>’s, has the density function <span class="math display">
    f_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) f_{k}(y|\boldsymbol{x}),
</span> and the cumulative density function <span class="math display">
    F_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) F_{k}(y|\boldsymbol{x}).
</span></p>
</section>
<section id="mixture-density-network-1" class="level3">
<h3 class="anchored" data-anchor-id="mixture-density-network-1">Mixture Density Network</h3>
<p>A mixture density network (MDN) <span class="math inline">\mathcal{M}_{\boldsymbol{w}^*}</span> outputs each distribution component’s mixing weights and parameters of <span class="math inline">Y</span> given the input features <span class="math inline">\boldsymbol{x}</span>, i.e., <span class="math display">
    \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})=(\boldsymbol{\pi}(\boldsymbol{x};\boldsymbol{w}^*), \boldsymbol{\theta}(\boldsymbol{x};\boldsymbol{w}^*)),
</span> where <span class="math inline">\boldsymbol{w}^*</span> is the networks’ weights found by minimising the following negative log-likelihood loss function <span class="math display">
    \mathcal{L}(\mathcal{D}, \boldsymbol{\theta})= - \sum_{i=1}^{n} \log f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w}^*),
</span> where <span class="math inline">\mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}</span> is the training dataset.</p>
</section>
<section id="mixture-density-network-2" class="level3">
<h3 class="anchored" data-anchor-id="mixture-density-network-2">Mixture Density Network</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="MDN.png" class="img-fluid figure-img"></p>
<figcaption>An MDN that outputs the parameters for a <span class="math inline">K</span> component mixture distribution. <span class="math inline">\boldsymbol{\theta}_k(\boldsymbol{x}; \boldsymbol{w}^*)= (\theta_{k,1}(\boldsymbol{x}; \boldsymbol{w}^*), \ldots, \theta_{k,|\boldsymbol{\theta}_k|}(\boldsymbol{x}; \boldsymbol{w}^*))</span> consists of the parameter estimates for the <span class="math inline">k</span>th mixture component.</figcaption>
</figure>
</div>
</section>
<section id="model-specification" class="level3">
<h3 class="anchored" data-anchor-id="model-specification">Model Specification</h3>
<p>Suppose there are two types of claims:</p>
<ul>
<li>Type I: <span class="math inline">Y_1|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_1(\boldsymbol{x}), \beta_1(\boldsymbol{x}))</span> and,</li>
<li>Type II: <span class="math inline">Y_2|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_2(\boldsymbol{x}), \beta_2(\boldsymbol{x}))</span>.</li>
</ul>
<p>The density of the actual claim amount <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x}</span> follows <span class="math display">
    \begin{align*}
        f_{Y|\boldsymbol{X}}(y|\boldsymbol{x})
        &amp;= \pi_1(\boldsymbol{x})\cdot \frac{\beta_1(\boldsymbol{x})^{\alpha_1(\boldsymbol{x})}}{\Gamma(\alpha_1(\boldsymbol{x}))}\mathrm{e}^{-\beta_1(\boldsymbol{x})y}y^{\alpha_1(\boldsymbol{x})-1} \\
        &amp;\quad + (1-\pi_1(\boldsymbol{x}))\cdot \frac{\beta_2(\boldsymbol{x})^{\alpha_2(\boldsymbol{x})}}{\Gamma(\alpha_2(\boldsymbol{x}))}\mathrm{e}^{-\beta_2(\boldsymbol{x})y}y^{\alpha_2(\boldsymbol{x})-1}.
    \end{align*}
</span> where <span class="math inline">\pi_1(\boldsymbol{x})</span> is the probability of a Type I claim given <span class="math inline">\boldsymbol{x}</span>.</p>
</section>
<section id="output" class="level3">
<h3 class="anchored" data-anchor-id="output">Output</h3>
<p>The aim is to find the optimum weights <span class="math display">
    \boldsymbol{w}^* = \underset{w}{\text{arg\,min}} \ \mathcal{L}(\mathcal{D}, \boldsymbol{w})
</span> for the Gamma mixture density network <span class="math inline">\mathcal{M}_{\boldsymbol{w}^*}</span> that outputs the mixing weights, shapes and scales of <span class="math inline">Y</span> given the input features <span class="math inline">\boldsymbol{x}</span>, i.e., <span class="math display">
    \begin{align*}
        \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})
        = ( &amp;\pi_1(\boldsymbol{x}; \boldsymbol{w}^*),
             \pi_2(\boldsymbol{x}; \boldsymbol{w}^*), \\
            &amp;\alpha_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \alpha_2(\boldsymbol{x}; \boldsymbol{w}^*), \\
            &amp;\beta_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \beta_2(\boldsymbol{x}; \boldsymbol{w}^*)
        ).
    \end{align*}
</span></p>
</section>
<section id="architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="architecture-1">Architecture</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Gamma_MDN.png" class="img-fluid figure-img"></p>
<figcaption>We demonstrate the structure of a gamma MDN that outputs the parameters for a gamma mixture with two components.</figcaption>
</figure>
</div>
</section>
<section id="code-import-legacy-keras-for-now" class="level3">
<h3 class="anchored" data-anchor-id="code-import-legacy-keras-for-now">Code: Import “legacy” Keras (for now)</h3>
<div id="d4095ac2" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tf_keras</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="keras_tfp_github_issue.png" class="img-fluid"> <img src="keras_tfp_github_issue_comment.png" class="img-fluid"></p>
<div class="footer">
<p>Source: Tensorflow Probability GitHub, <a href="https://github.com/tensorflow/probability/issues/1774#issuecomment-1841706103">Keras 3 breaks Tensorflow Probability upon import</a>, issue #1774.</p>
</div>
</section>
<section id="code-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="code-architecture-1">Code: Architecture</h3>
<p>The following code resembles the architecture of the architecture of the gamma MDN from the previous slide.</p>
<div id="4e25ed8f" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="annotated-cell-29"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-29-1"><a href="#annotated-cell-29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure reproducibility</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-29" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-29-2" class="code-annotation-target"><a href="#annotated-cell-29-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)<span class="op">;</span></span>
<span id="annotated-cell-29-3"><a href="#annotated-cell-29-3" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-29" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-29-4" class="code-annotation-target"><a href="#annotated-cell-29-4" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tf_keras.layers.Input(shape<span class="op">=</span>X_train.shape[<span class="dv">1</span>:])</span>
<span id="annotated-cell-29-5"><a href="#annotated-cell-29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-29-6"><a href="#annotated-cell-29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Two hidden layers </span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-29" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-29-7" class="code-annotation-target"><a href="#annotated-cell-29-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="annotated-cell-29-8"><a href="#annotated-cell-29-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="annotated-cell-29-9"><a href="#annotated-cell-29-9" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-29" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-29-10" class="code-annotation-target"><a href="#annotated-cell-29-10" aria-hidden="true" tabindex="-1"></a>pis <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x) <span class="co"># Mixing weights</span></span>
<span id="annotated-cell-29-11"><a href="#annotated-cell-29-11" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'exponential'</span>)(x) <span class="co"># Shape parameters</span></span>
<span id="annotated-cell-29-12"><a href="#annotated-cell-29-12" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> tf_keras.layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'exponential'</span>)(x) <span class="co"># Scale parameters</span></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-29" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-29-13" class="code-annotation-target"><a href="#annotated-cell-29-13" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> tf_keras.layers.Concatenate(axis<span class="op">=</span><span class="dv">1</span>)([pis, alphas, betas]) <span class="co"># shape = (None, 6)</span></span>
<span id="annotated-cell-29-14"><a href="#annotated-cell-29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-29-15"><a href="#annotated-cell-29-15" aria-hidden="true" tabindex="-1"></a>gamma_mdn <span class="op">=</span> tf_keras.Model(inputs, out)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-29" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-29" data-code-lines="2" data-code-annotation="1">Sets the random seeds for reproducibility</span>
</dd>
<dt data-target-cell="annotated-cell-29" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-29" data-code-lines="4" data-code-annotation="2">Defines the input layer with the number of neurons being equal to the number of input features</span>
</dd>
<dt data-target-cell="annotated-cell-29" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-29" data-code-lines="7" data-code-annotation="3">Specifies the hidden layers of the neural network</span>
</dd>
<dt data-target-cell="annotated-cell-29" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-29" data-code-lines="10" data-code-annotation="4">Specifies the neurons of the output layer. Here, <code>softmax</code> is used for <span class="math inline">\pi</span> values as they must sum up to 1. <code>exponential</code> activation is used for both <span class="math inline">\alpha</span>’s and <span class="math inline">\beta</span>’s as they must be non-negative.</span>
</dd>
<dt data-target-cell="annotated-cell-29" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-29" data-code-lines="13" data-code-annotation="5">Combines all of the outputs since Keras’ loss function requires a single output (which will now have 6 columns).</span>
</dd>
</dl>
</div>
</div>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>The negative log-likelihood loss function is given by</p>
<p><span class="math display">
    \mathcal{L}(\mathcal{D}, \boldsymbol{w})
    = - \frac{1}{n} \sum_{i=1}^{n} \log \  f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w})
</span> where the <span class="math inline">f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w})</span> is defined by <span class="math display">
\begin{align*}
    &amp;\pi_1(\boldsymbol{x};\boldsymbol{w})\cdot \frac{\beta_1(\boldsymbol{x};\boldsymbol{w})^{\alpha_1(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_1(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_1(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_1(\boldsymbol{x};\boldsymbol{w})-1} \\
    &amp; \quad + (1-\pi_1(\boldsymbol{x};\boldsymbol{w}))\cdot \frac{\beta_2(\boldsymbol{x};\boldsymbol{w})^{\alpha_2(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_2(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_2(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_2(\boldsymbol{x};\boldsymbol{w})-1}
\end{align*}
</span></p>
</section>
<section id="code-loss-training" class="level3">
<h3 class="anchored" data-anchor-id="code-loss-training">Code: Loss &amp; training</h3>
<p><code>tensorflow_probability</code> to the rescue.</p>
<div id="d8b2afb8" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="annotated-cell-30"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><a class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-30-1" class="code-annotation-target"><a href="#annotated-cell-30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability <span class="im">as</span> tfp</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-30-2" class="code-annotation-target"><a href="#annotated-cell-30-2" aria-hidden="true" tabindex="-1"></a>tfd <span class="op">=</span> tfp.distributions</span>
<span id="annotated-cell-30-3"><a href="#annotated-cell-30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-30-4"><a href="#annotated-cell-30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gamma_mixture_nll(y_true, y_pred):   </span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-30-5" class="code-annotation-target"><a href="#annotated-cell-30-5" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> y_pred.shape[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">3</span></span>
<span id="annotated-cell-30-6"><a href="#annotated-cell-30-6" aria-hidden="true" tabindex="-1"></a>    pis <span class="op">=</span> y_pred[:, :K]                                                    </span>
<span id="annotated-cell-30-7"><a href="#annotated-cell-30-7" aria-hidden="true" tabindex="-1"></a>    alphas <span class="op">=</span> y_pred[:, K:<span class="dv">2</span><span class="op">*</span>K]                                               </span>
<span id="annotated-cell-30-8"><a href="#annotated-cell-30-8" aria-hidden="true" tabindex="-1"></a>    betas <span class="op">=</span> y_pred[:, <span class="dv">2</span><span class="op">*</span>K:<span class="dv">3</span><span class="op">*</span>K]</span>
<span id="annotated-cell-30-9"><a href="#annotated-cell-30-9" aria-hidden="true" tabindex="-1"></a>    mixture_distribution <span class="op">=</span> tfd.MixtureSameFamily(</span>
<span id="annotated-cell-30-10"><a href="#annotated-cell-30-10" aria-hidden="true" tabindex="-1"></a>        mixture_distribution<span class="op">=</span>tfd.Categorical(probs<span class="op">=</span>pis),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-30-11" class="code-annotation-target"><a href="#annotated-cell-30-11" aria-hidden="true" tabindex="-1"></a>        components_distribution<span class="op">=</span>tfd.Gamma(alphas, betas))</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-30" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-30-12" class="code-annotation-target"><a href="#annotated-cell-30-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>tf_keras.backend.mean(mixture_distribution.log_prob(y_true))</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-30" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="1" data-code-annotation="1">Imports <code>tfp</code> class from <code>tensorflow_probability</code></span>
</dd>
<dt data-target-cell="annotated-cell-30" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="2" data-code-annotation="2">Stores statistical distributions in the <code>tfp</code> class as <code>tfd</code></span>
</dd>
<dt data-target-cell="annotated-cell-30" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="5" data-code-annotation="3">Extracts predicted values for all model components and stores them in separate matrices</span>
</dd>
<dt data-target-cell="annotated-cell-30" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="11" data-code-annotation="4">Specifies the mixture distribution using computed model components</span>
</dd>
<dt data-target-cell="annotated-cell-30" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-30" data-code-lines="12" data-code-annotation="5">Use the fitted model to calculate negative log likelihood given the observed data</span>
</dd>
</dl>
</div>
</div>
<div id="033e616e" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="annotated-cell-31"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><a class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-31-1" class="code-annotation-target"><a href="#annotated-cell-31-1" aria-hidden="true" tabindex="-1"></a>gamma_mdn.compile(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span>gamma_mixture_nll)</span>
<span id="annotated-cell-31-2"><a href="#annotated-cell-31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-31-3"><a href="#annotated-cell-31-3" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> gamma_mdn.fit(X_train, y_train,</span>
<span id="annotated-cell-31-4"><a href="#annotated-cell-31-4" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="annotated-cell-31-5"><a href="#annotated-cell-31-5" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[tf_keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">10</span>)],  </span>
<span id="annotated-cell-31-6"><a href="#annotated-cell-31-6" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="annotated-cell-31-7"><a href="#annotated-cell-31-7" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>, </span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-31" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-31-8" class="code-annotation-target"><a href="#annotated-cell-31-8" aria-hidden="true" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-31" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="1" data-code-annotation="1">Compiles the model using <code>adam</code> optimizer and the <code>gamma_mixture_nll</code> (negative log likelihood) as the loss function</span>
</dd>
<dt data-target-cell="annotated-cell-31" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-31" data-code-lines="8" data-code-annotation="2">Fits the model using the training data, with a validation split</span>
</dd>
</dl>
</div>
</div>
</section>
</section>
<section id="metrics-for-distributional-regression" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="metrics-for-distributional-regression">Metrics for Distributional Regression</h2>
<section id="proper-scoring-rules" class="level3">
<h3 class="anchored" data-anchor-id="proper-scoring-rules">Proper Scoring Rules</h3>
<p>Proper scoring rules provide a summary measure for the performance of the probabilistic predictions. They are useful in comparing performances across models.</p>
<dl>
<dt>Definition</dt>
<dd>
<p>A <em>scoring rule</em> is the equivalent of a loss function for distributional regression.</p>
<p>Denote <span class="math inline">S(F, y)</span> to be the score given to the forecasted distribution <span class="math inline">F</span> and an observation <span class="math inline">y \in \mathbb{R}</span>.</p>
</dd>
<dt>Definition</dt>
<dd>
<p>A scoring rule is called <em>proper</em> if <span class="math display">
\mathbb{E}_{Y \sim Q} S(Q, Y) \le \mathbb{E}_{Y \sim Q} S(F, Y)
</span> for all <span class="math inline">F</span> and <span class="math inline">Q</span> distributions.</p>
<p>It is called <em>strictly proper</em> if equality holds only if <span class="math inline">F = Q</span>.</p>
</dd>
</dl>
</section>
<section id="example-proper-scoring-rules" class="level3">
<h3 class="anchored" data-anchor-id="example-proper-scoring-rules">Example Proper Scoring Rules</h3>
<dl>
<dt>Logarithmic Score (NLL)</dt>
<dd>
<p>The logarithmic score is defined as <span class="math display">
    \mathrm{LogS}(f, y) = - \log f(y),
</span> where <span class="math inline">f</span> is the predictive density.</p>
</dd>
<dt>Continuous Ranked Probability Score (CRPS)</dt>
<dd>
<p>The continuous ranked probability score is defined as <span class="math display">
    \mathrm{crps}(F, y) = \int_{-\infty}^{\infty} (F(t) - {1}_{t\ge y})^2 \ \mathrm{d}t,
</span> where <span class="math inline">F</span> is the predicted c.d.f.</p>
</dd>
</dl>
<div class="footer">
<p>See, e.g., Taggert (2023), <a href="https://nla.gov.au/nla.obj-3160938615/view">Estimation of CRPS for precipitation forecasts…</a>, BoM Research Report 079.</p>
</div>
</section>
<section id="likelihoods" class="level3">
<h3 class="anchored" data-anchor-id="likelihoods">Likelihoods</h3>
<div id="a0ef0928" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.polyval(coefficients, X_toy[:<span class="dv">4</span>])</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>y_pred[<span class="dv">2</span>] <span class="op">*=</span> <span class="fl">1.1</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>sigma_preds <span class="op">=</span> sigma_toy <span class="op">*</span> np.array([<span class="fl">1.0</span>, <span class="fl">3.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="fl">5.0</span>, <span class="fl">3.0</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> y_pred[:<span class="dv">4</span>].min() <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>x_max <span class="op">=</span> y_pred[:<span class="dv">4</span>].max() <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma_toy</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(x_min, x_max, <span class="dv">100</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each normal distribution with different means vertically</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axes):</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    y_grid <span class="op">=</span> stats.norm.pdf(x_grid, y_pred[i], sigma_preds[i])</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_grid, y_grid)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    ax.plot([y_toy[i], y_toy[i]], [<span class="dv">0</span>, stats.norm.pdf(y_toy[i], y_pred[i], sigma_preds[i])], color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    ax.scatter([y_toy[i]], [stats.norm.pdf(y_toy[i], y_pred[i], sigma_preds[i])], color<span class="op">=</span><span class="st">'red'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f'$f(y ; </span><span class="ch">\\</span><span class="ss">boldsymbol</span><span class="ch">{{</span><span class="ss">x</span><span class="ch">}}</span><span class="ss">_</span><span class="ch">{{</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">}}</span><span class="ss">)$'</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([y_pred[i]], labels<span class="op">=</span>[<span class="vs">r'$\mu_{'</span> <span class="op">+</span> <span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="vs">r'}$'</span>])</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ax.set_ylim(0, 0.25)</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn off the y axes</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>    ax.yaxis.set_visible(<span class="va">False</span>)</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-45-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="code-nll" class="level3">
<h3 class="anchored" data-anchor-id="code-nll">Code: NLL</h3>
<div id="0ed4c041" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gamma_nll(mean, dispersion, y):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate shape and scale parameters from mean and dispersion</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    shape <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> dispersion<span class="op">;</span> scale <span class="op">=</span> mean <span class="op">*</span> dispersion</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a gamma distribution object</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    gamma_dist <span class="op">=</span> stats.gamma(a<span class="op">=</span>shape, scale<span class="op">=</span>scale)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.mean(gamma_dist.logpdf(y))</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># GLM</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>X_test_design <span class="op">=</span> sm.add_constant(X_test)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> gamma_glm.predict(X_test_design)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>nll_glm <span class="op">=</span> gamma_nll(mus, phi_glm, y_test)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co"># CANN</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>mus <span class="op">=</span> cann.predict(X_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>nll_cann <span class="op">=</span> gamma_nll(mus, phi_cann, y_test)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="co"># MDN</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>nll_mdn <span class="op">=</span> gamma_mdn.evaluate(X_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-comparisons" class="level3">
<h3 class="anchored" data-anchor-id="model-comparisons">Model Comparisons</h3>
<div id="4e80bb52" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'GLM: </span><span class="sc">{</span>round(nll_glm, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'CANN: </span><span class="sc">{</span>round(nll_cann, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'MDN: </span><span class="sc">{</span>round(nll_mdn, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GLM: 11.02
CANN: 10.44
MDN: 8.67</code></pre>
</div>
</div>
<p>The above results show that MDN provides the lowest value for the Logarithmic Score (NLL). Low values for NLL indicate better calibration. One possible reason for the better performance of the MDN model (compared to the Gamma model) is the added flexibility from multiple modelling components. The multiple modelling components in the MDN model, together, can capture the inherent variation in the data better.</p>
</section>
</section>
<section id="aleatoric-and-epistemic-uncertainty" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="aleatoric-and-epistemic-uncertainty">Aleatoric and Epistemic Uncertainty</h2>
<p>Uncertainty in deep learning refers to the level of doubt one would have about the predictions made by an AI-driven algorithm. Identifying and quantifying different sources of uncertainty that could exist in AI-driven algorithms is therefore important to ensure a credible application.</p>
<section id="categories-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="categories-of-uncertainty">Categories of uncertainty</h3>
<p>There are two major categories of uncertainty in statistical or machine learning:</p>
<ul>
<li>Aleatoric uncertainty</li>
<li>Epistemic uncertainty</li>
</ul>
<p>Since there is no consensus on the definitions of aleatoric and epistemic uncertainty, we provide the most acknowledged definitions in the following slides.</p>
</section>
<section id="aleatoric-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="aleatoric-uncertainty">Aleatoric Uncertainty</h3>
<p>Aleatoric uncertainty refers to the inherent variability associated with the data generating process. Among many ways to capture the aleatoric uncertainty, (i) combining with probabilistic models and (ii) considering mixture models are two useful methods to quantify the inherent variability.</p>
<dl>
<dt>Qualitative Definition</dt>
<dd>
<p><em>Aleatoric uncertainty refers to the statistical variability and inherent noise with data distribution that modelling cannot explain.</em></p>
</dd>
<dt>Quantitative Definition</dt>
<dd>
<p><span class="math display">\text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}) = \mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}],</span>i.e., if <span class="math inline">Y|\boldsymbol{X}=\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma^2)</span>, the aleatoric uncertainty would be <span class="math inline">\sigma^2</span>. Simply, it is the conditional variance of the response variable <span class="math inline">Y</span> given features/covariates <span class="math inline">\boldsymbol{x}</span>.</p>
</dd>
</dl>
</section>
<section id="epistemic-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="epistemic-uncertainty">Epistemic Uncertainty</h3>
<dl>
<dt>Qualitative Definition</dt>
<dd>
<p><em>Epistemic uncertainty refers to the lack of knowledge, limited data information, parameter errors and model errors.</em></p>
</dd>
<dt>Quantitative Definition</dt>
<dd>
<p><span class="math display">\text{Epi}(Y|\boldsymbol{X}=\boldsymbol{x}) = \text{Uncertainty}(Y|\boldsymbol{X}=\boldsymbol{x}) - \text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}),</span></p>
</dd>
</dl>
<p>i.e., the total uncertainty subtracting the aleatoric uncertainty <span class="math inline">\mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}]</span> would be the epistemic uncertainty.</p>
</section>
<section id="sources-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="sources-of-uncertainty">Sources of uncertainty</h3>
<p>There are many sources of uncertainty in statistical or machine learning models. Parameter error stems primarily due to lack of data. Model error stems from assuming wrong distributional properties of the data. Data uncertainty arises due to the lack of confidence we may have about the quality of the collected data. Noisy data, inconsistent data, data with missing values or data with missing important variables can result in data uncertainty.</p>
<p><em>If you decide to predict the claim amount of an individual using a deep learning model, which source(s) of uncertainty are you dealing with?</em></p>
<ol type="1">
<li>The inherent variability of the data-generating process <span class="math inline">\rightarrow</span> aleatoric uncertainty.</li>
<li>Parameter error <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
<li>Model error <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
<li>Data uncertainty <span class="math inline">\rightarrow</span> epistemic uncertainty.</li>
</ol>
</section>
</section>
<section id="avoiding-overfitting" class="level2" data-visibility="uncounted">
<h2 data-visibility="uncounted" class="anchored" data-anchor-id="avoiding-overfitting">Avoiding Overfitting</h2>
<section id="traditional-regularisation" class="level3">
<h3 class="anchored" data-anchor-id="traditional-regularisation">Traditional regularisation</h3>
<p>Say all the <span class="math inline">m</span> weights (excluding biases) are in the vector <span class="math inline">\boldsymbol{\theta}</span>. If we change the loss function to <span class="math display">
\text{Loss}_{1:n}
= \frac{1}{n} \sum_{i=1}^n \text{Loss}_i
  + \lambda \sum_{j=1}^{m} \left| \theta_j \right|
</span></p>
<p>this would be using <span class="math inline">L^1</span> regularisation. A loss like</p>
<p><span class="math display">
\text{Loss}_{1:n}
= \frac{1}{n} \sum_{i=1}^n \text{Loss}_i
  + \lambda \sum_{j=1}^{m} \theta_j^2
</span></p>
<p>is called <span class="math inline">L^2</span> regularisation.</p>
</section>
<section id="regularisation-in-keras" class="level3">
<h3 class="anchored" data-anchor-id="regularisation-in-keras">Regularisation in Keras</h3>
<div id="b2ad5aee" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>features, target <span class="op">=</span> fetch_california_housing(as_frame<span class="op">=</span><span class="va">True</span>, return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>NUM_FEATURES <span class="op">=</span> <span class="bu">len</span>(features.columns)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>X_main, X_test, y_main, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    features, target, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    X_main, y_main, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>X_train_sc <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>X_val_sc <span class="op">=</span> scaler.transform(X_val)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>X_test_sc <span class="op">=</span> scaler.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="2a8397d8" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.regularizers <span class="im">import</span> L1, L2</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> l1_model(regulariser_strength<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>  random.seed(<span class="dv">123</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> Sequential([</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>      Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>L1(regulariser_strength)),</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>      Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>  ])</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>  model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>  model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">4</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> model</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> l2_model(regulariser_strength<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>  random.seed(<span class="dv">123</span>)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> Sequential([</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>      Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>L2(regulariser_strength)),</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>      Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>  ])</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>  model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>  model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">10</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="weights-before-after-l2" class="level3">
<h3 class="anchored" data-anchor-id="weights-before-after-l2">Weights before &amp; after <span class="math inline">L^2</span></h3>
<div class="columns">
<div class="column">
<div id="1f6c7745" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> l2_model(<span class="fl">0.0</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-51-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="3b0b097f" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> l2_model(<span class="fl">1.0</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-52-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="weights-before-after-l1" class="level3">
<h3 class="anchored" data-anchor-id="weights-before-after-l1">Weights before &amp; after <span class="math inline">L^1</span></h3>
<div class="columns">
<div class="column">
<div id="a01dc97d" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> l1_model(<span class="fl">0.0</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-53-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div><div class="column">
<div id="135e21bd" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> l1_model(<span class="fl">1.0</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model.layers[<span class="dv">0</span>].get_weights()[<span class="dv">0</span>].flatten()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of weights almost 0: </span><span class="sc">{</span>np<span class="sc">.</span>sum(np.abs(weights) <span class="op">&lt;</span> <span class="fl">1e-5</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>plt.hist(weights, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of weights almost 0: 36</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-54-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="early-stopping-regularisation" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping-regularisation">Early-stopping regularisation</h3>
<blockquote class="blockquote">
<p>A very different way to regularize iterative learning algorithms such as gradient descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping… It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch”.</p>
</blockquote>
<blockquote class="blockquote">
<p>Alternatively, you can try building a model with slightly more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting too much. Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.</p>
</blockquote>
<div class="footer">
<p>Source: Géron (2019), Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition, Chapters 4 and 10.</p>
</div>
</section>
</section>
<section id="dropout" class="level2">
<h2 class="anchored" data-anchor-id="dropout">Dropout</h2>
<section id="dropout-1" class="level3">
<h3 class="anchored" data-anchor-id="dropout-1">Dropout</h3>
<p>Dropout is one of the most popular methods for reducing the risk of overfitting. Dropout is the act of randomly selecting a proportion of neurons and deactivating them during each training iteration. It is a regularization technique that aims to reduce overfitting and improve the generalization ability of the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dropout.png" class="img-fluid figure-img"></p>
<figcaption>An example of neurons dropped during training.</figcaption>
</figure>
</div>
<div class="footer">
<p>Sources: Marcus Lautier (2022).</p>
</div>
</section>
<section id="dropout-quote-1" class="level3">
<h3 class="anchored" data-anchor-id="dropout-quote-1">Dropout quote #1</h3>
<blockquote class="blockquote">
<p>It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them.</p>
</blockquote>
<div class="footer">
<p>Source: Aurélien Géron (2019), <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, p.&nbsp;366</p>
</div>
</section>
<section id="dropout-quote-2" class="level3">
<h3 class="anchored" data-anchor-id="dropout-quote-2">Dropout quote #2</h3>
<blockquote class="blockquote">
<p>The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.</p>
</blockquote>
<div class="footer">
<p>Source: Aurélien Géron (2019), <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, p.&nbsp;366</p>
</div>
</section>
<section id="code-dropout" class="level3">
<h3 class="anchored" data-anchor-id="code-dropout">Code: Dropout</h3>
<p>Dropout is just another layer in Keras.</p>
<p>The following code shows how we can apply a dropout to each hidden layer in the neural network. The dropout rate for each layer is 0.2. There is also an option called <code>seed</code> in the <code>Dropout</code> function, which can be used to ensure reproducibility.</p>
<div id="34332bda" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dropout</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">2</span>)<span class="op">;</span> </span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    Dropout(<span class="fl">0.2</span>),</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">4</span>, verbose<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="code-dropout-after-training" class="level3">
<h3 class="anchored" data-anchor-id="code-dropout-after-training">Code: Dropout after training</h3>
<p>Making predictions is the same as any other model:</p>
<p>Dropout has no impact on model predictions because <code>Dropout</code> function is carried out only during the training stage. Once the model finishes its training (once the weights and biases are computed), all neurons together contribute to the predictions(no dropping out during the prediction stage). Therefore, predictions from the model will not change across different runs.</p>
<div class="columns">
<div class="column">
<div id="1bdfd6e8" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>model.predict(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>                  verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>array([[1.0587903],
       [1.2814349],
       [0.9994641]], dtype=float32)</code></pre>
</div>
</div>
</div><div class="column">
<div id="a3305657" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>model.predict(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>                  verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>array([[1.0587903],
       [1.2814349],
       [0.9994641]], dtype=float32)</code></pre>
</div>
</div>
</div>
</div>
<p>We can make the model think it is still training:</p>
<p>By setting the <code>training=True</code>, we can let drop out happen during prediction stage as well. This will change predictions for the same output different. This is known as the <em>Monte Carlo dropout</em>.</p>
<div class="columns">
<div class="column">
<div id="599ae4a0" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    training<span class="op">=</span><span class="va">True</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([[1.082524  ],
       [0.74211466],
       [1.1583111 ]], dtype=float32)</code></pre>
</div>
</div>
</div><div class="column">
<div id="74b3720d" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>model(X_train_sc.head(<span class="dv">3</span>),</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    training<span class="op">=</span><span class="va">True</span>).numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>array([[1.0132376],
       [1.2697867],
       [0.7800578]], dtype=float32)</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="dropout-limitation" class="level3">
<h3 class="anchored" data-anchor-id="dropout-limitation">Dropout Limitation</h3>
<ul>
<li>Increased Training Time: Since dropout introduces noise into the training process, it can make the training process slower.</li>
<li>Sensitivity to Dropout Rates: the performance of dropout is highly dependent on the chosen dropout rate.</li>
</ul>
</section>
<section id="accidental-dropout-dead-neurons" class="level3">
<h3 class="anchored" data-anchor-id="accidental-dropout-dead-neurons">Accidental dropout (“dead neurons”)</h3>
<p>My first ANN for California housing</p>
<div class="columns">
<div class="column">
<p><br></p>
<div id="519d7c1c" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">123</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> model.fit(X_train, y_train,</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>hist.history[<span class="st">"loss"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>[25089.478515625,
 12.956829071044922,
 13.395614624023438,
 7.074806213378906,
 5.800335884094238]</code></pre>
</div>
</div>
</div><div class="column">
<div id="5d8d2c70" class="cell" data-execution_count="61">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-62-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="find-dead-relu-neurons" class="level3">
<h3 class="anchored" data-anchor-id="find-dead-relu-neurons">Find dead ReLU neurons</h3>
<div id="946716ce" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>acts <span class="op">=</span> model.layers[<span class="dv">0</span>](X_train).numpy()</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, acts.shape)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>acts[:<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(12384, 8) (12384, 30)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>array([[261.458   , 502.33704 ,  93.64283 , ..., 537.54865 , 325.7366  ,
        398.99435 ],
       [ 18.983932,  52.9067  ,   0.      , ...,  28.361092,  10.988864,
         58.194595],
       [266.2954  , 517.58154 ,  98.64309 , ..., 553.68005 , 336.69986 ,
        411.61124 ]], dtype=float32)</code></pre>
</div>
</div>
<div id="35b51d57" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>dead <span class="op">=</span> acts.mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>np.sum(dead)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>7</code></pre>
</div>
</div>
<div id="c6060830" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> np.where(dead)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>acts[:, idx<span class="op">-</span><span class="dv">1</span>:idx<span class="op">+</span><span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>array([[ 0.      ,  0.      ,  0.      ],
       [18.991873,  0.      ,  0.      ],
       [ 0.      ,  0.      ,  0.      ],
       ...,
       [ 0.      ,  0.      ,  0.      ],
       [ 0.      ,  0.      ,  0.      ],
       [ 0.      ,  0.      ,  0.      ]], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="trying-different-seeds" class="level3">
<h3 class="anchored" data-anchor-id="trying-different-seeds">Trying different seeds</h3>
<p>Create a function which counts the number of dead ReLU neurons in the first hidden layer for a given seed:</p>
<div id="c6a98a08" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_dead(seed):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    acts <span class="op">=</span> hidden(X_train).numpy()</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sum(acts.mean(axis<span class="op">=</span><span class="dv">0</span>) <span class="op">==</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we can try out different seeds:</p>
<div id="9bc87c23" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>num_dead <span class="op">=</span> [count_dead(seed) <span class="cf">for</span> seed <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1_000</span>)]</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>np.median(num_dead)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>5.0</code></pre>
</div>
</div>
</section>
<section id="look-at-distribution-of-dead-relus" class="level3">
<h3 class="anchored" data-anchor-id="look-at-distribution-of-dead-relus">Look at distribution of dead ReLUs</h3>
<div id="b20f8055" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>labels, counts <span class="op">=</span> np.unique(num_dead, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>plt.bar(labels, counts, align<span class="op">=</span><span class="st">'center'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="distributional-regression_files/figure-html/cell-70-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="ensembles" class="level2">
<h2 class="anchored" data-anchor-id="ensembles">Ensembles</h2>
<section id="ensembles-1" class="level3">
<h3 class="anchored" data-anchor-id="ensembles-1">Ensembles</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ensemble.png" class="img-fluid figure-img"></p>
<figcaption>Combine many models to get better predictions.</figcaption>
</figure>
</div>
<div class="footer">
<p>Source: Marcus Lautier (2022).</p>
</div>
</section>
<section id="deep-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="deep-ensembles">Deep Ensembles</h3>
<p>Train <span class="math inline">M</span> neural networks with different random initial weights independently (even in parallel).</p>
<div id="4d6167ab" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_model(seed):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    random.seed(seed)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential([</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">30</span>, activation<span class="op">=</span><span class="st">"leaky_relu"</span>),</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"exponential"</span>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    model.compile(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    es <span class="op">=</span> EarlyStopping(restore_best_weights<span class="op">=</span><span class="va">True</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train_sc, y_train, epochs<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>        callbacks<span class="op">=</span>[es], validation_data<span class="op">=</span>(X_val_sc, y_val), verbose<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ea99415c" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>seeds <span class="op">=</span> <span class="bu">range</span>(M)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seed <span class="kw">in</span> seeds:</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    models.append(build_model(seed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="deep-ensembles-ii" class="level3">
<h3 class="anchored" data-anchor-id="deep-ensembles-ii">Deep Ensembles II</h3>
<p>Say the trained weights by <span class="math inline">\boldsymbol{w}^{(1)}, \ldots, \boldsymbol{w}^{(M)}</span>, then we get predictions <span class="math inline">\bigl\{ \hat{y}(\boldsymbol{x}; \boldsymbol{w}^{(m)}) \bigr\}_{m=1}^{M}</span></p>
<div id="bb8f7065" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> []</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> model <span class="kw">in</span> models:</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    y_preds.append(model.predict(X_test_sc, verbose<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> np.array(y_preds)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>y_preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>array([[[3.2801466 ],
        [0.76298356],
        [2.4068608 ],
        ...,
        [2.3385763 ],
        [2.1730225 ],
        [1.096715  ]],

       [[3.1832185 ],
        [0.72296774],
        [2.5727806 ],
        ...,
        [2.3812106 ],
        [2.27971   ],
        [1.06247   ]],

       [[3.0994337 ],
        [0.77855957],
        [2.6037261 ],
        ...,
        [2.5923505 ],
        [2.354589  ],
        [1.2019893 ]]], dtype=float32)</code></pre>
</div>
</div>
</section>


</section>

<div id="quarto-appendix" class="default"><section id="package-versions" class="level3 appendix" data-visibility="uncounted"><h2 class="anchored quarto-appendix-heading">Package Versions</h2><div class="quarto-appendix-contents">

<div id="bbff7fac" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> watermark <span class="im">import</span> watermark</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(watermark(python<span class="op">=</span><span class="va">True</span>, packages<span class="op">=</span><span class="st">"keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tensorflow_probability,tf_keras"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Python implementation: CPython
Python version       : 3.11.9
IPython version      : 8.24.0

keras                 : 3.3.3
matplotlib            : 3.9.0
numpy                 : 1.26.4
pandas                : 2.2.2
seaborn               : 0.13.2
scipy                 : 1.11.0
torch                 : 2.3.1
tensorflow            : 2.16.1
tensorflow_probability: 0.24.0
tf_keras              : 2.16.0
</code></pre>
</div>
</div>
</div></section><section id="glossary" class="level3 appendix" data-visibility="uncounted"><h2 class="anchored quarto-appendix-heading">Glossary</h2><div class="quarto-appendix-contents">

<div class="columns">
<div class="column">
<ul>
<li>aleatoric and epistemic uncertainty</li>
<li>combined actuarial neural network</li>
<li>dead ReLU</li>
<li>deep ensembles</li>
<li>distributional forecasts</li>
<li>dropout</li>
</ul>
</div><div class="column">
<ul>
<li>generalised linear model</li>
<li>mixture density network</li>
<li>mixture distribution</li>
<li>Monte Carlo dropout</li>
<li>proper scoring rule</li>
</ul>
</div>
</div>


</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Labs/backpropagation-lab.html" class="pagination-link" aria-label="Lab: Backpropagation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Lab: Backpropagation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Advanced-Topics/interpretability.html" class="pagination-link" aria-label="Interpretability">
        <span class="nav-page-text">Interpretability</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>