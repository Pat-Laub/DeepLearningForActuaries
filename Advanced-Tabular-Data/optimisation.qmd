---
title: Optimisation
include-in-header:
  text: <script defer src="https://pyscript.net/latest/pyscript.js"></script>
resources: "minimise-with-gradients.py"
---

```{python}
#| echo: false
#| warning: false
import matplotlib

import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    matplotlib.pyplot.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    matplotlib.pyplot.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
matplotlib.pyplot.rcParams["figure.dpi"] = 350
matplotlib.pyplot.rcParams["savefig.bbox"] = "tight"
matplotlib.pyplot.rcParams["font.family"] = "serif"

matplotlib.pyplot.rcParams["axes.spines.right"] = False
matplotlib.pyplot.rcParams["axes.spines.top"] = False


def squareFig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()


def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortestSide = min(xl[1], yl[1])
    matplotlib.pyplot.plot(
        [0, shortestSide], [0, shortestSide], color="black", linestyle="--"
    )


import pandas

pandas.options.display.max_rows = 6

import numpy

numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow

tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], "GPU")


def skip_empty(line):
    if line.strip() != "":
        print(line.strip())
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import random
import numpy as np
import pandas as pd
```

:::

{{< include _maths-logistic-regression.qmd >}}

# Optimisation {data-visibility="uncounted"}

## Gradient-based learning

```{=html}
<div style="font-size: 0px;">
  <py-config>packages = ["matplotlib"]</py-config>
  </div>
<div>
  <!-- Source for slider with current value shown: https://stackoverflow.com/a/18936328 -->
  Make a guess: <input type="range" min="1" max="100" value="50" class="slider" id="new_guess" oninput="this.nextElementSibling.value = this.value">
  <output>50</output><br>
  Show derivatives: <input type="checkbox" id="derivs">
  Reveal function: <input type="checkbox" id="reveal">
</div>
<div id="mpl" style="text-align: center;"></div>
<py-script output="mpl" src="minimise-with-gradients.py" />
```

## Gradient descent pitfalls

![Potential problems with gradient descent.](Geron-mls2_0406.png)

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.
:::

## Go over all the training data

<br>

Called _batch gradient descent_.

<br>

```python
for i in range(num_epochs):
    gradient = evaluate_gradient(loss_function, data, weights)
    weights = weights - learning_rate * gradient
```

## Pick a random training example

<br>

Called _stochastic gradient descent_.

<br>

```python
for i in range(num_epochs):
    rnd.shuffle(data)
    for example in data:
        gradient = evaluate_gradient(loss_function, example, weights)
        weights = weights - learning_rate * gradient
```

## Take a group of training examples

<br>

Called _mini-batch gradient descent_.

<br>

```python
for i in range(num_epochs):
    rnd.shuffle(data)
    for b in range(num_batches):
        batch = data[b * batch_size : (b + 1) * batch_size]
        gradient = evaluate_gradient(loss_function, batch, weights)
        weights = weights - learning_rate * gradient
```

## Mini-batch gradient descent

::: columns
::: column

Why?

1. Because we have to (data is too big)
2. Because it is faster (lots of quick noisy steps > a few slow super accurate steps)
3. The noise helps us jump out of local minima

:::
::: column
![Example of jumping from local minima.](Geron-mls2_0406.png)
:::
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.
:::

## Learning rates

::: columns
::: column

![The learning rate is too small](Geron-mls2_0404.png)
:::
::: column
![The learning rate is too large](Geron-mls2_0405.png)
:::
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 4-4 and 4-5.
:::

## Learning rates #2

![Changing the learning rates for a robot arm.](matt-henderson-learning-rates-animation.mov){width=60%}

::: {.content-visible unless-format="revealjs"}

> "a nice way to see how the learning rate affects Stochastic Gradient Descent.
> we can use SGD to control a robot arm - minimizing the distance to the target as a function of the angles θᵢ. Too low a learning rate gives slow inefficient learning, too high and we see instability"

:::

::: footer
Source: Matt Henderson (2021), [Twitter post](https://twitter.com/matthen2/status/1520427516997025792)
:::

## Learning rate schedule

![Learning curves for various learning rates η](Geron-mls2_1108.png)

In training the learning rate may be tweaked manually.

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 11-8.
:::

## We need non-zero derivatives {.smaller}

This is why can't use accuracy as the loss function for classification.

Also why we can have the _dead ReLU_ problem.

::: {.content-hidden unless-format="revealjs"}
{{< video https://www.youtube.com/embed/KpKog-L9veg width="100%" height="80%" >}}
:::
::: {.content-visible unless-format="revealjs"}
{{< video https://www.youtube.com/embed/KpKog-L9veg >}}
:::

{{< include _loss-and-derivatives.qmd >}}

## Glossary {.appendix data-visibility="uncounted"}

- batches, batch size
- gradient-based learning, hill-climbing
- stochastic (mini-batch) gradient descent