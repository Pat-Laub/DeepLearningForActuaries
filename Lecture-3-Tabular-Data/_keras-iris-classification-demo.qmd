# Classification {visibility="uncounted"}

```{python}
#| echo: false
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

pandas.options.display.max_rows = 4
```

## Iris dataset

```{python}
from sklearn.datasets import load_iris
iris = load_iris()
names = ["SepalLength", "SepalWidth", "PetalLength", "PetalWidth"]
features = pd.DataFrame(iris.data, columns = names)
features
```

## Target variable

::: columns
::: column

```{python}
iris.target_names
```

```{python}
iris.target[:8]
```

```{python}
target = iris.target
target = target.reshape(-1, 1)
target[:8]
```
:::
::: column
```{python}
classes, counts = np.unique(
        target,
        return_counts=True
)
print(classes)
print(counts)
```

```{python}
iris.target_names[
  target[[0, 30, 60]]
]
```
:::
:::

```{python}
#| echo: false
pandas.options.display.max_rows = 6
```

## Split the data into train and test {.smaller}

```{python}
X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=24)
X_train
```

```{python}
X_test.shape, y_test.shape
```

## A basic classifier network

![A basic network for classifying into three categories.](basic-classifier-network.png)

::: footer
Source: Marcus Lautier (2022).
:::

::: {.content-visible unless-format="revealjs"}
Since the task is a classification problem, we use `softmax` activation function. The softmax function takes in the input and returns a probability vector, which tells us about the probability of a data point belonging to a certain class.
:::
## Create a classifier model

```{python}
NUM_FEATURES = len(features.columns)
NUM_CATS = len(np.unique(target))

print("Number of features:", NUM_FEATURES)
print("Number of categories:", NUM_CATS)
```

Make a function to return a Keras model:
```{python}
def build_model(seed=42):
    random.seed(seed)
    return Sequential([
        Dense(30, activation="relu"),
        Dense(NUM_CATS, activation="softmax")
    ])
```

## Fit the model

```{python}
model = build_model()
model.compile("adam", "SparseCategoricalCrossentropy")

model.fit(X_train, y_train, epochs=5, verbose=2);
```
::: {.content-visible unless-format="revealjs"}
Since the problem at hand is a classification problem, we define the optimizer and loss function accordingly. Optimizer is `adam` and the loss function is `SparseCategoricalCrossentropy`. If the response variable represents the category directly using an integer (i.e. if the response variable is not one-hot encoded), we must use `SparseCategoricalCrossentropy`. If the response variable (y label) is already one-hot encoded we can use `CategoricalCrossentropy`. 
:::

## Track accuracy as the model trains

```{python}
model = build_model()
model.compile("adam", "SparseCategoricalCrossentropy", \
        metrics=["accuracy"])
model.fit(X_train, y_train, epochs=5, verbose=2);
```
::: {.content-visible unless-format="revealjs"}
We can also specify which loss metric to monitor in assessing the performance during the training. The metric that is usually used in classification tasks is `accuracy`, which tracks the fraction of all predcitions which identified the class accurately. The metrics are not used for optimizing. They are only used to keep track of how well the model is performing during the optimization.  By setting `verbose=2` , we are not princting the output, and we can see how the loss is reducing and accuracy is improving.
:::

## Run a long fit
::: {.content-visible unless-format="revealjs"}
Run the model training for 500 epochs.
:::
```{python}

model = build_model()
model.compile("adam", "SparseCategoricalCrossentropy", \
        metrics=["accuracy"])
%time hist = model.fit(X_train, y_train, epochs=500, \
        validation_split=0.25, verbose=False)
```

Evaluation now returns both _loss_ and _accuracy_.
```{python}
model.evaluate(X_test, y_test, verbose=False)
```

## Add early stopping

```{python}
model = build_model()                                                   #<1>
model.compile("adam", "SparseCategoricalCrossentropy", \                
        metrics=["accuracy"])                                           #<2>

es = EarlyStopping(restore_best_weights=True, patience=50,              #<3>
        monitor="val_accuracy")                                         
%time hist_es = model.fit(X_train, y_train, epochs=500, \               
        validation_split=0.25, callbacks=[es], verbose=False);          #<4>

print(f"Stopped after {len(hist_es.history['loss'])} epochs.")
```
::: {.content-visible unless-format="revealjs"}
1. Defines a new model with the same architecture as `model_build` which is already constructed
2. Compiles the model with optimizer, loss function and metric
3. Defines the early stopping object as usual, with one slight change. The code is specified to activate the early stopping by monitoring the validation accuracy (`val_accuracy`), not the loss. 
4. Fits the model
:::

Evaluation on test set:
```{python}
model.evaluate(X_test, y_test, verbose=False)
```

## Fitting metrics

::: columns
::: column
```{python}
#| echo: false
matplotlib.pyplot.rcParams["figure.figsize"] = (2.5, 2.95)
plt.subplot(2, 1, 1)
plt.plot(hist.history["loss"])
plt.plot(hist.history["val_loss"])
plt.title("Loss")
plt.legend(["Training", "Validation"]);

plt.subplot(2, 1, 2)
plt.plot(hist_es.history["loss"])
plt.plot(hist_es.history["val_loss"])
plt.xlabel("Epoch");
```
:::
::: column
```{python}
#| echo: false
matplotlib.pyplot.rcParams["figure.figsize"] = (2.5, 3.25) 
plt.subplot(2, 1, 1)
plt.plot(hist.history["accuracy"])
plt.plot(hist.history["val_accuracy"])
plt.title("Accuracy")

plt.subplot(2, 1, 2)
plt.plot(hist_es.history["accuracy"])
plt.plot(hist_es.history["val_accuracy"])
plt.xlabel("Epoch");
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

::: {.content-visible unless-format="revealjs"}
Left hand side plots show how loss behaved without and with early stopping. Right hand side plots show how accuracy performed without and with early stopping.
:::
## What is the softmax activation?

It creates a "probability" vector: $\text{Softmax}(\boldsymbol{x}) = \frac{\mathrm{e}^x_i}{\sum_j \mathrm{e}^x_j} \,.$

In NumPy:
```{python}
out = np.array([5, -1, 6])
(np.exp(out) / np.exp(out).sum()).round(3)
```

In TensorFlow:
```{python}
out = tf.constant([[5.0, -1.0, 6.0]])
tf.keras.activations.softmax(out).numpy().round(3)
```

## Prediction using classifiers

```{python}
y_test[:4]
```
::: {.content-visible unless-format="revealjs"}
Response variable _y_ is an array with numeric integers, where each integer represents the class the data belongs to. However, the `model.predict()` function returns an array with probabilities not an array with integers. The array displays the probabilities of belonging to each category.
:::
```{python}
y_pred = model.predict(X_test.head(4), verbose=0)
y_pred
```
::: {.content-visible unless-format="revealjs"}
Using `np.argmax()` which returns index of the maximum value in an array, we can obtain the predicted class.
:::

```{python}
# Add 'keepdims=True' to get a column vector.
np.argmax(y_pred, axis=1)
```

```{python}
iris.target_names[np.argmax(y_pred, axis=1)]
```

## Cross-entropy loss: ELI5

::: columns
::: column

{{< video https://www.youtube.com/embed/6ArSys5qHAU width="560" height="315" >}}

:::
::: column

{{< video https://www.youtube.com/embed/xBEh66V9gZo width="560" height="315" >}}

:::
:::

## Why use cross-entropy loss?

```{python}
p = np.linspace(0, 1, 100)
plt.plot(p, (1-p)**2)
plt.plot(p, -np.log(p))
plt.legend(["MSE", "Cross-entropy"]);
```

::: {.content-visible unless-format="revealjs"}
The above plot shows how MSE and cross-entrphy penalize wrong predictions. The x-axis inidicates the severity of misclassification. Suppose the neural network predicted that there is near-zero probability of an observation being in class "1" when the actual class is "1". This represents a strong misclassification. The above graph shows how MSE does not impose heavy penalities for the misclassifications near zero. It displays a linear increment across the severity of misclassification. On the other hand, cross-entrophy penalizes bad predcitions strongly. Also, the misclassification penalty grows exponentially. This makes cross entrophy more suitable.
:::

## One-hot encoding {data-visibility="uncounted"}

```{python}
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse_output=False)

y_train_oh = enc.fit_transform(y_train)
y_test_oh = enc.transform(y_test)
```

::: columns
::: column
```{python}
y_train[:5]
```
:::
::: column
```{python}
y_train_oh[:5]
```
:::
:::

## Classifier given one-hot outputs

Create the model (_new loss function_):
```{python}
#| code-line-numbers: "|3"
model = build_model()
model.compile("adam", "CategoricalCrossentropy", \
    metrics=["accuracy"])
```

Fit the model (_new target variables_):
```{python}
model.fit(X_train, y_train_oh, epochs=100, verbose=False);
```

Evaluate the model (_new target variables_):
```{python}
model.evaluate(X_test, y_test_oh, verbose=False)
```
