---
title: Artificial Intelligence
---

```{python}
#| echo: false
#| warning: false
# TODO: Update following section
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    plt.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    plt.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
plt.rcParams["figure.dpi"] = 350
plt.rcParams["savefig.bbox"] = "tight"
plt.rcParams["font.family"] = "serif"

plt.rcParams["axes.spines.right"] = False
plt.rcParams["axes.spines.top"] = False


def square_fig():
    return plt.figure(figsize=(2, 2), dpi=350).gca()


from IPython.display import HTML
```

# Artificial Intelligence {data-visibility="uncounted"}

## Different goals of AI

Artificial intelligence describes an agent which is capable of:

----------------     -----------------------
Thinking humanly     Thinking rationally
Acting humanly       Acting rationally 
----------------     -----------------------

AI eventually become dominated by one approach, called _machine learning_, which itself is now dominated by _deep learning_ (neural networks).

There are AI algorithms for simple tasks that don't use machine learning though.

<!-- ## Expert systems -->

## Shakey the Robot (~1966 -- 1972)

::: {.columns}
::: {.column width="30%"}
![Shakey the Robot](SRI_Shakey_with_callouts.jpg)

:::
::: {.column width="70%"}

<br>

{{< video https://www.youtube.com/embed/7bsEN8mwUB8 width="560" height="315" >}}

:::
:::

::: footer
Source: Wikipedia page for [the Shakey Project](https://en.wikipedia.org/wiki/File:SRI_Shakey_with_callouts.jpg)
:::

## Route-finding I {.smaller}


::: columns
::: {.column width="60%"}
<!-- ![Dijkstra's algorithm (1959).^[Source: Wikipedia page for [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm).]](Dijkstras_progress_animation.gif) -->

> At its core, a pathfinding method searches a graph by starting at one vertex and exploring adjacent nodes until the destination node is reached, generally with the intent of finding the cheapest route. Although graph searching methods such as a breadth-first search would find a route if given enough time, other methods, which "explore" the graph, would tend to reach the destination sooner. An analogy would be a person walking across a room; rather than examining every possible route in advance, the person would generally walk in the direction of the destination and only deviate from the path to avoid an obstruction, and make deviations as minor as possible. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Pathfinding))

:::
::: {.column width="40%"}

<br>

![A* algorithm (1968).](Astar_progress_animation.gif)
:::
:::

::: footer
Source: Wikipedia page for [the A* search algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm).
:::

## Route-finding II

![[Tunes of the Kingdom: Evolving Physics and Sounds for ‘The Legend of Zelda: Tears of the Kingdom’](https://youtu.be/N-dPDsLTrTE?si=cwnV19wG1tS43e2b&t=3137 ), GDC 2024](zelda-slide.png)


## Evaluating a chess game I

Who's winning this game?

::: columns
::: {.column width="50%"}

::: figure
```{python}
#| echo: false
import chess
board = chess.Board("2r3k1/p3bp1p/2Bp1np1/4p3/1r6/B1R5/P1PP1P1P/R5K1 b - - 0 1")
board
```
:::

:::
::: {.column width="50%"}
```{python}
#| echo: false
PIECES = ["P", "N", "B", "R", "Q", "K"]
STANDARD_PIECE_VALUES = {"P": 1, "N": 3, "B": 3, "R": 5, "Q": 9, "K": 0}


def count_piece(board, piece, white=True):
    count = 0
    for square in chess.SQUARES:
        p = board.piece_at(square)
        if p and p.symbol() == piece:
            count += 1
    return count


table = "<table><tbody>"

total = 0
vals = []

for piece in PIECES:
    table += "<tr>"

    svg = chess.svg.piece(chess.Piece.from_symbol(piece), size="50px")
    numPiece = count_piece(board, piece)
    valPiece = STANDARD_PIECE_VALUES[piece]
    total += numPiece * valPiece
    if numPiece * valPiece > 0:
        vals.append(str(numPiece * valPiece))
    table += f'<td style="text-align: center">{svg}</td>'
    table += f'<td style="text-align: center">{numPiece} &times {valPiece} = {numPiece * valPiece}</td>'
    table += "</tr>"

table += f'<td style="text-align: center">White</td>'
# table += f'<td style="text-align: center">{" + ".join(vals)} = {total}</td>'
table += f'<td style="text-align: center">{total}</td>'
table += "</tr>"

table += "</tbody></table>"

HTML(table)
```
:::
:::

##  Evaluating a chess game II

Just add up the pieces for each player.

::: columns
::: {.column width="50%"}

::: figure
```{python}
#| echo: false
board
```
:::

:::
::: {.column width="50%"}
```{python}
#| echo: false
def count_piece(board, piece, white=True):
    count = 0
    for square in chess.SQUARES:
        p = board.piece_at(square)
        if p and p.symbol() == piece:
            count += 1
    return count


table = "<table><tbody>"

total = 0
vals = []

for piece in PIECES:
    table += "<tr>"

    piece = piece.lower()
    svg = chess.svg.piece(chess.Piece.from_symbol(piece), size="50px")
    numPiece = count_piece(board, piece)
    valPiece = STANDARD_PIECE_VALUES[piece.upper()]
    total += numPiece * valPiece
    if numPiece * valPiece > 0:
        vals.append(str(numPiece * valPiece))
    table += f'<td style="text-align: center">{svg}</td>'
    table += f'<td style="text-align: center">{numPiece} &times {valPiece} = {numPiece * valPiece}</td>'
    table += "</tr>"

table += f'<td style="text-align: center">Black</td>'
# table += f'<td style="text-align: center">{" + ".join(vals)} = {total}</td>'
table += f'<td style="text-align: center">{total}</td>'
table += "</tr>"

table += "</tbody></table>"

HTML(table)
```

:::
:::

::: fragment
Overall: 21 &minus; 22 = &minus;1.
:::

## The minimax algorithm

::: columns
::: column
![The minimax algorithm for chess.](chess_minimax.png)
:::
::: column
![Pseudocode for the minimax algorithm.](sebastian-lague-minimax-pseudocode.png)
:::
:::

::: footer
Source: codeRtime, [Programming a simple minimax chess engine in R](https://www.codertime.org/minimax-chess-engine-programming-r/), and Sebastian Lague (2018), [Algorithms Explained – minimax and alpha-beta pruning](https://youtu.be/l-hh51ncgDI).
:::

## Chess

Deep Blue (1997)

::: columns
::: {.column width="68%"}
![Gary Kasparov playing Deep Blue.](deep-blue.jpeg)
:::
::: {.column width="28%"}
![Cartoon of the match.](deep-blue-cartoon.l2005-6.telegraph1131746-matt-pritchett.jpeg)
:::
:::

::: footer
Sources: Mark Robert Anderson (2017), [Twenty years on from Deep Blue vs Kasparov](https://theconversation.com/twenty-years-on-from-deep-blue-vs-kasparov-how-a-chess-match-started-the-big-data-revolution-76882), The Conversation article, and [Computer History Museum](https://www.computerhistory.org/chess/stl-431e1a079ea63/).
:::


## Machine Learning

Tried *making a computer smart*, too hard!

Make a computer that can **learn** to be smart.

![The Venn diagram of Artificial Intelligence, Machine Learning, and Deep Learning.](AI-vs-ML-vs-Deep-Learning.png)

::: footer
Source: Edureka (2020), [AI Vs Machine Learning Vs Deep Learning Edureka](https://www.edureka.co/blog/ai-vs-machine-learning-vs-deep-learning).
:::

## Definition

::: columns 
::: {.column width="55%"}

> "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed"
Arthur Samuel (1959)

:::
::: {.column width="45%"}
![](xkcd-machine_learning_2x.png)
:::
:::

::: footer
Source: Randall Munroe (2017), [xkcd #1838: Machine Learning](https://xkcd.com/1838/).
:::

<!--
## Traditional AI versus ML

<br>

::: columns
::: column
![The traditional approach.](Geron-mls2_0101.png)
:::
::: column
![The machine learning approach.](Geron-mls2_0102.png)
:::
:::

:::footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 1-1 and 1-2.
:::

## Benefits of ML

<br>

::: columns
::: {.column .aligned-column}
![Machine learning can automatically adapt to change.](Geron-mls2_0103.png)
:::
::: {.column .aligned-column}
![Machine learning can help humans to learn.](Geron-mls2_0104.png)
::::
:::

:::footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 1-3 and 1-4.
:::
-->

# Deep Learning Successes (Images) {data-visibility="uncounted"}

## Image Classification I

::: columns
::: {.column width="40%"}
What is this?
![](mystery1.jpg)
:::
::: {.column width="10%"}
:::
::: column
Options:

```{=html}
<ol type="1">
<li><span title="1.10%">punching bag</span></li>
<li><span title="11.87%">goblet</span></li>
<li><span title="0.71%">red wine</span></li>
<li><span title="1.21%">hourglass</span></li>
<li><span title="31.25%">balloon</span></li>
</ol>
```

::: {.callout-note}
Hover over the options to see AI's prediction (i.e. the probability of the photo being in that category).
:::

:::
:::

::: footer
Source: [Wikipedia](https://en.wikipedia.org/wiki/Balloon#/media/File:Congrats_bqt.jpg)
:::

## Image Classification II

::: columns
::: column
What is this?

![](mystery2.jpg)
:::
::: column
Options:

```{=html}
<ol type="1">
<li><span title="0.11%">sea urchin</span></li>
<li><span title="0.71%">porcupine</span></li>
<li><span title="41.32%">echidna</span></li>
<li><span title="0.24%">platypus</span></li>
<li><span title="0.11%">quill</span></li>
</ol>
```
:::
:::

::: footer
Source: [Wikipedia](https://en.wikipedia.org/wiki/Echidna#/media/File:Short-beaked_echidna_in_ANBG.jpg)
:::

## Image Classification III

::: columns
::: column
What is this?

![](mystery3.jpg)
:::
::: column
Options:

```{=html}
<ol type="1">
<li><span title="0.72%">dingo</span></li>
<li><span title="50.70%">malinois</span></li>
<li><span title="1.18%">German shepherd</span></li>
<li><span title="0.37%">muzzle</span></li>
<li><span title="2.24%">kelpie</span></li>
</ol>
```
:::
:::

::: footer
Source: [Wikipedia](https://en.wikipedia.org/wiki/File:Malinois_Shepherd3.JPG)
:::

## ImageNet Challenge

[ImageNet](https://www.image-net.org/index.php) and the _ImageNet Large Scale Visual Recognition Challenge (ILSVRC)_; originally [1,000 synsets](https://image-net.org/challenges/LSVRC/2014/browse-synsets).

![AlexNet --- a neural network developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton --- won the ILSVRC 2012 challenge convincingly.](AlexNet-Results.png)

::: footer
Source: James Briggs & Laura Carnevali, [_AlexNet and ImageNet: The Birth of Deep Learning_](https://www.pinecone.io/learn/series/image-search/imagenet/), Embedding Methods for Image Search, Pinecone Blog
:::

## How were the images labelled? {.smaller}

::: columns
::: column
![The original 'mechanical turk' (1770)](mechanical-turk.jpeg)
:::
::: column
> "Two years later, the first version of ImageNet was released with 12 million images structured and labeled in line with the WordNet ontology. If one person had annotated one image/minute and did nothing else in those two years (including sleeping or eating), it would have taken 22 years and 10 months.
>
> To do this in under two years, Li turned to Amazon Mechanical Turk, a crowdsourcing platform where anyone can hire people from around the globe to perform tasks cost-effectively."
:::
:::

::: footer
Sources: Editors of Encyclopaedia Britannica, [The Mechanical Turk: AI Marvel or Parlor Trick?](https://www.britannica.com/story/the-mechanical-turk-ai-marvel-or-parlor-trick), and <br>
James Briggs & Laura Carnevali, [_AlexNet and ImageNet: The Birth of Deep Learning_](https://www.pinecone.io/learn/series/image-search/imagenet/), Embedding Methods for Image Search, Pinecone Blog
:::

## Needed a graphics card

::: columns
::: column
A graphics processing unit (GPU)

![My deep learning PC](deep-learning-pc.jpg)

:::
::: column

> "**4.2. Training on multiple GPUs**
A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs."

:::
:::

::: footer
Source: Krizhevsky, Sutskever and Hinton (2017), [_ImageNet Classification with Deep Convolutional Neural Networks_](https://dl.acm.org/doi/pdf/10.1145/3065386), Communications of the ACM 
:::

## Lee Sedol plays AlphaGo (2016)

Deep Blue was a win for AI, AlphaGo a win for ML.

![Lee Sedol playing AlphaGo AI](New-Yorker-House-Alpha-Go-2.jpeg) 

I highly recommend [this documentary about the event](https://youtu.be/WXuK6gekU1Y).

::: footer
Source: Patrick House (2016), [AlphaGo, Lee Sedol, and the Reassuring Future of Humans and Machines](https://www.newyorker.com/tech/annals-of-technology/alphago-lee-sedol-and-the-reassuring-future-of-humans-and-machines), New Yorker article.
:::

## Generative Adversarial Networks (2014)

[https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/)

::: columns
::: column
![A GAN-generated face](thispersondoesnotexist1.jpg)
:::
::: column
![A GAN-generated face](thispersondoesnotexist2.jpg)
:::
:::

## Diffusion models

::: columns
::: column
![Painting of avocado skating while wearing a hoodie](DALL·E 2022-08-03 12.56.46 - painting of avocado skating while wearing a hoodie.png)
:::
::: column
![A surrealist painting of an alpaca studying for an exam](DALL·E 2022-08-03 13.02.20 - a surrealist painting of an alpaca studying for an exam.png)
:::
:::

::: footer
Source: Dall-E 2 images, prompts by ACTL3143 students in 2022.
:::

## Dall-E 2 (2022) vs Dall-E 3 (2023) {.smaller}

Same prompt: "A beautiful calm photorealistic view of an waterside metropolis that has been neglected for hundreds of years and is overgrown with nature"

::: columns
::: column
![Dall-E 2](DALL·E 2023-10-15 20.25.57 - A beautiful calm photorealistic view of an waterside metropolis that has been neglected for hundreds of years and is overgrown with nature.png)
:::
::: column
![Dall-E 3](DALL·E 2023-10-15 20.25.11 - Photo of a once-majestic metropolis by the water, now abandoned for centuries. The city's skyscrapers and buildings are cloaked in thick green vines a.png)
:::
:::

::: footer
Dall-E 3 rewrites it as: "Photo of a once-majestic metropolis by the water, now abandoned for centuries. The city's skyscrapers and buildings are cloaked in thick green vines..."
:::

# Deep Learning Successes (Text) {data-visibility="uncounted"}

## GPT

::: columns
::: {.column width="50%"}
![AI predictions in the classification demo were from GPT code.](ChatGPT-4o-example.png)
:::
::: {.column width="50%"}
**Homework** Get ChatGPT to:

- generate images
- translate code
- explain code
- run code
- analyse a dataset
- critique code
- critique writing
- voice chat with you

Compare to Copilot.

:::
:::

::: footer
Source: [ChatGPT conversation](https://chatgpt.com/share/e/d5ba6b79-45ec-444d-a748-437834dae6e2).
:::

## Code generation (GitHub Copilot)

{{< video assignmentexplain-Smallest.mp4 >}}

::: footer
Source: [GitHub Blog](https://github.blog/2022-09-08-github-copilot-now-available-for-teachers/)
:::

## Students get Copilot for free {.smaller}

::: columns
::: column
![Use a free trial then sign up for free education account](copilot__1_.png)
:::
::: column
A student post from last year:

> I strongly recommend taking a photo holding up your Academic Statement to your phone's front facing camera when getting verified for the student account on GitHub. No other method of taking/uploading photo proofs worked for me. Furthermore, I had to make sure the name on the statement matched my profile exactly and also had to put in a bio. 
>
> Good luck with this potentially annoying process!
:::
:::

**Homework** It's a slow process, so get this going early.

::: footer
Source: [GitHub Education for Students](https://github.com/edu/students)
:::

## Programmers are increasingly using AI

![Question: What is your experience with the following AI tools?](jetbrains-2023-dev-survey-ai-tools.png)

::: footer
Source: JetBrains, [The State of Developer Ecosystem 2023](https://www.jetbrains.com/lp/devecosystem-2023/ai/#ai_tools_experience).
:::

# Classifying Machine Learning Tasks {data-visibility="uncounted"}

## A taxonomy of problems

::: columns
::: {.column width="60%"}
![Machine learning categories in ACTL3142.](kaggle-types-of-ml-problems.jpg)
:::
::: {.column width="40%"}
New ones:

- Reinforcement learning
- Semi-supervised learning
- Active learning

:::
:::

::: footer
Source: Kaggle, [Getting Started](https://www.kaggle.com/getting-started/169622).
:::

## Supervised learning

The main focus of this course.

### Regression

- Given policy $\hookrightarrow$ predict the rate of claims.
- Given policy $\hookrightarrow$ predict claim severity.
- Given a reserving triangle $\hookrightarrow$ predict future claims.

### Classification

- Given a claim $\hookrightarrow$ classify as fraudulent or not.
- Given a customer $\hookrightarrow$ predict customer retention patterns.

## Supervised learning: mathematically

![A recipe for supervised learning.](recipe-for-supervised-ml.png)

::: footer
Source: Matthew Gormley (2021), [Introduction to Machine Learning Lecture Slides](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf), Slide 67.
:::

## Self-supervised learning

Data which 'labels itself'. Example: language model.

<center>
![](Chaudhary-nlp-ssl-causal-language-modeling-steps.png)
</center>

!['Autoregressive' (e.g. GPT) versus 'masked' model (e.g. BERT).](Chaudhary-nlp-ssl-masked-lm.png)


::: footer
Source: Amit Chaudhary (2020), [Self Supervised Representation Learning in NLP](https://amitness.com/2020/05/self-supervised-learning-nlp/).
:::

## Example: image inpainting

::: columns
::: {.column width="33%"}
![Original image](inpainting-input.jpg)
:::
::: {.column width="33%"}
![Randomly remove a part](inpainting-missing.jpg)
:::
::: {.column width="33%"}
![Try to fill it in from context](inpainting-generative-fill.jpg)
:::
:::

Other examples: image super-resolution, denoising images.

::: footer
See Liu et al. (2018), [Image Inpainting for Irregular Holes using Partial Convolutions](https://arxiv.org/pdf/1804.07723.pdf).
:::


## Example: Deoldify images #1

<!-- Could show Hindenburg Disaster in colour video -->

![A deoldified version of the famous "Migrant Mother" photograph.](deoldify-migrant-mother.jpeg)

:::footer
Source: [Deoldify package](https://github.com/jantic/DeOldify).
:::

## Example: Deoldify images #2

![A deoldified Golden Gate Bridge under construction.](deoldify-golden-gate-bridge.jpeg)

:::footer
Source: [Deoldify package](https://github.com/jantic/DeOldify).
:::

# Neural Networks {data-visibility="uncounted"}

## How do real neurons work?

::: {.content-hidden unless-format="revealjs"}
{{< video https://www.youtube.com/embed/6qS83wD29PY width="100%" height="80%" >}}
:::
::: {.content-visible unless-format="revealjs"}
{{< video https://www.youtube.com/embed/6qS83wD29PY >}}
:::

## A neuron 'firing'

::: {.content-visible unless-format="revealjs"}
Similar to a biological neuron, an artificial neuron 'fires' when the combined input information exceeds a certain threshold. This activation can be seen as a step function. The difference is that the artificial neuron uses mathematical rules (e.g. weighted sum) to 'fire' whereas 'firing' in the biological neurons is far more complex and dynamic.
:::

```{python}
#| echo: false
import numpy as np

x = [-5, -1e-10, 0, 5]
y = [x_i >= 0 for x_i in x]

plt.plot(x, y)

# Annotate the top-left corner of the plot with the name
# of the activation function
plt.annotate(
    "Step function",
    xy=(0.2, 1),
    xycoords="axes fraction",
    xytext=(-5, -5),
    textcoords="offset points",
    ha="left",
    va="top",
)

plt.xlabel("Input")
plt.ylabel("Output");
```

## An artificial neuron 

![A neuron in a neural network with a ReLU activation.](single-neuron.png)

::: {.content-visible unless-format="revealjs"}
The figure shows how we first compute the weighted sum of inputs, and then evaluate the summation using the step function. If the weighted sum is greater than the pre-set threshold, the neuron `fires'. 
:::

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron

::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 . 
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

Here, $x_1$, $x_2$, $x_3$ is just some fixed data.

:::
::: {.column width="45%"}
![A neuron in a neural network with a ReLU activation.](single-neuron.png)
:::
:::

The weights $w_1$, $w_2$, $w_3$ should be 'learned'.

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron with bias

::: {.content-visible unless-format="revealjs"} 
The bias is a constant term added to the product of inputs and weights. It helps in shifting the entire activation function to either the negative or positive side. This shifting can either accelerate or delay the activation. For example, if the bias is negative, it will shift the entire curve to the right, making the activation harder. This is similar to delaying the activation.
::: 


::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 + b .
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

The weights $w_1$, $w_2$, $w_3$ and bias $b$ should be 'learned'.
:::
::: {.column width="45%"}

::: {.panel-tabset}

### Bias = -4

```{python}
#| echo: false
def plot_relu_with_bias(bias):
    xs = np.linspace(-10, 10, 1_000)
    xs_with_bias = xs + bias
    ys = xs_with_bias * (xs_with_bias >= 0)

    square_fig().plot(xs, ys)
    plt.xlim([-5, 5])
    plt.ylim([-1, 7])
    plt.xlabel("Weighted Sum Input")
    plt.ylabel("Output")


plot_relu_with_bias(-4)
```

### 0

```{python}
#| echo: false
plot_relu_with_bias(0) 
```

### 4

```{python}
#| echo: false
plot_relu_with_bias(4) 
```

:::
:::
:::

## A basic neural network

![A basic fully-connected/dense network.](basic-neural-network.png)

::: {.content-visible unless-format="revealjs"}
This neural network consists of an input layer with 2 neurons ($x_1, x_2$), an output layer with 3 neurons, and 1 hidden layer with 4 neurons. Since every neuron is linked to every other neuron, this is called a fully connected neural network. Since we have 2 inputs and 1 bias in the input layer, each neuron in the hidden layer has 2+1=3 parameters to learn. Similarly, there are 4 neurons and 1 bias in the hidden layer. Hence, each neuron in the output layer has 4+1=5 parameters to learn.
:::
 
::: footer
Source: Marcus Lautier (2022).
:::

## Step-function activation

### Perceptrons

Brains and computers are binary, so make a perceptron with binary data.
Seemed reasonable, impossible to train.

### Modern neural network

Replace binary state with continuous state.
Still rather slow to train.

::: {.callout-note}
It's a neur**al** network made of neur**on**s, not a "neuron network".
:::

## Try different activation functions

```{python}
#| echo: false
import numpy as np


def plot_activation(x, y, ax, name, hideX=False):
    ax.plot(x, y)
    if y.min() < 0:
        ax.axhline(0, ls="--", c="black", lw=0.5)

    # Annotate the top-left corner of the subplot with the name
    # of the activation function
    ax.annotate(
        name,
        xy=(0.2, 1),
        xycoords="axes fraction",
        xytext=(-5, -5),
        textcoords="offset points",
        ha="left",
        va="top",
    )

    if hideX:
        ax.xaxis.set_visible(False)


x = np.linspace(-5, 5, 500)

fig, axs = plt.subplots(2, 2)
y = x
plot_activation(x, y, axs[0, 0], "Linear", hideX=True)
y = x > 0
plot_activation(x, y, axs[0, 1], "Step", hideX=True)
y = np.tanh(x)
plot_activation(x, y, axs[1, 0], "tanh")
y = x * (x > 0)
plot_activation(x, y, axs[1, 1], "ReLU")

axs[0, 0].set(ylabel="Output")
axs[1, 0].set(xlabel="Input", ylabel="Output")
axs[1, 1].set(xlabel="Input");
```

::: {.content-visible unless-format="revealjs"}
Activation functions are essential for a neural network design. They provide the mathematical rule for 'firing' the neuron. There are many activation functions, and the choice of the activation function depends on the problem we are trying to solve. Note: If we use the 'linear' activation function at every neuron, then the regression learning problem becomes a simple linear regression. But if we use 'ReLu', 'tanh', or any other non-linear function, then, we can introduce non-linearity into the model so that the model can learn complex non-linear patterns in the data. There are activation functions in both the hidden layers and the output layer. The activation function in the hidden layer controls how the neural network learns complex non-linear patterns in the training data. The choice of activation function in the output layer determines the type of predictions we get.  
:::

## Flexible

> One can show that an MLP is a **universal approximator**, meaning 
> it can model any suitably smooth function, given enough hidden units,
> to any desired level of accuracy (Hornik 1991). One can either make
> the model be "wide" or "deep"; the latter has some advantages...

::: footer
Source: Murphy (2012), Machine Learning: A Probabilistic Perspective, 1st Ed, p. 566.
:::

## Feature engineering {.smaller}

::: columns
::: {.column width="55%"}
![](feature-engineering.png)
:::
::: {.column width="45%"}
![](facial-recognition.jpeg)
![](modelling-ratio.png)
:::
:::
Doesn't mean deep learning is always the best option!

::: {.content-visible unless-format="revealjs"}
A major part of traditional machine learning (TML) involves conducting feature engineering to extract relevant features manually. In contrast, representational learning does not involve heavy manual feature engineering, rather, it learns relevant features automatically from data during the task. Therefore, the effort spent on feature engineering in representational learning is minimal compared to TML.
:::

::: footer
Sources: Marcus Lautier (2022) & Fenjiro (2019), [_Face Id: Deep Learning for Face Recognition_](https://medium.com/@fenjiro/face-id-deep-learning-for-face-recognition-324b50d916d1), Medium.
:::

## Quiz

In this ANN, how many of the following are there:

::: columns
::: {.column width="32%"}

- features,
- targets,
- weights,
- biases, and
- parameters?

What is the depth?

:::
::: {.column width="68%"}
![An artificial neural network.](neural-network-circles.png)
:::
:::

::: {.content-visible unless-format="revealjs"}
There are three inputs, hence, three features. There is one neuron in the output layer, hence, one target. There are $3 \times 4 + 4 \times 4 + 4\times 1 = 32$ arrows, hence, there are 32 weights in total. Since there is 1 bias for each neuron, there are 9 biases in total. The number of total parameters to learn equals to the sum of weights and biases, hence, there are $32+9=41$ parameters in total. 
:::

::: footer
Source: Dertat (2017), [_Applied Deep Learning - Part 1: Artificial Neural Networks_](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), Medium.
:::

## Package Versions {.appendix data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"))
```

## Glossary {.appendix data-visibility="uncounted"}

::: columns
::: column
- activations, activation function
- artificial neural network
- biases (in neurons)
- classification problem
- deep network, network depth
- dense or fully-connected layer
- feed-forward neural network
- labelled/unlabelled data
- machine learning
:::
::: column
- minimax algorithm
- neural network architecture
- perceptron
- ReLU
- representation learning
- sigmoid activation function
- targets
- weights (in a neuron)
:::
:::
