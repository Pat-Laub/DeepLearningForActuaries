---
title: Classification
---

```{python}
#| echo: false
#| warning: false
import os

# os.environ["KERAS_BACKEND"] = "torch"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import torch

torch.set_num_threads(1)

import matplotlib

# TODO: Update following section
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    plt.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    plt.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
plt.rcParams["figure.dpi"] = 350
plt.rcParams["savefig.bbox"] = "tight"
plt.rcParams["font.family"] = "serif"

plt.rcParams["axes.spines.right"] = False
plt.rcParams["axes.spines.top"] = False


def squareFig():
    return plt.figure(figsize=(2, 2), dpi=350).gca()


def add_diagonal_line():
    xl = plt.xlim()
    yl = plt.ylim()
    shortestSide = min(xl[1], yl[1])
    plt.plot([0, shortestSide], [0, shortestSide], color="black", linestyle="--")


import pandas as pandas

pandas.options.display.max_rows = 6

import random

random.seed(1234)

import keras

keras.utils.set_random_seed(1)
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from keras.models import Sequential
from keras.layers import Dense, Input
from keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn import set_config

set_config(transform_output="pandas")
```

:::

# TLDR 

## Classification models in Keras

::: {.content-visible unless-format="revealjs"}
If the target is categorical variable with only two options, this is a binary classification problem.
The neural network's output layer should have **one neuron** with a **sigmoid activation** function.
The loss function should be **binary cross-entropy**. In Keras, this is called `loss="binary_crossentropy"`.

If the target has more than two options, this is a multi-class classification problem.
The neural network's output layer should have **as many neurons as there are classes** with a **softmax activation** function.
The loss function should be **categorical cross-entropy**. In Keras, this is done with `loss="sparse_categorical_crossentropy"`.
:::

If the number of classes is $c$, then:

| Target | Output Layer | Loss Function |
|--------|--------------|---------------|
| Binary <br> $(c=2)$ | 1 neuron with `sigmoid` activation | Binary Cross-Entropy |
| Multi-class <br> $(c > 2)$ | $c$ neurons with `softmax` activation | Categorical Cross-Entropy |


## Optionally output logits

::: {.content-visible unless-format="revealjs"}
If you find that the training is unstable, you can try to use a **linear activation** in the final layer and the have the loss functions implement the activation function.
:::

If the number of classes is $c$, then:

| Target | Output Layer | Loss Function |
|--------|--------------|---------------|
| Binary <br> $(c=2)$ | 1 neuron with `linear` activation | Binary Cross-Entropy (`from_logits=True`) |
| Multi-class <br> $(c > 2)$ | $c$ neurons with `linear` activation | Categorical Cross-Entropy (`from_logits=True`) |

## Code examples {.smaller}

::: columns
::: column
**Binary**

```python
model = Sequential([
  # Skipping the earlier layers
  Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy")
```
:::
::: column
**Multi-class**

```python
model = Sequential([
  # Skipping the earlier layers
  Dense(n_classes, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy")
```
:::
:::

::: columns
::: column
**Binary (logits)**

```python
from keras.losses import BinaryCrossentropy
model = Sequential([
  # Skipping the earlier layers
  Dense(1, activation="linear")
])
loss = BinaryCrossentropy(from_logits=True)
model.compile(loss=loss)
```
:::
::: column
**Multi-class (logits)**

```python
from keras.losses import SparseCategoricalCrossentropy

model = Sequential([
  # Skipping the earlier layers
  Dense(n_classes, activation="linear")
])
loss = SparseCategoricalCrossentropy(from_logits=True)
model.compile(loss=loss)
```
:::
:::


{{< include _keras-iris-classification-demo.qmd >}}

{{< include _stroke-prediction.qmd >}}

## Package Versions {.appendix data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"))
```

## Glossary {.appendix data-visibility="uncounted"}

- accuracy
- classification problem
- confusion matrix
- cross-entropy loss
- metrics
- sigmoid activation function
- sofmax activation