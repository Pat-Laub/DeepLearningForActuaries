---
title: "Artificial Intelligence and Deep Learning Models for Actuarial Applications"
subtitle: "Lecture slides from UNSW's ACTL3143 & ACTL5111 courses"
author: Dr Patrick Laub
bibliography: library.bib
csl: apa.csl
format:
  html: default
---

# Overview

These are the lecture slides from my recent "Artificial Intelligence and Deep Learning Models for Actuarial Applications" courses (coded ACTL3143 & ACTL5111) at UNSW.
They can be used to see what topics I covered in these courses.
The slides are not intended to be used to learn deep learning from scratch.
For that, you need to attend the lectures & complete the assessment.

# Lecture Materials

- [Course Overview](/Artificial-Intelligence/course-overview.html) ([slides](/Artificial-Intelligence/course-overview.pdf))
- [Artificial Intelligence](/Artificial-Intelligence/artificial-intelligence.html) ([slides](/Artificial-Intelligence/artificial-intelligence.pdf))
- [Python](/Artificial-Intelligence/python.html) ([slides](/Artificial-Intelligence/python.pdf))
- [Deep Learning with Keras](/Tabular-Data/deep-learning-keras.html) ([slides](/Tabular-Data/deep-learning-keras.pdf))
- [Categorical Variables](/Tabular-Data/categorical-variables.html) ([slides](/Tabular-Data/categorical-variables.pdf))
- [Classification](/Tabular-Data/classification.html) ([slides](/Tabular-Data/classification.pdf))
- [Project](/Tabular-Data/project.html) ([slides](/Tabular-Data/project.pdf))
- [Computer Vision](/Computer-Vision/computer-vision.html) ([slides](/Computer-Vision/computer-vision.pdf))
- [Natural Language Processing](/Natural-Language-Processing/natural-language-processing.html) ([slides](/Natural-Language-Processing/natural-language-processing.pdf))
- [Time Series & Recurrent Neural Networks](/Time-Series-And-Recurrent-Neural-Networks/time-series-and-rnns.html) ([slides](/Time-Series-And-Recurrent-Neural-Networks/time-series-and-rnns.pdf))
- [Entity Embedding](/Advanced-Tabular-Data/entity-embedding.html) ([slides](/Advanced-Tabular-Data/entity-embedding.pdf))
- [Optimisation](/Advanced-Tabular-Data/optimisation.html) ([slides](/Advanced-Tabular-Data/optimisation.pdf))
- [Distributional Regression](/Distributional-Regression/distributional-regression.html) ([slides](/Distributional-Regression/distributional-regression.pdf))
- [Interpretability](/Advanced-Topics/interpretability.html) ([slides](/Advanced-Topics/interpretability.pdf))
- [Generative Networks](/Generative-Networks/generative-networks.html) ([slides](/Generative-Networks/generative-networks.pdf))
- [Generative Adversarial Networks](/Generative-Networks/gans.html) ([slides](/Generative-Networks/gans.pdf))

# Readings

The readings from the book will come mainly from @geron2022hands, which is available through the UNSW Library's access to O'Reilly Media texts.
I'll give references to the 3rd edition, but if you get your hands on a copy of the 2nd edition then that is also fine.
Some readings will be from @james2021statistical (or equivalently the the Python version @james2023statistical) which is [available online](https://www.statlearning.com); you'll need the 2nd edition for this (the deep learning chapter is not in the 1st edition).
Note, if I say "read from A up to B", that means to read A but stop at B (without reading it).

| Week | Readings |
|:-:|--------------|
| 1 | @geron2022hands: Chapter 1, Chapter 2 (up to "Handling Text and Categorical Attributes"), @james2021statistical: Sections 10.1 & 10.2
| 2 | @geron2022hands: Chapter 2 (up to "Launch, Monitor, and Maintain Your System"), Chapter 3 (up to "Multilabel Classification"), Chapter 10 ("Implementing MLPs with Keras" up to "Building Complex Models Using the Functional API")
| 3 | @james2021statistical: Section 10.3, @geron2022hands: Chapter 14 (just skim through the specific historical architectures, like InceptionNet etc.) |
| 4 | @james2021statistical: Section 10.4, @vajjala2020practical: Chapters 1 and 2 (up to "Modeling") |
| 5 | @james2021statistical: Section 10.5, @geron2022hands: Chapter 15, @hyndman2018forecasting: Section 5.1-5.3 and 5.8 |
| 7 | @geron2022hands: Chapter 11, Chapter 13 "Encoding Categorical Features Using Embeddings" |
| 8 | [@charpentier2024insurance](https://link.springer.com/book/10.1007/978-3-031-49783-4), [@molnar2020interpretable](https://christophm.github.io/interpretable-ml-book/), [@barocas-hardt-narayanan](https://fairmlbook.org), @o2017weapons. |
| 9 | @chollet2021deep: Chapter 12 |
| 10 | - |

: {tbl-colwidths="[6,94]"}

The following readings are for those who are taking ACTL3142/ACTL5110 at the same time as ACTL3143/ACTL5111 (or who just need to brush up on that course a little):

| Week | Readings (ACTL3142 Revision) |
|:--:|--------------|
| 1 | @james2021statistical: Chapter 2, Sections 3.1, 3.2, and 5.1.1 |
| 2 | @james2021statistical: Section 3.3.1, 4.1, 4.2, 4.3 |

: {tbl-colwidths="[6,94]"}

Other useful resources include the Actuaries Institute's [Actuariesâ€™ Analytical Cookbook](https://actuariesinstitute.github.io/cookbook) and the Swiss Association of Actuaries' [Actuarial Data Science Tutorials](https://www.actuarialdatascience.org/ADS-Tutorials/).

<!-- 
# List of Topics Covered

## [Lecture 1: Python](/Artificial-Intelligence/python.qmd)

::: columns
::: column

- default arguments
- dictionaries
- f-strings
- function definitions
- Google Colaboratory
- `help`
- list
:::
::: column
- `pip install ...`
- `range`
- slicing
- tuple
- `type`
- whitespace indentation
- zero-indexing
:::
:::

## [Lecture 2: Deep Learning](/Tabular-Data/deep-learning-keras.qmd)

::: columns
::: column
- activations, activation function
- artificial neural network
- biases (in neurons)
- callbacks
- cost/loss function
- deep/shallow network, network depth
- dense or fully-connected layer
- early stopping
- epoch
- feed-forward neural network
:::
::: column
- Keras, Tensorflow, PyTorch
- matplotlib, seaborn
- neural network architecture
- overfitting
- perceptron
- ReLU activation
- representation learning
- training/validation/test split
- universal approximation theorem
- weights (in a neuron)
:::
:::

## Tutorial 2: Forward Pass

::: columns
::: column
- batches, batch size
- forward pass of network
- gradient-based learning
:::
::: column
- learning rate
- stochastic (mini-batch) gradient descent
:::
:::

## [Lecture 3: Tabular Data](/Tabular-Data/categorical-variables.qmd)

::: columns
:::: column
**Categorical Variables**

- entity embeddings
- Input layer
- Keras functional API
- nominal variables
- ordinal variables
- Reshape layer
- skip connection
- wide & deep network
::::
:::: column
**Classification**

- accuracy
- confusion matrix
- cross-entropy loss
- metrics
- sigmoid activation
- sofmax activation
::::
:::

## [Lecture 4: Computer Vision](/Computer-Vision/computer-vision.qmd)

::: columns
::: column
- AlexNet, GoogLeNet, Inception
- channels
- computer vision
- convolutional layer & CNN
- error analysis
- fine-tuning
- filter/kernel
:::
::: column
- flatten layer
- ImageNet
- max pooling
- MNIST
- stride
- tensor (rank, dimension)
- transfer learning
:::
:::

## Tutorial 4: Backpropagation

- backpropagation
- partial derivatives

## [Lecture 5: Natural Language Processing](/Natural-Language-Processing/natural-language-processing.qmd)

::: columns
:::: column
- bag of words
- lemmatization
- one-hot embedding
- stop words
::::
:::: column
- vocabulary
- word embeddings/vectors
- word2vec
::::
:::

## [Lecture 6: Uncertainy Quantification](/Uncertainty-Quantification/uncertainty-quantification.qmd)

::: columns
::: column
- aleatoric and epistemic uncertainty
- Bayesian neural network
- deep ensembles
- dropout
- ensemble model
- CANN
- GLM
:::
::: column
- MDN
- mixture distribution
- Monte Carlo dropout
- posterior sampling
- proper scoring rule
- uncertainty quantification
- variational approximation
:::
:::

## [Lecture 7: Recurrent Networks and Time Series](/Time-Series-And-Recurrent-Neural-Networks/time-series-and-rnns.qmd)

- GRU
- LSTM
- recurrent neural networks
- SimpleRNN

## [Lecture 8: Generative Networks](/Generative-Networks/generative-networks.qmd)


## Lecture 8-9: Generative Networks

::: columns
:::: column
- autoencoder (variational)
- beam search
- bias
- ChatGPT (& RLHF)
- DeepDream
- generative adversarial networks
- greedy sampling
::::
:::: column
- Hugging Face
- language model
- latent space
- neural style transfer
- softmax temperature
- stochastic sampling
::::
:::

## [Lecture 9: Interpretability](/Advanced-Topics/interpretability.qmd)

- global interpretability
- Grad-CAM
- inherent interpretability
- LIME
- local interpretability
- permutation importance
- post-hoc interpretability
- SHAP values
-->

# Contributors

- Tian (Eric) Dong
- Michael Jacinto
- Marcus Lautier
- Sam Luo
- Hang Nguyen
- Gayani Thalagoda

# Copyright

Patrick Laub
