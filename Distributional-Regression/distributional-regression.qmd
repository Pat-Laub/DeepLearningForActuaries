---
title: Distributional Regression
---

::: {.content-visible unless-format="revealjs"}

### Acknowledgments

Thanks to Eric Dong for making the original version of these slides.

:::

```{python}
#| echo: false
#| warning: false
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import matplotlib

# TODO: Update following section
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    plt.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    plt.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
plt.rcParams["figure.dpi"] = 350
plt.rcParams["savefig.bbox"] = "tight"
plt.rcParams["font.family"] = "serif"

plt.rcParams["axes.spines.right"] = False
plt.rcParams["axes.spines.top"] = False

import pandas as pandas

pandas.options.display.max_rows = 6

import random

random.seed(1234)

import tf_keras
import tensorflow

tensorflow.random.set_seed(1)
tensorflow.get_logger().setLevel("ERROR")
tensorflow.config.set_visible_devices([], "GPU")
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import random
import matplotlib.pyplot as plt
import numpy as np
import numpy.random as rnd
import pandas as pd

from sklearn import set_config
from sklearn.linear_model import LinearRegression
set_config(transform_output="pandas")

import tf_keras
import tensorflow as tf
from tf_keras.callbacks import EarlyStopping
from tf_keras.layers import Dense
from tf_keras.models import Sequential
from tf_keras.layers import Input, Dense, Concatenate
from tf_keras.models import Model
from tf_keras.initializers import Constant

from sklearn.model_selection import train_test_split
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OrdinalEncoder

import scipy.stats as stats
import statsmodels.api as sm
```

:::

# Traditional Regression {visibility="uncounted"}

## Traditional Regression

Multiple linear regression assumes the data-generating process is

$$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \varepsilon$$

where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$.

We estimate the coefficients $\beta_0, \beta_1, \ldots, \beta_p$ by minimising the sum of squared residuals or mean squared error

$$\text{RSS} := \sum_{i=1}^n (y_i - \hat{y}_i)^2
, \quad \text{MSE} := \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 ,
$$

where $\hat{y}_i$ is the predicted value for the $i$th observation.

## Visualising the distribution of each $Y$

```{python}
#| code-fold: true
# Generate sample data for linear regression
np.random.seed(0)
X = np.linspace(0, 10, 10)
np.random.shuffle(X)

beta_0 = 2
beta_1 = 3
y = beta_0 + beta_1 * X + np.random.normal(scale=2, size=X.shape)

# Fit a simple linear regression model
coefficients = np.polyfit(X, y, 1)
predicted_y = np.polyval(coefficients, X)

# Plot the data points and the fitted line
plt.scatter(X, y, label='Data Points')
plt.plot(X, predicted_y, color='red', label='Fitted Line')

# Draw the normal distribution bell curve sideways at each data point
for i in range(len(X)):
    mu = predicted_y[i]
    sigma = 2  # Assuming a standard deviation for the normal distribution
    y_values = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
    x_values = stats.norm.pdf(y_values, mu, sigma) + X[i]
    plt.plot(x_values, y_values, color='blue', alpha=0.5)

plt.xlabel('$x$')
plt.ylabel('$y$')
plt.legend()
```

## The probabilistic view

$$Y_i \sim \mathcal{N}(\mu_i, \sigma^2)$$

where $\mu_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}$,
and the $\sigma^2$ is known.

The $\mathcal{N}(\mu, \sigma^2)$ normal distribution has p.d.f.

$$f(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) .$$

The likelihood function is

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)
$$
$$
\Rightarrow \ell(\boldsymbol{\beta}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
$$

Perform maximum likelihood estimation to find $\boldsymbol{\beta}$.

## The predicted distributions

```{python}
#| code-fold: true
y_pred = np.polyval(coefficients, X[:4])

fig, axes = plt.subplots(4, 1, figsize=(5.0, 3.0))

x_min = y_pred[:4].min() - 4*sigma
x_max = y_pred[:4].max() + 4*sigma
x_grid = np.linspace(x_min, x_max, 100)

# Plot each normal distribution with different means vertically
for i, ax in enumerate(axes):
    mu = y_pred[i]
    y_grid = stats.norm.pdf(x_grid, mu, sigma)
    ax.plot(x_grid, y_grid)
    ax.set_ylabel(f'$f(y ; \\boldsymbol{{x}}_{{{i+1}}})$')
    ax.set_xticks([y_pred[i]], labels=[r'$\mu_{' + str(i+1) + r'}$'])

plt.tight_layout();
```

## The machine learning view

The negative log-likelihood $\text{NLL}(\boldsymbol{\beta}) := -\ell(\boldsymbol{\beta})$ is to be minimised:

$$
\text{NLL}(\boldsymbol{\beta})
= \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 .
$$

As $\sigma^2$ is fixed, minimising NLL is equivalent to minimising MSE:

$$
\begin{aligned}
\widehat{\boldsymbol{\beta}}
&= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{NLL}(\boldsymbol{\beta}) \\
&= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2 \\
&= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \frac{1}{n} \sum_{i=1}^n \Bigl( y_i - \hat{y}_i(\boldsymbol{x}_i; \boldsymbol{\beta}) \Bigr)^2 \\
&= \underset{\boldsymbol{\beta}}{\operatorname{arg\,min}}\,\, \text{MSE}\bigl( \boldsymbol{y}, \hat{\boldsymbol{y}}(\boldsymbol{\mathbf{X}}; \boldsymbol{\beta}) \bigr).
\end{aligned}
$$

## Generalised Linear Model (GLM)

The GLM is often characterised by the mean prediction:

$$
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = g^{-1} \left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)
$$

where $g$ is the link function.

Common GLM distributions for the response variable include: 

- Normal distribution with identity link (just MLR)
- Bernoulli distribution with logit link (logistic regression)
- Poisson distribution with log link (Poisson regression)
- Gamma distribution with log link

## Logistic regression

A Bernoulli distribution with parameter $p$ has p.m.f.

$$
f(y)\ =\ \begin{cases}
p & \text{if } y = 1 \\
1 - p & \text{if } y = 0
\end{cases}
\ =\ p^y (1 - p)^{1 - y}.
$$

Our model is $Y|\boldsymbol{X}=\boldsymbol{x}$ follows a Bernoulli distribution with parameter

$$
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \frac{1}{1 + \exp\left(-\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)} = \mathbb{P}(Y=1|\boldsymbol{X}=\boldsymbol{x}).
$$

The likelihood function, using $\mu_i := \mu(\boldsymbol{x}_i; \boldsymbol{\beta})$, is

$$
L(\boldsymbol{\beta})
\ =\ \prod_{i=1}^n \begin{cases}
\mu_i & \text{if } y_i = 1 \\
1 - \mu_i & \text{if } y_i = 0
\end{cases}
\ =\ \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i} .
$$

## Binary cross-entropy loss

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1 - y_i}
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
$$

The negative log-likelihood is

$$
\text{NLL}(\boldsymbol{\beta}) = -\sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
$$

The binary cross-entropy loss is identical:
$$
\text{BCE}(\boldsymbol{y}, \boldsymbol{\mu}) = -\sum_{i=1}^n \Bigl( y_i \log(\mu_i) + (1 - y_i) \log(1 - \mu_i) \Bigr).
$$

## Poisson regression

A Poisson distribution with rate $\lambda$ has p.m.f.
$$
f(y) = \frac{\lambda^y \exp(-\lambda)}{y!}.
$$

Our model is $Y|\boldsymbol{X}=\boldsymbol{x}$ is Poisson distributed with parameter

$$
\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right) .
$$

The likelihood function is

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{ \mu_i^{y_i} \exp(-\mu_i) }{y_i!}
$$
$$
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( -\mu_i + y_i \log(\mu_i) - \log(y_i!) \Bigr).
$$

## Poisson loss

The negative log-likelihood is

$$
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) + \log(y_i!) \Bigr) .
$$

The Poisson loss is

$$
\text{Poisson}(\boldsymbol{y}, \boldsymbol{\mu}) = \sum_{i=1}^n \Bigl( \mu_i - y_i \log(\mu_i) \Bigr).
$$

## Gamma regression

A gamma distribution with mean $\mu$ and dispersion $\phi$ has p.d.f.
$$
f(y; \mu, \phi) = \frac{(\mu \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y^{\frac{1}{\phi} - 1} \mathrm{e}^{-\frac{y}{\mu \phi}}
$$

Our model is $Y|\boldsymbol{X}=\boldsymbol{x}$ is gamma distributed with a dispersion of $\phi$ and a mean of
$\mu(\boldsymbol{x}; \boldsymbol{\beta}) = \exp\left(\left\langle \boldsymbol{\beta}, \boldsymbol{x} \right\rangle\right)$.

The likelihood function is
$$
L(\boldsymbol{\beta}) = \prod_{i=1}^n \frac{(\mu_i \phi)^{-\frac{1}{\phi}}}{\Gamma\left(\frac{1}{\phi}\right)} y_i^{\frac{1}{\phi} - 1} \exp\left(-\frac{y_i}{\mu_i \phi}\right)
$$

$$
\Rightarrow \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ -\frac{1}{\phi} \log(\mu_i \phi) - \log \Gamma\left(\frac{1}{\phi}\right) + \left(\frac{1}{\phi} - 1\right) \log(y_i) - \frac{y_i}{\mu_i \phi} \right].
$$

## Gamma loss

The negative log-likelihood is

$$
\text{NLL}(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i \phi) + \log \Gamma\left(\frac{1}{\phi}\right) - \left(\frac{1}{\phi} - 1\right) \log(y_i) + \frac{y_i}{\mu_i \phi} \right].
$$

Since $\phi$ is a nuisance parameter
$$
\text{NLL}(\boldsymbol{\beta})
= \sum_{i=1}^n \left[ \frac{1}{\phi} \log(\mu_i) + \frac{y_i}{\mu_i \phi} \right] + \text{const} 
\propto \sum_{i=1}^n \left[ \log(\mu_i) + \frac{y_i}{\mu_i} \right].
$$

::: {.callout-note}
As $\log(\mu_i) = \log(y_i) - \log(y_i / \mu_i)$, we could write an alternative version
$$
\text{NLL}(\boldsymbol{\beta})
\propto \sum_{i=1}^n \left[ \log(y_i) - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) + \frac{y_i}{\mu_i} \right]
\propto \sum_{i=1}^n \left[ \frac{y_i}{\mu_i} - \log\Bigl(\frac{y_i}{\mu_i}\Bigr) \right].
$$
:::

## Why do actuaries use GLMs?

- GLMs are interpretable.
- GLMs are flexible (can handle different types of response variables).
- We get the full distribution of the response variable, not just the mean.

This last point is particularly important for analysing worst-case scenarios.

# Stochastic Forecasts {visibility="uncounted"}

## Stock price forecasting

```{python}
#| code-fold: true
def lagged_timeseries(df, target, window=30):
    lagged = pd.DataFrame()
    for i in range(window, 0, -1):
        lagged[f"T-{i}"] = df[target].shift(i)
    lagged["T"] = df[target].values
    return lagged


stocks = pd.read_csv("../Time-Series-And-Recurrent-Neural-Networks/aus_fin_stocks.csv")
stocks["Date"] = pd.to_datetime(stocks["Date"])
stocks = stocks.set_index("Date")
_ = stocks.pop("ASX200")
stock = stocks[["CBA"]]
stock = stock.ffill()

df_lags = lagged_timeseries(stock, "CBA", 40)

# Split the data in time
X_train = df_lags.loc[:"2018"]
X_val = df_lags.loc["2019"]
X_test = df_lags.loc["2020":]

# Remove any with NAs and split into X and y
X_train = X_train.dropna()
X_val = X_val.dropna()
X_test = X_test.dropna()

y_train = X_train.pop("T")
y_val = X_val.pop("T")
y_test = X_test.pop("T")

X_train = X_train / 100
X_val = X_val / 100
X_test = X_test / 100
y_train = y_train / 100
y_val = y_val / 100
y_test = y_test / 100

lr = LinearRegression()
lr.fit(X_train, y_train);

stocks.plot()
plt.ylabel("Stock Price")
plt.legend(loc="upper center", bbox_to_anchor=(0.5, -0.5), ncol=4);
```

## Noisy auto-regressive forecast

```{python}
def noisy_autoregressive_forecast(model, X_val, sigma, suppress=False):
    """
    Generate a multi-step forecast using the given model.
    """
    multi_step = pd.Series(index=X_val.index, name="Multi Step")

    # Initialize the input data for forecasting
    input_data = X_val.iloc[0].values.reshape(1, -1)

    for i in range(len(multi_step)):
        # Ensure input_data has the correct feature names
        input_df = pd.DataFrame(input_data, columns=X_val.columns)
        if suppress:
            next_value = model.predict(input_df, verbose=0)
        else:
            next_value = model.predict(input_df)

        next_value += np.random.normal(0, sigma)

        multi_step.iloc[i] = next_value

        # Append that prediction to the input for the next forecast
        if i + 1 < len(multi_step):
            input_data = np.append(input_data[:, 1:], next_value).reshape(1, -1)

    return multi_step
```

## Original forecast

```{python}
lr_forecast = noisy_autoregressive_forecast(lr, X_val, 0)
```

```{python}
#| code-fold: true
stock.loc[lr_forecast.index, "AR Linear"] = 100 * lr_forecast

def plot_forecasts(stock):
    stock.loc["2018-12":"2019"].plot()
    plt.axvline("2019", color="black", linestyle="--")
    plt.ylabel("Stock Price")
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))

plot_forecasts(stock)
```

```{python}
residuals = y_train - lr.predict(X_train)
sigma = np.std(residuals)
```

## With noise

```{python}
np.random.seed(1)
lr_noisy_forecast = noisy_autoregressive_forecast(lr, X_val, sigma)
```

```{python}
#| code-fold: true
stock.loc[lr_noisy_forecast.index, "AR Noisy Linear"] = 100 * lr_noisy_forecast
plot_forecasts(stock)
```

## With noise {visibility="uncounted"}

```{python}
np.random.seed(2)
lr_noisy_forecast = noisy_autoregressive_forecast(lr, X_val, sigma)
```

```{python}
#| code-fold: true
stock.loc[lr_noisy_forecast.index, "AR Noisy Linear"] = 100 * lr_noisy_forecast
plot_forecasts(stock)
```

## With noise {visibility="uncounted"}

```{python}
np.random.seed(3)
lr_noisy_forecast = noisy_autoregressive_forecast(lr, X_val, sigma)
```

```{python}
#| code-fold: true
stock.loc[lr_noisy_forecast.index, "AR Noisy Linear"] = 100 * lr_noisy_forecast
plot_forecasts(stock)
```

## Many noisy forecasts {visibility="uncounted"}

```{python}
num_forecasts = 500
forecasts = []
for i in range(num_forecasts):
    forecasts.append(noisy_autoregressive_forecast(lr, X_val, sigma) * 100)
noisy_forecasts = pd.concat(forecasts, axis=1)
noisy_forecasts.index = X_val.index
```

```{python}
#| code-fold: true
noisy_forecasts.loc["2018-12":"2019"].plot(legend=False, alpha=0.4)
plt.ylabel("Stock Price");
```

## 95% "prediction intervals"

```{python}
# Calculate quantiles for the forecasts
lower_quantile = noisy_forecasts.quantile(0.025, axis=1)
upper_quantile = noisy_forecasts.quantile(0.975, axis=1)
mean_forecast = noisy_forecasts.mean(axis=1)
```

```{python}
#| code-fold: true
# Plot the mean forecast
plt.figure(figsize=(8, 3))

plt.plot(stock.loc["2018-12":"2019"].index, stock.loc["2018-12":"2019"]["CBA"], label="CBA")

plt.plot(mean_forecast, label="Mean")

# Plot the quantile-based shaded area
plt.fill_between(mean_forecast.index, 
                 lower_quantile, 
                 upper_quantile, 
                 color="grey", alpha=0.2)

# Plot settings
plt.axvline(pd.Timestamp("2019-01-01"), color="black", linestyle="--")
plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
plt.xlabel("Date")
plt.ylabel("Stock Price")
plt.tight_layout();
```

## Residuals 

```{python}
#| echo: false
set_square_figures()
```

::: columns
::: column
```{python}
y_pred = lr.predict(X_train)
residuals = y_train - y_pred
residuals -= np.mean(residuals)
residuals /= np.std(residuals)
stats.shapiro(residuals)
```

::: {.callout-note .fragment}
Probably should model the log-returns instead of the stock prices.
:::

:::
::: column
```{python}
#| code-fold: true
plt.hist(residuals, bins=40, density=True)
x = np.linspace(-3, 3, 100)
plt.xlim(-3, 3)
plt.plot(x, stats.norm.pdf(x, 0, 1));
```
:::
:::


## Q-Q plot and P-P plot {visibility="uncounted"}


::: columns
::: column
```{python}
#| code-fold: true
sm.qqplot(residuals, line="45");
```
:::
::: column
```{python}
#| code-fold: true
sm.ProbPlot(residuals).ppplot(line="45");
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

# GLMs and Neural Networks {visibility="uncounted"}

## Code: Data

``` {python}
import pandas as pd
sev_df = pd.read_csv('freMTPL2sev.csv')                                 #<1>
freq_df = pd.read_csv('freMTPL2freq.csv')                               #<2>

# Create a copy of freq dataframe without 'claimfreq' column
freq_without_claimfreq = freq_df.drop(columns=['ClaimNb'])              #<3>

# Merge severity dataframe with freq_without_claimfreq dataframe
new_sev_df = pd.merge(sev_df, freq_without_claimfreq, on='IDpol', 
                                                      how='left')       #<4>
new_sev_df = new_sev_df.dropna()                                        #<5>
new_sev_df = new_sev_df.drop("IDpol", axis=1)                           #<6>
new_sev_df[:2]                                                          #<7>
```
1. Imports `freMTPL2sev.csv` dataset
2. Imports `freMTPL2freq.csv` dataset
3. Drops `ClaimNb` column
4. Merges the two datasets ,`sev_df` and `freq_without_claimfreq` by matching the `IDpol` column. Assigning `how='left'` ensures that all rows from the left dataset `sev_df` is considered, and only the matching columns from `freq_without_claimfreq` are selected
5. Drops missing values or/and NAN values
6. Drops the `IDpol` column from `new_sev_df`
7. Retrieves first two rows of the dataset

## Code: Preprocessing
::: {.content-visible unless-format="revealjs"}
Next we carry out some basic preprocessing
:::

```{python}
X_train, X_test, y_train, y_test = train_test_split(
  new_sev_df.drop("ClaimAmount", axis=1),
  new_sev_df["ClaimAmount"],
  random_state=2023)

# Reset each index to start at 0 again.
X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)
```

## Code: Preprocessing

::: {.content-visible unless-format="revealjs"}
Next we define the column transfer. The column transfer first applies ordinal encoding to `VehBrand`, `Region`, `Area` and `VehGas` variables, and applies standard scaling to all remaining numerical values. Next we fit the defined column transfer to the training set. The fitted transformation is then applied on both training and test sets. (Note that the fitting is only carried out on the train set and the same fit is applied to both train, validation and test sets.)
:::

::: {.content-visible unless-format="revealjs"}
Since this task does not apply entity embeddings, `VehBrand` and `Region` variables are dropped from the dataframe.
:::

```{python}
ct = make_column_transformer(
  (OrdinalEncoder(), ["Area", "VehGas"]),
  ("drop", ["VehBrand", "Region"]),
  remainder=StandardScaler(),
  verbose_feature_names_out=False
)

X_train = ct.fit_transform(X_train)
X_test = ct.transform(X_test)
```

- $\texttt{VehGas=1}$ if the car gas is regular.
- $\texttt{Area=0}$ represents the rural area, and $\texttt{Area=5}$ represents the urban center.

## Histogram of the `ClaimAmount`

::: {.content-visible unless-format="revealjs"}
Plotting the empirical distribution of the target variable help us get an understanding of the inherent variability associated with the data.
:::

```{python}
plt.hist(y_train[y_train < 5000], bins=30);
```

::: {.content-visible unless-format="revealjs"}
The following section illustrates how embedding a GLM in a neural network architecture can help us quantify the uncertainty relating to the predictions coming from the neural network. The idea is to first fit a GLM, and use the predictions from the GLM and predictions from the neural network part to define a custom loss function. This embedding presents an opportunity to compute the dispersion parameter $\phi_{CANN}$ for the neural network.
<!--The dispersion parameter provides insights into whether the model accurately captures the inherent variability (aleatoric uncertainty) in the data or not.-->
:::

::: {.content-visible unless-format="revealjs"}
The idea of GLM is to find a linear combination of independent variables $\boldsymbol{x}$ and coefficients $\boldsymbol{\beta}$, apply a non-linear transformation ($g^{-1}$) to that linear combination and set it equal to conditional mean of the response variable $Y$ given an instance $\boldsymbol{x}$. The non-linear transformation provides added flexibility.
:::

## Gamma GLM

Suppose a fitted gamma GLM model has

- a log link function $g(x)=\log(x)$ and
- regression coefficients $\boldsymbol{\beta}=(\beta_0, \beta_1, \beta_2, \beta_3)$.

Then, it estimates the conditional mean of $Y$ given a new instance $\boldsymbol{x}=(1, x_1, x_2, x_3)$ as follows:
$$
    \mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}]=g^{-1}(\langle \boldsymbol{\beta}, \boldsymbol{x}\rangle)=\exp\big(\beta_0+ \beta_1x_1+\beta_2x_2+\beta_3x_3\big).
$$

A GLM can model any other exponential family distribution using an appropriate link function $g$.

## Gamma GLM loss

If $Y|\boldsymbol{X}=\boldsymbol{x}$ is a gamma r.v. with mean $\mu(\boldsymbol{x}; \boldsymbol{\beta})$ and dispersion parameter $\phi$, we can minimise the negative log-likelihood (NLL)
$$
    \text{NLL} \propto \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})} + \text{const},
$$
i.e., we ignore the dispersion parameter $\phi$ while estimating the regression coefficients.

## Fitting Steps

Step 1. Use the advanced second derivative iterative method to find the regression coefficients:
$$
    \widehat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{arg\,min}} \ \sum_{i=1}^{n}\log \mu (\boldsymbol{x}_i; \boldsymbol{\beta})+\frac{y_i}{\mu (\boldsymbol{x}_i; \boldsymbol{\beta})}
$$

Step 2. Estimate the dispersion parameter:
$$
    \phi = \frac{1}{n-p}\sum_{i=1}^{n}\frac{(y_i-\mu(\boldsymbol{x}_i; \boldsymbol{\beta} ))^2}{\mu(\boldsymbol{x}_i; \boldsymbol{\beta} )^2}
$$ 

## Code: Gamma GLM

In Python, we can fit a gamma GLM as follows:


```{python}
import statsmodels.api as sm

# Add a column of ones to include an intercept in the model
X_train_design = sm.add_constant(X_train)

# Create a Gamma GLM with a log link function
gamma_glm = sm.GLM(y_train, X_train_design,                   
            family=sm.families.Gamma(sm.families.links.Log()))

# Fit the model
gamma_glm = gamma_glm.fit()
```

::: columns
::: column
```{python}
gamma_glm.params
```
:::
::: column
```{python}
# Dispersion Parameter
mus = gamma_glm.predict(X_train_design)
residuals = y_train - mus
variance = mus**2
dof = (len(y_train)-X_train.shape[1])
phi_glm =  np.sum(residuals**2/variance)/dof
print(phi_glm)
```
:::
:::

::: {.content-visible unless-format="revealjs"}
The above example of fitting a Gamma distribution assumes a constant dispersion, meaning that, the dispersion of claim amount is constant for all policyholders. If we believe that the constant dispersion assumption is quite strong, we can use a double GLM model. Fitting a GLM is the traditional way of modelling a claim amount.
:::

## ANN can feed into a GLM

![Combining GLM & ANN.](richman-glm-and-ann.png)

::: footer
Source: Ronald Richman (2022), Mind the Gap - Safely Incorporating Deep Learning Models into the Actuarial Toolkit, IFoA seminar, Slide 14.
:::

# Combined Actuarial Neural Network {visibility="uncounted"}

## CANN

The Combined Actuarial Neural Network is a novel actuarial neural network architecture proposed by Schelldorfer and WÃ¼thrich (2019). We summarise the CANN approach as follows:

- Find the coefficients $\boldsymbol{\beta}$ of the GLM with a link function $g(\cdot)$.
- Find the weights $\boldsymbol{w}_{\text{CANN}}$ of a neural network $\mathcal{M}_{\text{CANN}}:\mathbb{R}^{p}\to\mathbb{R}$.
- Given a new instance $\boldsymbol{x}$, we have $$\mathbb{E}[Y|\boldsymbol{X}=\boldsymbol{x}] = g^{-1}\Big( \langle\boldsymbol{\beta}, \boldsymbol{x}\rangle + \mathcal{M}_{\text{CANN}}(\boldsymbol{x};\boldsymbol{w}_{\text{CANN}})\Big).$$

## Architecture

![CANN approach.](CANN.png)

## Code: Architecture

```{python}
# Ensure reproducibility
random.seed(1); tf.random.set_seed(1)                                   #<1>

# Pre-defined constants
glm_weights = gamma_glm.params.iloc[1:].values                          #<2>
glm_bias = gamma_glm.params.iloc[0]                                     #<3>

# Define model inputs
inputs = Input(shape=X_train.shape[1:])                                 #<4>

# Non-trainable GLM linear part
glm_logmu = Dense(1, activation='linear', trainable=False,
                     kernel_initializer=Constant(glm_weights),
                     bias_initializer=Constant(glm_bias))(inputs)       #<5>         

# Neural network layers
x = Dense(64, activation='relu')(inputs)                                #<6>
x = Dense(64, activation='relu')(x)                                     #<7>
cann_logmu = Dense(1, activation='linear')(x)                           #<8>
```
1. Sets the random seed for reproducibility
2. Stores weights computed from GLM in `glm_weights`
3. Stores bias computed from GLM in `glm_bias`
4. Specifies the model input features
5. Adds a `Dense` layer with just one neuron, to store the model output from the GLM. The linear activation is used to make sure that the output is a linear combination of inputs. The weights are set to be non-trainable, hence the values obtained during GLM fitting will not change during the neural network training process. `kernel_initializer=Constant(glm_weights)` and `bias_initializer=Constant(glm_bias)` ensures that weights are initialized with the optimal values estimated from GLM fit. 
6. Adds another `Dense` layer 
7. Adds another `Dense` layer 
8. Adds the output layer with linear activation

## Code: Loss Function

```{python}
# Combine GLM and CANN estimates
cann = Model(inputs, Concatenate(axis=1)([cann_logmu, glm_logmu]))      #<1>
```
1. Since the output of the model is evaluated by combining the output from both branches, the model is constructed by concatenating outputs from `cann_logmu` and `glm_logmu`. Note that there are two predicted values, one predicted value from the `glm_logmu` component and the other coming from the `cann_logmu` component.

We need to customise the loss function for CANN.

```{python}
def cann_negative_log_likelihood(y_true, y_pred):
    # The new mean estimate
    cann_logmu = y_pred[:, 0]                                       #<1>
    glm_logmu = y_pred[:, 1]                                        #<2>
    mu = tf.math.exp(cann_logmu + glm_logmu)                        #<3>

    # Compute the negative log likelihood of the Gamma distribution
    nll = tf.reduce_mean(cann_logmu + glm_logmu + y_true/mu)        #<4>
    
    return nll
```
1. Stores the first column of the `y_pred` matrix as `cann_logmu` (the prediction from the CANN)
2. Stores the second column of the `y_pred` matrix as `glm_logmu` (the prediction from the glm)
3. Computes the exponential of the sum of them as `mu`
4. Computes the negative log likelihood of a Gamma distribution where $\log(\mu)$ is now the sum `cann_logmu + glm_logmu`

## Code: Model Training

```{python}
#| eval: false
cann.compile(optimizer="adam", loss=cann_negative_log_likelihood)       #<1>
hist = cann.fit(X_train, y_train,
    epochs=100, 
    callbacks=[EarlyStopping(patience=10)],  
    verbose=0,
    batch_size=64, 
    validation_split=0.2)                                               #<2>
```
1. Compiles the model with adam optimizer and the custom loss function
2. Fits the model (with a validation split defined inside the fit function)

```{python}
#| echo: false
if not os.path.exists("cann.keras"):
    cann.compile(optimizer="adam", loss=cann_negative_log_likelihood)
    cann.fit(X_train, y_train,
      epochs=100, 
      callbacks=[EarlyStopping(patience=10)],  
      verbose=0,
      batch_size=64, 
      validation_split=0.2)
    cann.save("cann.keras")
else:
    cann = tf_keras.models.load_model("cann.keras", 
                                      custom_objects={"cann_negative_log_likelihood": cann_negative_log_likelihood})
```

Find the dispersion parameter.

``` {python}
mus = np.exp(np.sum(cann.predict(X_train, verbose=0), axis = 1))
residuals = y_train - mus
variance = mus**2
dof = (len(y_train)-X_train.shape[1])
phi_cann = np.sum(residuals**2/variance) / dof
print(phi_cann)
```

# Mixture Density Network {visibility="uncounted"}

## Mixture Distribution

::: {.content-visible unless-format="revealjs"}
One intuitive way to capture uncertainty using neural networks would be to estimate the parameters of the target distribution, instead of predicting the value it self. For example, suppose we want to predict $y$ coming from a Gaussian distribution. Most common method would be to predict $(\hat{y})$ directly using a single neuron at the output layer. Another possible way would be to estimate the parameters ($\mu$ and $\sigma$) of the $y$ distribution using 2 neurons at the output layer. Estimating parameters of the distribution instead of point estimates for $y$ can help us get an idea about the uncertainty. However, assuming distributional properties at times could be too restrictive. For example, it is possible that the actual distribution of $y$ values is bimodal or multi modal. In such situations, assuming a mixture distribution is more intuitive.    
:::

Given a finite set of resulting random variables $(Y_1, \ldots, Y_{K})$, one can generate a multinomial random variable $Y\sim \text{Multinomial}(1, \boldsymbol{\pi})$. Meanwhile, $Y$ can be regarded as a mixture of $Y_1, \ldots, Y_{K}$, i.e.,
$$
  Y = \begin{cases} 
      Y_1 & \text{w.p. } \pi_1, \\ 
      \vdots & \vdots\\
      Y_K & \text{w.p. } \pi_K, \\ 
  \end{cases}
$$
where we define a set of finite set of weights $\boldsymbol{\pi}=(\pi_{1} \ldots, \pi_{K})$ such that $\pi_k \ge 0$ for $k \in \{1, \ldots, K\}$ and $\sum_{k=1}^{K}\pi_k=1$.

## Mixture Distribution

Let $f_{Y_k|\boldsymbol{X}}$ and $F_{Y_k|\boldsymbol{X}}$ be the p.d.f. and the c.d.f of $Y_k|\boldsymbol{X}$ for all $k \in \{1, \ldots, K\}$.

The random variable $Y|\boldsymbol{X}$, which mixes $Y_k|\boldsymbol{X}$'s with weights $\pi_k$'s, has the density function
$$
    f_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) f_{k}(y|\boldsymbol{x}),
$$
and the cumulative density function
$$
    F_{Y|\boldsymbol{X}}(y|\boldsymbol{x}) = \sum_{k=1}^{K}\pi_k(\boldsymbol{x}) F_{k}(y|\boldsymbol{x}).
$$

## Mixture Density Network

A mixture density network (MDN) $\mathcal{M}_{\boldsymbol{w}^*}$ outputs each distribution component's mixing weights and parameters of $Y$ given the input features $\boldsymbol{x}$, i.e.,
$$
    \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})=(\boldsymbol{\pi}(\boldsymbol{x};\boldsymbol{w}^*), \boldsymbol{\theta}(\boldsymbol{x};\boldsymbol{w}^*)),
$$
where $\boldsymbol{w}^*$ is the networks' weights found by minimising the following negative log-likelihood loss function 
$$
    \mathcal{L}(\mathcal{D}, \boldsymbol{\theta})= - \sum_{i=1}^{n} \log f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w}^*),
$$
where $\mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$ is the training dataset.

## Mixture Density Network

![An MDN that outputs the parameters for a $K$ component mixture distribution. $\boldsymbol{\theta}_k(\boldsymbol{x}; \boldsymbol{w}^*)= (\theta_{k,1}(\boldsymbol{x}; \boldsymbol{w}^*), \ldots, \theta_{k,|\boldsymbol{\theta}_k|}(\boldsymbol{x}; \boldsymbol{w}^*))$ consists of the parameter estimates for the $k$th mixture component.](MDN.png)

## Model Specification

Suppose there are two types of claims:

- Type I: $Y_1|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_1(\boldsymbol{x}), \beta_1(\boldsymbol{x}))$ and,
- Type II: $Y_2|\boldsymbol{X}=\boldsymbol{x}\sim \text{Gamma}(\alpha_2(\boldsymbol{x}), \beta_2(\boldsymbol{x}))$.

The density of the actual claim amount $Y|\boldsymbol{X}=\boldsymbol{x}$ follows
$$
    \begin{align*}
        f_{Y|\boldsymbol{X}}(y|\boldsymbol{x})
        &= \pi_1(\boldsymbol{x})\cdot \frac{\beta_1(\boldsymbol{x})^{\alpha_1(\boldsymbol{x})}}{\Gamma(\alpha_1(\boldsymbol{x}))}\mathrm{e}^{-\beta_1(\boldsymbol{x})y}y^{\alpha_1(\boldsymbol{x})-1} \\
        &\quad + (1-\pi_1(\boldsymbol{x}))\cdot \frac{\beta_2(\boldsymbol{x})^{\alpha_2(\boldsymbol{x})}}{\Gamma(\alpha_2(\boldsymbol{x}))}\mathrm{e}^{-\beta_2(\boldsymbol{x})y}y^{\alpha_2(\boldsymbol{x})-1}.
    \end{align*}
$$
where $\pi_1(\boldsymbol{x})$ is the probability of a Type I claim given $\boldsymbol{x}$.

## Output

The aim is to find the optimum weights
$$
    \boldsymbol{w}^* = \underset{w}{\text{arg\,min}} \ \mathcal{L}(\mathcal{D}, \boldsymbol{w})
$$
for the Gamma mixture density network $\mathcal{M}_{\boldsymbol{w}^*}$ that outputs the mixing weights, shapes and scales of $Y$ given the input features $\boldsymbol{x}$, i.e.,
$$
    \begin{align*}
        \mathcal{M}_{\boldsymbol{w}^*}(\boldsymbol{x})
        = ( &\pi_1(\boldsymbol{x}; \boldsymbol{w}^*),
             \pi_2(\boldsymbol{x}; \boldsymbol{w}^*), \\
            &\alpha_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \alpha_2(\boldsymbol{x}; \boldsymbol{w}^*),\\ 
            &\beta_1(\boldsymbol{x}; \boldsymbol{w}^*),
            \beta_2(\boldsymbol{x}; \boldsymbol{w}^*)
        ).
    \end{align*}
$$

## Architecture

![We demonstrate the structure of a gamma MDN that outputs the parameters for a gamma mixture with two components.](Gamma_MDN.png)

## Code: Architecture

The following code resembles the architecture of the architecture of the gamma MDN from the previous slide.

```{python}
# Ensure reproducibility
random.seed(1); tf.random.set_seed(1)                                   #<1>

inputs = Input(shape=X_train.shape[1:])                                 #<2>

# Two hidden layers 
x = Dense(64, activation='relu')(inputs)                                #<3>
x = Dense(64, activation='relu')(x)

pis = Dense(2, activation='softmax')(x) # Mixing weights                 #<4>
alphas = Dense(2, activation='exponential')(x) # Shape parameters
betas = Dense(2, activation='exponential')(x) # Scale parameters

# `y_pred` will now have 6 columns
gamma_mdn = Model(inputs, Concatenate(axis=1)([pis, alphas, betas]))    #<5>
```
1. Sets the random seeds for reproducibility
2. Defines the input layer with the number of neurons being equal to the number of input features
3. Specifies the hidden layers of the neural network
4. Specifies the neurons of the output layer. Here, `softmax` is used for $\pi$ values as they must sum up to 1. `exponential` activation is used for both $\alpha$'s and $\beta$'s as they must be non-negative.
5. Defines the model by specifying the inputs and outputs


## Loss Function

The negative log-likelihood loss function is given by

$$
    \mathcal{L}(\mathcal{D}, \boldsymbol{w})
    = - \sum_{i=1}^{n} \log \  f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w}) 
$$
where the $f_{Y|\boldsymbol{X}}(y_i|\boldsymbol{x}, \boldsymbol{w})$ is defined by
$$
\begin{align*}
    &\pi_1(\boldsymbol{x};\boldsymbol{w})\cdot \frac{\beta_1(\boldsymbol{x};\boldsymbol{w})^{\alpha_1(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_1(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_1(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_1(\boldsymbol{x};\boldsymbol{w})-1} \\
    & \quad + (1-\pi_1(\boldsymbol{x};\boldsymbol{w}))\cdot \frac{\beta_2(\boldsymbol{x};\boldsymbol{w})^{\alpha_2(\boldsymbol{x};\boldsymbol{w})}}{\Gamma(\alpha_2(\boldsymbol{x};\boldsymbol{w}))}\mathrm{e}^{-\beta_2(\boldsymbol{x};\boldsymbol{w})y}y^{\alpha_2(\boldsymbol{x};\boldsymbol{w})-1} 
\end{align*}
$$

## Code: Loss Function

We employ functions from `tensorflow_probability` to code the loss function for the gamma MDN.
The `MixtureSameFamily` function facilitates defining a mixture distribution all components from the same distribution but have different parametrization.

```{python}
import tensorflow_probability as tfp                                        #<1>
tfd = tfp.distributions                                                     #<2>
K = 2 # number of mixture components                                        #<3>

def gamma_mixture_nll(y_true, y_pred):                                      
    K = y_pred.shape[1] // 3                                                #<4>
    pis =  y_pred[:, :K]                                                    
    alphas = y_pred[:, K:2*K]                                               
    betas = y_pred[:, 2*K:3*K]                                              

    # The mixture distribution is a MixtureSameFamily distribution
    mixture_distribution = tfd.MixtureSameFamily(
        mixture_distribution=tfd.Categorical(probs=pis),
        components_distribution=tfd.Gamma(alphas, betas))                   #<5>

    # The loss is the negative log-likelihood of the data
    return -mixture_distribution.log_prob(y_true)                           #<6>
```
1. Imports `tfp` class from `tensorflow_probability`
2. Stores statistical distributions in the `tfp` class as `tfd`
3. Specifies the number of components in the mixture model
4. Extracts predicted values for all model components and stores them in separate matrices
5. Specifies the mixture distribution using computed model components
6. Use the fitted model to calculate negative log likelihood given the observed data

## Code: Model Training

```{python}
#| eval: false
# Employ the loss function from previous slide
gamma_mdn.compile(optimizer="adam", loss=gamma_mixture_nll)             #<1>

hist = gamma_mdn.fit(X_train, y_train,
    epochs=100, 
    callbacks=[EarlyStopping(patience=10)],  
    verbose=0,
    batch_size=64, 
    validation_split=0.2)                                               #<2>
```
1. Compiles the model using `adam` optimizer and the `gamma_mixture_nll` (negative log likelihood) as the loss function
2. Fits the model using the training data, with a validation split

```{python}
#| echo: false
if not os.path.exists("gamma_mdn.keras"):
    gamma_mdn.compile(optimizer="adam", loss=gamma_mixture_nll)
    gamma_mdn.fit(X_train, y_train,
      epochs=100, 
      callbacks=[EarlyStopping(patience=10)],  
      verbose=0,
      batch_size=64, 
      validation_split=0.2)
    gamma_mdn.save("gamma_mdn.keras")
else:
    gamma_mdn = tf_keras.models.load_model("gamma_mdn.keras", 
                                           custom_objects={'gamma_mixture_nll': gamma_mixture_nll})
```

<!--
# Calibration
-->

# Metrics for Distributional Regression {visibility="uncounted"}

## Proper Scoring Rules

::: {.content-visible unless-format="revealjs"}
Proper scoring rules provide a summary measure for the performance of the probabilistic predictions. They are useful in comparing performances across models. 
:::

Definition

:   *The scoring rule* $S : \mathcal{F} \times \mathbb{R} \to \bar{\mathbb{R}}$ is proper relative to the class $\mathcal{F}$ if $$
    S(G, G)\le S(F, G)
    $$ for all $F,G\in \mathcal{F}$. It is strictly proper if equality holds only if $F = G$.

Examples:

- Logarithmic Score (NLL)
- Continuous Ranked Probability Score (CRPS)

## Proper Scoring Rules

Logarithmic Score (NLL)

:   The logarithmic score is defined as
    $$
        \mathrm{LogS}(f, y) = - \log f(y),
    $$
    where $f$ is the predictive density.

Continuous Ranked Probability Score (CRPS)

:   The continuous ranked probability score is defined as
    $$
        \mathrm{crps}(F, y) = \int_{-\infty}^{\infty} (F(t) - {1}_{t\ge y})^2 \ \mathrm{d}t,
    $$
    where $F$ is the cumulative distribution function.

## Code: NLL

```{python}
from scipy.stats import gamma

def gamma_nll(mean, dispersion, y):
    # Calculate shape and scale parameters from mean and dispersion
    shape = 1 / dispersion; scale = mean * dispersion

    # Create a gamma distribution object
    gamma_dist = gamma(a=shape, scale=scale)
    
    return -np.mean(gamma_dist.logpdf(y))

# GLM
X_test_design = sm.add_constant(X_test)
mus = gamma_glm.predict(X_test_design)
nll_glm = gamma_nll(mus, phi_glm, y_test)

# CANN
mus = np.exp(np.sum(cann.predict(X_test, verbose=0), axis = 1))
nll_cann = gamma_nll(mus, phi_cann, y_test)

# MDN
nll_mdn = gamma_mdn.evaluate(X_test, y_test, verbose=0)
```

## Model Comparisons

```{python}
print(f'GLM: {round(nll_glm, 2)}')
print(f'CANN: {round(nll_cann, 2)}')
print(f'MDN: {round(nll_mdn, 2)}')
```

::: {.content-visible unless-format="revealjs"}
The above results show that MDN provides the lowest value for the Logarithmic Score (NLL). Low values for NLL indicate better calibration. One possible reason for the better performance of the MDN model (compared to the Gamma model) is the added flexibility from multiple modelling components. The multiple modelling components in the MDN model, together, can capture the inherent variation in the data better.
:::


# Aleatoric and Epistemic Uncertainty {visibility="uncounted"}

::: {.content-visible unless-format="revealjs"}
Uncertainty in deep learning refers to the level of doubt one would have about the predictions made by an AI-driven algorithm.
Identifying and quantifying different sources of uncertainty that could exist in AI-driven algorithms is therefore important to ensure a credible application.
:::

## Categories of uncertainty

There are two major categories of uncertainty in statistical or machine learning:

- Aleatoric uncertainty
- Epistemic uncertainty

Since there is no consensus on the definitions of aleatoric and epistemic uncertainty, we provide the most acknowledged definitions in the following slides.

## Aleatoric Uncertainty

::: {.content-visible unless-format="revealjs"}
Aleatoric uncertainty refers to the inherent variability associated with the data generating process. Among many ways to capture the aleatoric uncertainty, (i) combining with probabilistic models and (ii) considering mixture models are two useful methods to quantify the inherent variability.
:::

Qualitative Definition

:   *Aleatoric uncertainty refers to the statistical variability and inherent noise with data distribution that modelling cannot explain.*

Quantitative Definition

:   $$\text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}) = \mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}],$$i.e., if $Y|\boldsymbol{X}=\boldsymbol{x} \sim \mathcal{N}(\mu, \sigma^2)$, the aleatoric uncertainty would be $\sigma^2$. Simply, it is the conditional variance of the response variable $Y$ given features/covariates $\boldsymbol{x}$.

## Epistemic Uncertainty

Qualitative Definition

:   *Epistemic uncertainty refers to the lack of knowledge, limited data information, parameter errors and model errors.*

Quantitative Definition

:   $$\text{Epi}(Y|\boldsymbol{X}=\boldsymbol{x}) = \text{Uncertainty}(Y|\boldsymbol{X}=\boldsymbol{x}) - \text{Ale}(Y|\boldsymbol{X}=\boldsymbol{x}),$$

i.e., the total uncertainty subtracting the aleatoric uncertainty $\mathbb{V}[Y|\boldsymbol{X}=\boldsymbol{x}]$ would be the epistemic uncertainty.

## Sources of uncertainty

::: {.content-visible unless-format="revealjs"}
There are many sources of uncertainty in statistical or machine learning models.
Parameter error stems primarily due to lack of data.
Model error stems from assuming wrong distributional properties of the data.
Data uncertainty arises due to the lack of confidence we may have about the quality of the collected data.
Noisy data, inconsistent data, data with missing values or data with missing important variables can result in data uncertainty.
:::

*If you decide to predict the claim amount of an individual using a deep learning model, which source(s) of uncertainty are you dealing with?*

1. The inherent variability of the data-generating process $\rightarrow$ aleatoric uncertainty.
2. Parameter error $\rightarrow$ epistemic uncertainty.
3. Model error $\rightarrow$ epistemic uncertainty.
4. Data uncertainty $\rightarrow$ epistemic uncertainty.

## Notation {.appendix data-visibility="uncounted"}

- scalars are denoted by lowercase letters, e.g., $y$,
- vectors are denoted by bold lowercase letters, e.g.,
$$\boldsymbol{y} = (y_1, \ldots, y_n) ,$$
- random variables are denoted by capital letters, e.g., $Y$
- random vectors are denoted by bold capital letters, e.g.,
$$\boldsymbol{X} = (X_1, \ldots, X_p) ,$$
- matrices are denoted by bold uppercase non-italics letters, e.g.,
$$\mathbf{X} = \begin{pmatrix} x_{11} & \cdots & x_{1p} \\ \vdots & \ddots & \vdots \\ x_{n1} & \cdots & x_{np} \end{pmatrix} .$$

<!-- If we need random matrices, then run to the nearest fire exit. -->

## Regression notation {.appendix data-visibility="uncounted"}

- $n$ is the number of observations, $p$ is the number of features,
- the true coefficients are $\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)$,
- $\beta_0$ is the intercept, $\beta_1, \ldots, \beta_p$ are the coefficients,
- $\widehat{\boldsymbol{\beta}}$ is the estimated coefficient vector,
- $\boldsymbol{x}_i = (1, x_{i1}, x_{i2}, \ldots, x_{ip})$ is the feature vector for the $i$th observation,
- $y_i$ is the response variable for the $i$th observation,
- $\hat{y}_i$ is the predicted value for the $i$th observation,
- probability density functions (p.d.f.), probability mass functions (p.m.f.), cumulative distribution functions (c.d.f.).

<!-- - the error $\varepsilon$ random variable is not capitalised, -->

## Package Versions {.appendix data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tensorflow_probability,tf_keras"))
```

## Glossary {.appendix data-visibility="uncounted"}

::: columns
::: column
- aleatoric and epistemic uncertainty
- deep ensembles
- CANN
- GLM
:::
::: column
- MDN
- mixture distribution
- posterior sampling
- proper scoring rule
:::
:::